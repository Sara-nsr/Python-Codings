{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN_Warehouse_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojjars285vYB"
      },
      "source": [
        "## **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QS2FJR_5HXY"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import RMSprop, SGD\n",
        "from tensorflow.keras import initializers\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "V5RzDInHKbjS",
        "outputId": "c5829f6f-abda-4516-c35c-f8fc0cc14826"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-44b60b46-a9b8-4716-b9e9-2b24013e670f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-44b60b46-a9b8-4716-b9e9-2b24013e670f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Round 5 Unbiased Scores.csv to Round 5 Unbiased Scores.csv\n",
            "'Round 5 Unbiased Scores.csv'   \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9EOj2N7LLut"
      },
      "source": [
        "def load_knn_data():\n",
        "  # X_train = np.genfromtxt('train.csv', delimiter=',')\n",
        "  X_train = np.genfromtxt('Round 5 Unbiased Scores.csv', delimiter=',',skip_header=True)\n",
        "  X_train = X_train[:,0:4]\n",
        "  # fit scaler on data\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit(X_train)\n",
        "  # apply transform\n",
        "  X_train = scaler.transform(X_train)\n",
        "  # opt = SGD(lr=0.01, momentum=0.9, clipvalue=0.5)\n",
        "  # print(normalized)\n",
        "  # inverse = scaler.inverse_transform(normalized)\n",
        "  print(X_train.shape)\n",
        "  return X_train, scaler\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeULAEz8Ma1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee500dcf-02dc-4c51-837c-6557b0202a62"
      },
      "source": [
        "a,b = load_knn_data()\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3868, 4)\n",
            "MinMaxScaler(copy=True, feature_range=(0, 1))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xTZRB8qG_D-w",
        "outputId": "4cb1b785-0675-4673-9e3d-5babc70c1b71"
      },
      "source": [
        "class WGAN():\n",
        "    def __init__(self):\n",
        "      ################### img to data\n",
        "        self.data_rows = 1\n",
        "        self.data_cols = 4\n",
        "        # self.channels = 1\n",
        "        self.data_shape = (self.data_cols)\n",
        "        ############################################################\n",
        "        self.latent_dim = 4\n",
        "\n",
        "\n",
        "\n",
        "        # Following parameter and optimizer set as recommended in paper\n",
        "        self.n_critic = 5\n",
        "        self.clip_value = 0.1\n",
        "        optimizer = RMSprop(lr=0.0000005)\n",
        "\n",
        "        # Build and compile the critic\n",
        "        self.critic = self.build_critic()\n",
        "        self.critic.compile(loss=self.wasserstein_loss,\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generated imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        data = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.critic.trainable = False\n",
        "\n",
        "        # The critic takes generated images as input and determines validity\n",
        "        valid = self.critic(data)\n",
        "\n",
        "        # The combined model  (stacked generator and critic)\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss=self.wasserstein_loss,\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "        self.DLOS = np.array([])\n",
        "        self.GLOS = np.array([])\n",
        "    def wasserstein_loss(self, y_true, y_pred):\n",
        "        return K.mean(y_true * y_pred)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)\n",
        "        model.add(Dense(50 , activation=\"relu\", input_dim=self.latent_dim,kernel_initializer=initializer))\n",
        "        model.add(Dense(30 , activation=\"relu\"))\n",
        "        model.add(Dense(15 , activation=\"relu\"))\n",
        "        model.add(Dense(4 , activation=\"sigmoid\"))\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        data = model(noise)\n",
        "\n",
        "        return Model(noise, data)\n",
        "    def build_critic(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)\n",
        "        model.add(Dense(50 , activation=\"relu\",input_dim=4,kernel_initializer=initializer))\n",
        "        model.add(Dense(25 , activation=\"relu\"))\n",
        "        model.add(Dense(8 , activation=\"relu\"))\n",
        "        model.add(Dense(1))\n",
        "        model.summary()\n",
        "        \n",
        "\n",
        "        data = Input(shape=self.data_shape)\n",
        "        validity = model(data)\n",
        "        \n",
        "\n",
        "        return Model(data, validity)\n",
        "    # validity= build_critic(data)  \n",
        "    # print(validity)\n",
        "#################################################sample interval50 to 2\n",
        "########################################################batch size 128 to 2\n",
        "    def train(self, epochs, batch_size, sample_interval):\n",
        "\n",
        "        # Load the dataset\n",
        "        #####################################################\n",
        "        # (X_train, _), (_, _) = mnist.load_data()\n",
        "        X_train,_ = load_knn_data() \n",
        "\n",
        "        # Rescale -1 to 1\n",
        "        ##################################################\n",
        "        # X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "        # X_train = np.expand_dims(X_train, axis=3)\n",
        "        \n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = -np.ones((batch_size, 1))\n",
        "        fake = np.ones((batch_size, 1))\n",
        "        # print(valid)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            for _ in range(self.n_critic):\n",
        "\n",
        "                # ---------------------\n",
        "                #  Train Discriminator\n",
        "                # ---------------------\n",
        "\n",
        "                # Select a random batch of images\n",
        "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "                data = X_train[idx]\n",
        "                # print(data)\n",
        "                \n",
        "                # Sample noise as generator input\n",
        "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "                # Generate a batch of new images\n",
        "                gen_data = self.generator.predict(noise)\n",
        "                # print(gen_data.shape)\n",
        "\n",
        "                # Train the critic\n",
        "                d_loss_real = self.critic.train_on_batch(data, valid)\n",
        "                # print(d_loss_real)\n",
        "                d_loss_fake = self.critic.train_on_batch(gen_data, fake)\n",
        "                # print(d_loss_fake)\n",
        "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
        "\n",
        "                # Clip critic weights\n",
        "                # for l in self.critic.layers:\n",
        "                #     weights = l.get_weights()\n",
        "                #     weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
        "                #     l.set_weights(weights)\n",
        "\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "            self.DLOS = np.append(self.DLOS,d_loss[0])\n",
        "            self.GLOS = np.append(self.GLOS,g_loss[0])\n",
        "            # Plot the progress\n",
        "            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch,  1-d_loss[0],  1-g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "        r, c = 6, 5\n",
        "        noise = np.random.normal(0, 1, (c, self.latent_dim))\n",
        "        gen_data = self.generator.predict(noise)\n",
        "        # print(noise)\n",
        "        # gen_data.to_csv(\"gen_data.csv\",index=False)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        # gen_data = 0.5 * gen_data + 0.5\n",
        "        # print(gen_data[0])\n",
        "        print(gen_data) \n",
        "\n",
        "\n",
        "  \n",
        "if __name__ == '__main__':\n",
        "    wgan = WGAN()\n",
        "    ##################################sample interval 50 to 2\n",
        "    ###################################epoch4000 to 400\n",
        "    #################################batch size 32 to 2\n",
        "    wgan.train(epochs=10000, batch_size=100, sample_interval=50)\n",
        "\n",
        "\n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "3793 [D loss: 1.061674] [G loss: 1.266224]\n",
            "3794 [D loss: 1.041411] [G loss: 1.279169]\n",
            "3795 [D loss: 1.060965] [G loss: 1.267452]\n",
            "3796 [D loss: 1.045719] [G loss: 1.267322]\n",
            "3797 [D loss: 1.060578] [G loss: 1.256877]\n",
            "3798 [D loss: 1.055170] [G loss: 1.256411]\n",
            "3799 [D loss: 1.052048] [G loss: 1.265461]\n",
            "3800 [D loss: 1.054270] [G loss: 1.264109]\n",
            "[[0.5515908  0.47205934 0.49678746 0.48326063]\n",
            " [0.6686574  0.41389245 0.620569   0.33860642]\n",
            " [0.54775894 0.4214548  0.48739365 0.54498476]\n",
            " [0.5295304  0.47366482 0.53784055 0.4882913 ]\n",
            " [0.62124306 0.421101   0.5259379  0.4897347 ]]\n",
            "3801 [D loss: 1.039170] [G loss: 1.267849]\n",
            "3802 [D loss: 1.051950] [G loss: 1.274601]\n",
            "3803 [D loss: 1.042219] [G loss: 1.270890]\n",
            "3804 [D loss: 1.064618] [G loss: 1.272559]\n",
            "3805 [D loss: 1.044152] [G loss: 1.274815]\n",
            "3806 [D loss: 1.038158] [G loss: 1.277291]\n",
            "3807 [D loss: 1.058199] [G loss: 1.259179]\n",
            "3808 [D loss: 1.037022] [G loss: 1.267522]\n",
            "3809 [D loss: 1.048789] [G loss: 1.269365]\n",
            "3810 [D loss: 1.049960] [G loss: 1.272335]\n",
            "3811 [D loss: 1.051048] [G loss: 1.271640]\n",
            "3812 [D loss: 1.046274] [G loss: 1.273560]\n",
            "3813 [D loss: 1.056126] [G loss: 1.259643]\n",
            "3814 [D loss: 1.034439] [G loss: 1.263878]\n",
            "3815 [D loss: 1.061080] [G loss: 1.265748]\n",
            "3816 [D loss: 1.064590] [G loss: 1.261553]\n",
            "3817 [D loss: 1.041428] [G loss: 1.270263]\n",
            "3818 [D loss: 1.054521] [G loss: 1.277988]\n",
            "3819 [D loss: 1.057299] [G loss: 1.275899]\n",
            "3820 [D loss: 1.049718] [G loss: 1.261978]\n",
            "3821 [D loss: 1.056234] [G loss: 1.265055]\n",
            "3822 [D loss: 1.065087] [G loss: 1.256751]\n",
            "3823 [D loss: 1.056187] [G loss: 1.260035]\n",
            "3824 [D loss: 1.038899] [G loss: 1.281329]\n",
            "3825 [D loss: 1.048819] [G loss: 1.276310]\n",
            "3826 [D loss: 1.051393] [G loss: 1.264221]\n",
            "3827 [D loss: 1.042637] [G loss: 1.261439]\n",
            "3828 [D loss: 1.047901] [G loss: 1.261738]\n",
            "3829 [D loss: 1.036025] [G loss: 1.273570]\n",
            "3830 [D loss: 1.050360] [G loss: 1.257755]\n",
            "3831 [D loss: 1.051377] [G loss: 1.269235]\n",
            "3832 [D loss: 1.047442] [G loss: 1.273304]\n",
            "3833 [D loss: 1.062646] [G loss: 1.259478]\n",
            "3834 [D loss: 1.050126] [G loss: 1.269248]\n",
            "3835 [D loss: 1.054778] [G loss: 1.266970]\n",
            "3836 [D loss: 1.068655] [G loss: 1.276491]\n",
            "3837 [D loss: 1.071365] [G loss: 1.257346]\n",
            "3838 [D loss: 1.053523] [G loss: 1.273007]\n",
            "3839 [D loss: 1.049306] [G loss: 1.266348]\n",
            "3840 [D loss: 1.053208] [G loss: 1.271070]\n",
            "3841 [D loss: 1.074612] [G loss: 1.258177]\n",
            "3842 [D loss: 1.049489] [G loss: 1.262825]\n",
            "3843 [D loss: 1.059291] [G loss: 1.267112]\n",
            "3844 [D loss: 1.052544] [G loss: 1.270775]\n",
            "3845 [D loss: 1.056435] [G loss: 1.266086]\n",
            "3846 [D loss: 1.050709] [G loss: 1.267855]\n",
            "3847 [D loss: 1.030847] [G loss: 1.282187]\n",
            "3848 [D loss: 1.048180] [G loss: 1.274752]\n",
            "3849 [D loss: 1.047761] [G loss: 1.259525]\n",
            "3850 [D loss: 1.054457] [G loss: 1.273343]\n",
            "[[0.518502   0.49748483 0.5009023  0.48816517]\n",
            " [0.5023115  0.4983512  0.501101   0.50076115]\n",
            " [0.56052005 0.43243262 0.5237512  0.47296354]\n",
            " [0.60243833 0.25424346 0.49377242 0.65925205]\n",
            " [0.5103508  0.45941353 0.51363134 0.5155434 ]]\n",
            "3851 [D loss: 1.046044] [G loss: 1.260508]\n",
            "3852 [D loss: 1.054157] [G loss: 1.252979]\n",
            "3853 [D loss: 1.051851] [G loss: 1.274575]\n",
            "3854 [D loss: 1.070387] [G loss: 1.270835]\n",
            "3855 [D loss: 1.041300] [G loss: 1.262794]\n",
            "3856 [D loss: 1.058403] [G loss: 1.262767]\n",
            "3857 [D loss: 1.051375] [G loss: 1.266886]\n",
            "3858 [D loss: 1.052902] [G loss: 1.271752]\n",
            "3859 [D loss: 1.055354] [G loss: 1.265769]\n",
            "3860 [D loss: 1.056116] [G loss: 1.260813]\n",
            "3861 [D loss: 1.056368] [G loss: 1.267465]\n",
            "3862 [D loss: 1.040192] [G loss: 1.271720]\n",
            "3863 [D loss: 1.053291] [G loss: 1.274640]\n",
            "3864 [D loss: 1.046908] [G loss: 1.273335]\n",
            "3865 [D loss: 1.043442] [G loss: 1.270086]\n",
            "3866 [D loss: 1.054989] [G loss: 1.276353]\n",
            "3867 [D loss: 1.055260] [G loss: 1.270410]\n",
            "3868 [D loss: 1.041513] [G loss: 1.270923]\n",
            "3869 [D loss: 1.061357] [G loss: 1.281928]\n",
            "3870 [D loss: 1.067015] [G loss: 1.256580]\n",
            "3871 [D loss: 1.039599] [G loss: 1.265766]\n",
            "3872 [D loss: 1.048665] [G loss: 1.267005]\n",
            "3873 [D loss: 1.040118] [G loss: 1.265599]\n",
            "3874 [D loss: 1.052939] [G loss: 1.259886]\n",
            "3875 [D loss: 1.055866] [G loss: 1.271627]\n",
            "3876 [D loss: 1.051721] [G loss: 1.259045]\n",
            "3877 [D loss: 1.046157] [G loss: 1.266641]\n",
            "3878 [D loss: 1.044850] [G loss: 1.269248]\n",
            "3879 [D loss: 1.049974] [G loss: 1.266648]\n",
            "3880 [D loss: 1.066801] [G loss: 1.254785]\n",
            "3881 [D loss: 1.064248] [G loss: 1.266492]\n",
            "3882 [D loss: 1.061310] [G loss: 1.266975]\n",
            "3883 [D loss: 1.059061] [G loss: 1.257700]\n",
            "3884 [D loss: 1.050610] [G loss: 1.264433]\n",
            "3885 [D loss: 1.049440] [G loss: 1.257878]\n",
            "3886 [D loss: 1.040379] [G loss: 1.256321]\n",
            "3887 [D loss: 1.066789] [G loss: 1.269694]\n",
            "3888 [D loss: 1.047260] [G loss: 1.265451]\n",
            "3889 [D loss: 1.047456] [G loss: 1.270374]\n",
            "3890 [D loss: 1.053049] [G loss: 1.258940]\n",
            "3891 [D loss: 1.074801] [G loss: 1.260190]\n",
            "3892 [D loss: 1.038581] [G loss: 1.266545]\n",
            "3893 [D loss: 1.045357] [G loss: 1.268975]\n",
            "3894 [D loss: 1.055089] [G loss: 1.263788]\n",
            "3895 [D loss: 1.040658] [G loss: 1.261069]\n",
            "3896 [D loss: 1.049057] [G loss: 1.265599]\n",
            "3897 [D loss: 1.043163] [G loss: 1.265043]\n",
            "3898 [D loss: 1.055419] [G loss: 1.264913]\n",
            "3899 [D loss: 1.050273] [G loss: 1.278431]\n",
            "3900 [D loss: 1.050408] [G loss: 1.265201]\n",
            "[[0.70964855 0.31826213 0.5452908  0.45784238]\n",
            " [0.5035432  0.49917167 0.50230867 0.4996913 ]\n",
            " [0.5222391  0.38816375 0.5404065  0.6092136 ]\n",
            " [0.6269268  0.42295796 0.56376344 0.47963023]\n",
            " [0.6301186  0.3641853  0.5012141  0.5056041 ]]\n",
            "3901 [D loss: 1.057911] [G loss: 1.259679]\n",
            "3902 [D loss: 1.064290] [G loss: 1.262609]\n",
            "3903 [D loss: 1.036866] [G loss: 1.270772]\n",
            "3904 [D loss: 1.055630] [G loss: 1.256258]\n",
            "3905 [D loss: 1.053637] [G loss: 1.262236]\n",
            "3906 [D loss: 1.067024] [G loss: 1.258810]\n",
            "3907 [D loss: 1.039131] [G loss: 1.276304]\n",
            "3908 [D loss: 1.057688] [G loss: 1.254203]\n",
            "3909 [D loss: 1.059795] [G loss: 1.259901]\n",
            "3910 [D loss: 1.039943] [G loss: 1.280929]\n",
            "3911 [D loss: 1.050786] [G loss: 1.271547]\n",
            "3912 [D loss: 1.062807] [G loss: 1.258580]\n",
            "3913 [D loss: 1.055373] [G loss: 1.266736]\n",
            "3914 [D loss: 1.057471] [G loss: 1.265602]\n",
            "3915 [D loss: 1.044679] [G loss: 1.274605]\n",
            "3916 [D loss: 1.055106] [G loss: 1.260325]\n",
            "3917 [D loss: 1.059146] [G loss: 1.265926]\n",
            "3918 [D loss: 1.049716] [G loss: 1.267310]\n",
            "3919 [D loss: 1.054323] [G loss: 1.263232]\n",
            "3920 [D loss: 1.053097] [G loss: 1.253774]\n",
            "3921 [D loss: 1.042646] [G loss: 1.277466]\n",
            "3922 [D loss: 1.045008] [G loss: 1.259529]\n",
            "3923 [D loss: 1.056107] [G loss: 1.267680]\n",
            "3924 [D loss: 1.040991] [G loss: 1.270423]\n",
            "3925 [D loss: 1.067196] [G loss: 1.254616]\n",
            "3926 [D loss: 1.064329] [G loss: 1.267139]\n",
            "3927 [D loss: 1.068314] [G loss: 1.251528]\n",
            "3928 [D loss: 1.035912] [G loss: 1.269317]\n",
            "3929 [D loss: 1.046822] [G loss: 1.270665]\n",
            "3930 [D loss: 1.061557] [G loss: 1.264829]\n",
            "3931 [D loss: 1.049874] [G loss: 1.269149]\n",
            "3932 [D loss: 1.044446] [G loss: 1.272524]\n",
            "3933 [D loss: 1.062509] [G loss: 1.260699]\n",
            "3934 [D loss: 1.043573] [G loss: 1.269129]\n",
            "3935 [D loss: 1.062527] [G loss: 1.258463]\n",
            "3936 [D loss: 1.050656] [G loss: 1.259456]\n",
            "3937 [D loss: 1.052211] [G loss: 1.258815]\n",
            "3938 [D loss: 1.055540] [G loss: 1.269808]\n",
            "3939 [D loss: 1.042515] [G loss: 1.273752]\n",
            "3940 [D loss: 1.043536] [G loss: 1.272142]\n",
            "3941 [D loss: 1.040051] [G loss: 1.275777]\n",
            "3942 [D loss: 1.052528] [G loss: 1.274523]\n",
            "3943 [D loss: 1.037708] [G loss: 1.265707]\n",
            "3944 [D loss: 1.036818] [G loss: 1.271298]\n",
            "3945 [D loss: 1.060371] [G loss: 1.258877]\n",
            "3946 [D loss: 1.054339] [G loss: 1.263426]\n",
            "3947 [D loss: 1.061358] [G loss: 1.257633]\n",
            "3948 [D loss: 1.073036] [G loss: 1.266634]\n",
            "3949 [D loss: 1.045380] [G loss: 1.273938]\n",
            "3950 [D loss: 1.063208] [G loss: 1.263877]\n",
            "[[0.7405953  0.33497378 0.56354827 0.3768519 ]\n",
            " [0.53743577 0.48826662 0.5017601  0.47385597]\n",
            " [0.74550277 0.23846635 0.5401558  0.5274071 ]\n",
            " [0.50073254 0.47390908 0.51221806 0.5165763 ]\n",
            " [0.73347855 0.40819424 0.6601913  0.2528567 ]]\n",
            "3951 [D loss: 1.067630] [G loss: 1.252436]\n",
            "3952 [D loss: 1.067715] [G loss: 1.257410]\n",
            "3953 [D loss: 1.059682] [G loss: 1.259275]\n",
            "3954 [D loss: 1.042820] [G loss: 1.265687]\n",
            "3955 [D loss: 1.059548] [G loss: 1.258321]\n",
            "3956 [D loss: 1.063162] [G loss: 1.263866]\n",
            "3957 [D loss: 1.053971] [G loss: 1.270546]\n",
            "3958 [D loss: 1.044830] [G loss: 1.272968]\n",
            "3959 [D loss: 1.047961] [G loss: 1.266316]\n",
            "3960 [D loss: 1.059243] [G loss: 1.263279]\n",
            "3961 [D loss: 1.045088] [G loss: 1.261108]\n",
            "3962 [D loss: 1.037911] [G loss: 1.273529]\n",
            "3963 [D loss: 1.051964] [G loss: 1.261423]\n",
            "3964 [D loss: 1.059922] [G loss: 1.248724]\n",
            "3965 [D loss: 1.046289] [G loss: 1.271837]\n",
            "3966 [D loss: 1.048090] [G loss: 1.270437]\n",
            "3967 [D loss: 1.056429] [G loss: 1.268043]\n",
            "3968 [D loss: 1.044597] [G loss: 1.264871]\n",
            "3969 [D loss: 1.047380] [G loss: 1.281984]\n",
            "3970 [D loss: 1.037833] [G loss: 1.265641]\n",
            "3971 [D loss: 1.067150] [G loss: 1.266798]\n",
            "3972 [D loss: 1.059153] [G loss: 1.255594]\n",
            "3973 [D loss: 1.045759] [G loss: 1.254481]\n",
            "3974 [D loss: 1.040925] [G loss: 1.266283]\n",
            "3975 [D loss: 1.048189] [G loss: 1.258577]\n",
            "3976 [D loss: 1.049235] [G loss: 1.268618]\n",
            "3977 [D loss: 1.049326] [G loss: 1.265343]\n",
            "3978 [D loss: 1.064365] [G loss: 1.263540]\n",
            "3979 [D loss: 1.059632] [G loss: 1.258582]\n",
            "3980 [D loss: 1.048016] [G loss: 1.265024]\n",
            "3981 [D loss: 1.059168] [G loss: 1.265282]\n",
            "3982 [D loss: 1.044336] [G loss: 1.266748]\n",
            "3983 [D loss: 1.054779] [G loss: 1.251746]\n",
            "3984 [D loss: 1.064745] [G loss: 1.275738]\n",
            "3985 [D loss: 1.060615] [G loss: 1.267500]\n",
            "3986 [D loss: 1.054849] [G loss: 1.276115]\n",
            "3987 [D loss: 1.046611] [G loss: 1.253034]\n",
            "3988 [D loss: 1.066334] [G loss: 1.248385]\n",
            "3989 [D loss: 1.063803] [G loss: 1.261814]\n",
            "3990 [D loss: 1.053669] [G loss: 1.268287]\n",
            "3991 [D loss: 1.041967] [G loss: 1.273950]\n",
            "3992 [D loss: 1.059904] [G loss: 1.249482]\n",
            "3993 [D loss: 1.059077] [G loss: 1.265245]\n",
            "3994 [D loss: 1.069274] [G loss: 1.257242]\n",
            "3995 [D loss: 1.043386] [G loss: 1.259250]\n",
            "3996 [D loss: 1.058535] [G loss: 1.254121]\n",
            "3997 [D loss: 1.048634] [G loss: 1.256064]\n",
            "3998 [D loss: 1.060281] [G loss: 1.248255]\n",
            "3999 [D loss: 1.047312] [G loss: 1.267404]\n",
            "4000 [D loss: 1.058391] [G loss: 1.256593]\n",
            "[[0.7842307  0.28070283 0.57961935 0.41409653]\n",
            " [0.58934355 0.4077451  0.5113792  0.48302147]\n",
            " [0.63105386 0.39325607 0.52900165 0.46448538]\n",
            " [0.502502   0.49484742 0.50218236 0.50264835]\n",
            " [0.55183804 0.42617616 0.5056266  0.5261943 ]]\n",
            "4001 [D loss: 1.050104] [G loss: 1.259095]\n",
            "4002 [D loss: 1.062886] [G loss: 1.254639]\n",
            "4003 [D loss: 1.047388] [G loss: 1.249925]\n",
            "4004 [D loss: 1.047951] [G loss: 1.263598]\n",
            "4005 [D loss: 1.051312] [G loss: 1.268424]\n",
            "4006 [D loss: 1.067494] [G loss: 1.258792]\n",
            "4007 [D loss: 1.048982] [G loss: 1.265071]\n",
            "4008 [D loss: 1.041281] [G loss: 1.269546]\n",
            "4009 [D loss: 1.054617] [G loss: 1.259598]\n",
            "4010 [D loss: 1.058990] [G loss: 1.259272]\n",
            "4011 [D loss: 1.056747] [G loss: 1.266684]\n",
            "4012 [D loss: 1.054233] [G loss: 1.260630]\n",
            "4013 [D loss: 1.049853] [G loss: 1.257290]\n",
            "4014 [D loss: 1.055616] [G loss: 1.262089]\n",
            "4015 [D loss: 1.044580] [G loss: 1.271288]\n",
            "4016 [D loss: 1.057194] [G loss: 1.251713]\n",
            "4017 [D loss: 1.028781] [G loss: 1.267798]\n",
            "4018 [D loss: 1.057143] [G loss: 1.258325]\n",
            "4019 [D loss: 1.057950] [G loss: 1.259533]\n",
            "4020 [D loss: 1.062194] [G loss: 1.263483]\n",
            "4021 [D loss: 1.057394] [G loss: 1.256306]\n",
            "4022 [D loss: 1.061879] [G loss: 1.255529]\n",
            "4023 [D loss: 1.053951] [G loss: 1.259241]\n",
            "4024 [D loss: 1.067887] [G loss: 1.268885]\n",
            "4025 [D loss: 1.054552] [G loss: 1.259941]\n",
            "4026 [D loss: 1.057128] [G loss: 1.263878]\n",
            "4027 [D loss: 1.031604] [G loss: 1.273462]\n",
            "4028 [D loss: 1.050023] [G loss: 1.270448]\n",
            "4029 [D loss: 1.046029] [G loss: 1.266859]\n",
            "4030 [D loss: 1.058538] [G loss: 1.259661]\n",
            "4031 [D loss: 1.044953] [G loss: 1.271804]\n",
            "4032 [D loss: 1.054148] [G loss: 1.269041]\n",
            "4033 [D loss: 1.047272] [G loss: 1.260170]\n",
            "4034 [D loss: 1.046290] [G loss: 1.264418]\n",
            "4035 [D loss: 1.059392] [G loss: 1.261922]\n",
            "4036 [D loss: 1.042014] [G loss: 1.266548]\n",
            "4037 [D loss: 1.051525] [G loss: 1.259766]\n",
            "4038 [D loss: 1.058163] [G loss: 1.255388]\n",
            "4039 [D loss: 1.061144] [G loss: 1.264331]\n",
            "4040 [D loss: 1.068804] [G loss: 1.255585]\n",
            "4041 [D loss: 1.038215] [G loss: 1.272236]\n",
            "4042 [D loss: 1.042273] [G loss: 1.264287]\n",
            "4043 [D loss: 1.057730] [G loss: 1.260519]\n",
            "4044 [D loss: 1.050991] [G loss: 1.273013]\n",
            "4045 [D loss: 1.042174] [G loss: 1.260405]\n",
            "4046 [D loss: 1.057916] [G loss: 1.257422]\n",
            "4047 [D loss: 1.040495] [G loss: 1.266040]\n",
            "4048 [D loss: 1.062842] [G loss: 1.257564]\n",
            "4049 [D loss: 1.067766] [G loss: 1.266383]\n",
            "4050 [D loss: 1.070072] [G loss: 1.251250]\n",
            "[[0.67336416 0.41163665 0.58641994 0.3690413 ]\n",
            " [0.5018937  0.49837178 0.50159156 0.50148296]\n",
            " [0.7988666  0.2675456  0.6788325  0.24923465]\n",
            " [0.65360427 0.34008    0.4886631  0.5242944 ]\n",
            " [0.50686157 0.5010977  0.5034024  0.49639118]]\n",
            "4051 [D loss: 1.056591] [G loss: 1.268332]\n",
            "4052 [D loss: 1.046260] [G loss: 1.272647]\n",
            "4053 [D loss: 1.063657] [G loss: 1.260902]\n",
            "4054 [D loss: 1.056561] [G loss: 1.251194]\n",
            "4055 [D loss: 1.062199] [G loss: 1.263907]\n",
            "4056 [D loss: 1.057800] [G loss: 1.253961]\n",
            "4057 [D loss: 1.062405] [G loss: 1.262736]\n",
            "4058 [D loss: 1.044165] [G loss: 1.252823]\n",
            "4059 [D loss: 1.056265] [G loss: 1.272574]\n",
            "4060 [D loss: 1.052317] [G loss: 1.263450]\n",
            "4061 [D loss: 1.044283] [G loss: 1.253803]\n",
            "4062 [D loss: 1.068547] [G loss: 1.255137]\n",
            "4063 [D loss: 1.050862] [G loss: 1.259749]\n",
            "4064 [D loss: 1.048988] [G loss: 1.255273]\n",
            "4065 [D loss: 1.067079] [G loss: 1.260814]\n",
            "4066 [D loss: 1.048451] [G loss: 1.257414]\n",
            "4067 [D loss: 1.041163] [G loss: 1.265388]\n",
            "4068 [D loss: 1.056936] [G loss: 1.267390]\n",
            "4069 [D loss: 1.056556] [G loss: 1.266190]\n",
            "4070 [D loss: 1.056691] [G loss: 1.264305]\n",
            "4071 [D loss: 1.059639] [G loss: 1.260258]\n",
            "4072 [D loss: 1.029875] [G loss: 1.269983]\n",
            "4073 [D loss: 1.059278] [G loss: 1.249582]\n",
            "4074 [D loss: 1.065066] [G loss: 1.260221]\n",
            "4075 [D loss: 1.071338] [G loss: 1.263167]\n",
            "4076 [D loss: 1.066352] [G loss: 1.254618]\n",
            "4077 [D loss: 1.050443] [G loss: 1.258871]\n",
            "4078 [D loss: 1.059664] [G loss: 1.249938]\n",
            "4079 [D loss: 1.046126] [G loss: 1.266852]\n",
            "4080 [D loss: 1.052957] [G loss: 1.251748]\n",
            "4081 [D loss: 1.064718] [G loss: 1.262131]\n",
            "4082 [D loss: 1.053784] [G loss: 1.257649]\n",
            "4083 [D loss: 1.059318] [G loss: 1.258486]\n",
            "4084 [D loss: 1.043906] [G loss: 1.271754]\n",
            "4085 [D loss: 1.039954] [G loss: 1.268112]\n",
            "4086 [D loss: 1.074818] [G loss: 1.259554]\n",
            "4087 [D loss: 1.065914] [G loss: 1.260179]\n",
            "4088 [D loss: 1.060519] [G loss: 1.261421]\n",
            "4089 [D loss: 1.071664] [G loss: 1.257888]\n",
            "4090 [D loss: 1.072542] [G loss: 1.248717]\n",
            "4091 [D loss: 1.055501] [G loss: 1.259570]\n",
            "4092 [D loss: 1.048241] [G loss: 1.257515]\n",
            "4093 [D loss: 1.042850] [G loss: 1.254508]\n",
            "4094 [D loss: 1.051527] [G loss: 1.264510]\n",
            "4095 [D loss: 1.044998] [G loss: 1.263315]\n",
            "4096 [D loss: 1.051066] [G loss: 1.267694]\n",
            "4097 [D loss: 1.057820] [G loss: 1.260662]\n",
            "4098 [D loss: 1.057046] [G loss: 1.262359]\n",
            "4099 [D loss: 1.046025] [G loss: 1.270734]\n",
            "4100 [D loss: 1.053633] [G loss: 1.263266]\n",
            "[[0.76718867 0.4166033  0.6819925  0.22059336]\n",
            " [0.64257693 0.37257645 0.48843923 0.46710205]\n",
            " [0.8820319  0.20604154 0.70995176 0.24978781]\n",
            " [0.5089147  0.5023235  0.5041121  0.49445575]\n",
            " [0.6478799  0.41949022 0.5839638  0.3911391 ]]\n",
            "4101 [D loss: 1.060750] [G loss: 1.269976]\n",
            "4102 [D loss: 1.049781] [G loss: 1.250666]\n",
            "4103 [D loss: 1.061256] [G loss: 1.251282]\n",
            "4104 [D loss: 1.063178] [G loss: 1.253135]\n",
            "4105 [D loss: 1.053843] [G loss: 1.267739]\n",
            "4106 [D loss: 1.059892] [G loss: 1.267798]\n",
            "4107 [D loss: 1.057925] [G loss: 1.265018]\n",
            "4108 [D loss: 1.067064] [G loss: 1.256239]\n",
            "4109 [D loss: 1.067353] [G loss: 1.254912]\n",
            "4110 [D loss: 1.043301] [G loss: 1.266675]\n",
            "4111 [D loss: 1.061801] [G loss: 1.253988]\n",
            "4112 [D loss: 1.059358] [G loss: 1.259949]\n",
            "4113 [D loss: 1.051939] [G loss: 1.253801]\n",
            "4114 [D loss: 1.077356] [G loss: 1.257044]\n",
            "4115 [D loss: 1.058416] [G loss: 1.250560]\n",
            "4116 [D loss: 1.048678] [G loss: 1.265053]\n",
            "4117 [D loss: 1.069414] [G loss: 1.245037]\n",
            "4118 [D loss: 1.050565] [G loss: 1.253507]\n",
            "4119 [D loss: 1.064153] [G loss: 1.250088]\n",
            "4120 [D loss: 1.048463] [G loss: 1.249182]\n",
            "4121 [D loss: 1.043134] [G loss: 1.260572]\n",
            "4122 [D loss: 1.076796] [G loss: 1.245246]\n",
            "4123 [D loss: 1.054685] [G loss: 1.267720]\n",
            "4124 [D loss: 1.052493] [G loss: 1.249319]\n",
            "4125 [D loss: 1.070548] [G loss: 1.252150]\n",
            "4126 [D loss: 1.069497] [G loss: 1.250266]\n",
            "4127 [D loss: 1.062031] [G loss: 1.251859]\n",
            "4128 [D loss: 1.041461] [G loss: 1.279144]\n",
            "4129 [D loss: 1.051831] [G loss: 1.258708]\n",
            "4130 [D loss: 1.040340] [G loss: 1.266527]\n",
            "4131 [D loss: 1.057637] [G loss: 1.252083]\n",
            "4132 [D loss: 1.057590] [G loss: 1.264411]\n",
            "4133 [D loss: 1.039712] [G loss: 1.266531]\n",
            "4134 [D loss: 1.059492] [G loss: 1.251849]\n",
            "4135 [D loss: 1.052818] [G loss: 1.272821]\n",
            "4136 [D loss: 1.058902] [G loss: 1.261182]\n",
            "4137 [D loss: 1.052616] [G loss: 1.259420]\n",
            "4138 [D loss: 1.064749] [G loss: 1.253271]\n",
            "4139 [D loss: 1.068589] [G loss: 1.258363]\n",
            "4140 [D loss: 1.058959] [G loss: 1.250170]\n",
            "4141 [D loss: 1.050502] [G loss: 1.266507]\n",
            "4142 [D loss: 1.052505] [G loss: 1.265451]\n",
            "4143 [D loss: 1.049910] [G loss: 1.253607]\n",
            "4144 [D loss: 1.056592] [G loss: 1.254167]\n",
            "4145 [D loss: 1.069098] [G loss: 1.247813]\n",
            "4146 [D loss: 1.066320] [G loss: 1.254468]\n",
            "4147 [D loss: 1.062495] [G loss: 1.249623]\n",
            "4148 [D loss: 1.069261] [G loss: 1.262444]\n",
            "4149 [D loss: 1.052297] [G loss: 1.252244]\n",
            "4150 [D loss: 1.054266] [G loss: 1.264872]\n",
            "[[0.6574342  0.37422645 0.577946   0.3871214 ]\n",
            " [0.77932316 0.3068935  0.5274687  0.3764021 ]\n",
            " [0.8853765  0.19298013 0.6526718  0.29470804]\n",
            " [0.5103232  0.45062733 0.5293583  0.5360064 ]\n",
            " [0.55589414 0.4553703  0.51167816 0.48759422]]\n",
            "4151 [D loss: 1.057116] [G loss: 1.258492]\n",
            "4152 [D loss: 1.040202] [G loss: 1.259276]\n",
            "4153 [D loss: 1.066645] [G loss: 1.245433]\n",
            "4154 [D loss: 1.057305] [G loss: 1.260988]\n",
            "4155 [D loss: 1.036031] [G loss: 1.266770]\n",
            "4156 [D loss: 1.059972] [G loss: 1.253015]\n",
            "4157 [D loss: 1.065937] [G loss: 1.246918]\n",
            "4158 [D loss: 1.062455] [G loss: 1.258703]\n",
            "4159 [D loss: 1.059922] [G loss: 1.256883]\n",
            "4160 [D loss: 1.056300] [G loss: 1.262781]\n",
            "4161 [D loss: 1.060841] [G loss: 1.265231]\n",
            "4162 [D loss: 1.065490] [G loss: 1.266374]\n",
            "4163 [D loss: 1.052899] [G loss: 1.253460]\n",
            "4164 [D loss: 1.066931] [G loss: 1.260436]\n",
            "4165 [D loss: 1.080822] [G loss: 1.247826]\n",
            "4166 [D loss: 1.067506] [G loss: 1.252432]\n",
            "4167 [D loss: 1.061627] [G loss: 1.267526]\n",
            "4168 [D loss: 1.052507] [G loss: 1.262945]\n",
            "4169 [D loss: 1.052806] [G loss: 1.256897]\n",
            "4170 [D loss: 1.052936] [G loss: 1.264995]\n",
            "4171 [D loss: 1.048831] [G loss: 1.266658]\n",
            "4172 [D loss: 1.054262] [G loss: 1.263308]\n",
            "4173 [D loss: 1.064027] [G loss: 1.258715]\n",
            "4174 [D loss: 1.069194] [G loss: 1.257331]\n",
            "4175 [D loss: 1.062139] [G loss: 1.256119]\n",
            "4176 [D loss: 1.062355] [G loss: 1.253555]\n",
            "4177 [D loss: 1.058265] [G loss: 1.260336]\n",
            "4178 [D loss: 1.064263] [G loss: 1.255527]\n",
            "4179 [D loss: 1.039104] [G loss: 1.268271]\n",
            "4180 [D loss: 1.053386] [G loss: 1.252414]\n",
            "4181 [D loss: 1.065502] [G loss: 1.251061]\n",
            "4182 [D loss: 1.064446] [G loss: 1.245269]\n",
            "4183 [D loss: 1.048579] [G loss: 1.253509]\n",
            "4184 [D loss: 1.061713] [G loss: 1.246609]\n",
            "4185 [D loss: 1.039992] [G loss: 1.268396]\n",
            "4186 [D loss: 1.050417] [G loss: 1.262331]\n",
            "4187 [D loss: 1.062760] [G loss: 1.258828]\n",
            "4188 [D loss: 1.047118] [G loss: 1.259303]\n",
            "4189 [D loss: 1.050768] [G loss: 1.263332]\n",
            "4190 [D loss: 1.038034] [G loss: 1.264988]\n",
            "4191 [D loss: 1.047802] [G loss: 1.258173]\n",
            "4192 [D loss: 1.051233] [G loss: 1.246306]\n",
            "4193 [D loss: 1.043267] [G loss: 1.270877]\n",
            "4194 [D loss: 1.026574] [G loss: 1.261195]\n",
            "4195 [D loss: 1.067482] [G loss: 1.251181]\n",
            "4196 [D loss: 1.052897] [G loss: 1.263443]\n",
            "4197 [D loss: 1.075446] [G loss: 1.245016]\n",
            "4198 [D loss: 1.051332] [G loss: 1.259894]\n",
            "4199 [D loss: 1.053137] [G loss: 1.252450]\n",
            "4200 [D loss: 1.032179] [G loss: 1.268873]\n",
            "[[0.549412   0.45831844 0.5177459  0.4791763 ]\n",
            " [0.5020221  0.4982876  0.5016333  0.501442  ]\n",
            " [0.61306465 0.38630617 0.6278859  0.47184023]\n",
            " [0.8747037  0.33217326 0.7381366  0.19562979]\n",
            " [0.6393604  0.3482289  0.56169254 0.42462888]]\n",
            "4201 [D loss: 1.055321] [G loss: 1.265703]\n",
            "4202 [D loss: 1.048755] [G loss: 1.266569]\n",
            "4203 [D loss: 1.042177] [G loss: 1.260540]\n",
            "4204 [D loss: 1.057912] [G loss: 1.258864]\n",
            "4205 [D loss: 1.077436] [G loss: 1.249474]\n",
            "4206 [D loss: 1.047094] [G loss: 1.253221]\n",
            "4207 [D loss: 1.061891] [G loss: 1.258147]\n",
            "4208 [D loss: 1.070165] [G loss: 1.261707]\n",
            "4209 [D loss: 1.049295] [G loss: 1.262540]\n",
            "4210 [D loss: 1.043933] [G loss: 1.262323]\n",
            "4211 [D loss: 1.063462] [G loss: 1.254299]\n",
            "4212 [D loss: 1.059953] [G loss: 1.255818]\n",
            "4213 [D loss: 1.061786] [G loss: 1.250733]\n",
            "4214 [D loss: 1.059791] [G loss: 1.256265]\n",
            "4215 [D loss: 1.063967] [G loss: 1.248836]\n",
            "4216 [D loss: 1.055346] [G loss: 1.249353]\n",
            "4217 [D loss: 1.066645] [G loss: 1.251470]\n",
            "4218 [D loss: 1.062852] [G loss: 1.248645]\n",
            "4219 [D loss: 1.062945] [G loss: 1.259212]\n",
            "4220 [D loss: 1.058741] [G loss: 1.260184]\n",
            "4221 [D loss: 1.055772] [G loss: 1.256933]\n",
            "4222 [D loss: 1.065614] [G loss: 1.250943]\n",
            "4223 [D loss: 1.050030] [G loss: 1.249406]\n",
            "4224 [D loss: 1.050654] [G loss: 1.267293]\n",
            "4225 [D loss: 1.058829] [G loss: 1.245272]\n",
            "4226 [D loss: 1.042287] [G loss: 1.259078]\n",
            "4227 [D loss: 1.039480] [G loss: 1.272052]\n",
            "4228 [D loss: 1.058911] [G loss: 1.249584]\n",
            "4229 [D loss: 1.067260] [G loss: 1.245534]\n",
            "4230 [D loss: 1.054555] [G loss: 1.263070]\n",
            "4231 [D loss: 1.075940] [G loss: 1.253952]\n",
            "4232 [D loss: 1.046949] [G loss: 1.264253]\n",
            "4233 [D loss: 1.064080] [G loss: 1.248199]\n",
            "4234 [D loss: 1.057874] [G loss: 1.259370]\n",
            "4235 [D loss: 1.036551] [G loss: 1.256878]\n",
            "4236 [D loss: 1.048623] [G loss: 1.269010]\n",
            "4237 [D loss: 1.054365] [G loss: 1.253565]\n",
            "4238 [D loss: 1.049723] [G loss: 1.257896]\n",
            "4239 [D loss: 1.057542] [G loss: 1.260536]\n",
            "4240 [D loss: 1.063794] [G loss: 1.257747]\n",
            "4241 [D loss: 1.046025] [G loss: 1.261013]\n",
            "4242 [D loss: 1.061316] [G loss: 1.250051]\n",
            "4243 [D loss: 1.060187] [G loss: 1.252142]\n",
            "4244 [D loss: 1.053638] [G loss: 1.264492]\n",
            "4245 [D loss: 1.062526] [G loss: 1.257496]\n",
            "4246 [D loss: 1.061007] [G loss: 1.250452]\n",
            "4247 [D loss: 1.048439] [G loss: 1.255053]\n",
            "4248 [D loss: 1.055536] [G loss: 1.256537]\n",
            "4249 [D loss: 1.045726] [G loss: 1.255299]\n",
            "4250 [D loss: 1.049532] [G loss: 1.257307]\n",
            "[[0.53354514 0.35661218 0.54689735 0.59563386]\n",
            " [0.53469723 0.42884842 0.5197379  0.5467557 ]\n",
            " [0.62336576 0.3849259  0.5363407  0.45224535]\n",
            " [0.5065972  0.49645486 0.50056565 0.500336  ]\n",
            " [0.837293   0.23927538 0.65457654 0.326714  ]]\n",
            "4251 [D loss: 1.045759] [G loss: 1.256385]\n",
            "4252 [D loss: 1.058209] [G loss: 1.260954]\n",
            "4253 [D loss: 1.073634] [G loss: 1.248228]\n",
            "4254 [D loss: 1.056020] [G loss: 1.263314]\n",
            "4255 [D loss: 1.059938] [G loss: 1.251407]\n",
            "4256 [D loss: 1.060695] [G loss: 1.252627]\n",
            "4257 [D loss: 1.055058] [G loss: 1.254145]\n",
            "4258 [D loss: 1.077255] [G loss: 1.245406]\n",
            "4259 [D loss: 1.070118] [G loss: 1.249265]\n",
            "4260 [D loss: 1.061234] [G loss: 1.248997]\n",
            "4261 [D loss: 1.068378] [G loss: 1.248863]\n",
            "4262 [D loss: 1.060438] [G loss: 1.244195]\n",
            "4263 [D loss: 1.043893] [G loss: 1.259618]\n",
            "4264 [D loss: 1.047457] [G loss: 1.261540]\n",
            "4265 [D loss: 1.067199] [G loss: 1.262494]\n",
            "4266 [D loss: 1.057189] [G loss: 1.261034]\n",
            "4267 [D loss: 1.056933] [G loss: 1.261150]\n",
            "4268 [D loss: 1.052280] [G loss: 1.251862]\n",
            "4269 [D loss: 1.058995] [G loss: 1.265102]\n",
            "4270 [D loss: 1.052833] [G loss: 1.251031]\n",
            "4271 [D loss: 1.062004] [G loss: 1.267022]\n",
            "4272 [D loss: 1.059352] [G loss: 1.251107]\n",
            "4273 [D loss: 1.057098] [G loss: 1.246651]\n",
            "4274 [D loss: 1.052528] [G loss: 1.264202]\n",
            "4275 [D loss: 1.054631] [G loss: 1.250036]\n",
            "4276 [D loss: 1.040083] [G loss: 1.264410]\n",
            "4277 [D loss: 1.062918] [G loss: 1.246025]\n",
            "4278 [D loss: 1.050098] [G loss: 1.243570]\n",
            "4279 [D loss: 1.057586] [G loss: 1.245366]\n",
            "4280 [D loss: 1.060523] [G loss: 1.245955]\n",
            "4281 [D loss: 1.067047] [G loss: 1.254897]\n",
            "4282 [D loss: 1.056154] [G loss: 1.263995]\n",
            "4283 [D loss: 1.057030] [G loss: 1.254902]\n",
            "4284 [D loss: 1.069156] [G loss: 1.253282]\n",
            "4285 [D loss: 1.055358] [G loss: 1.262639]\n",
            "4286 [D loss: 1.053202] [G loss: 1.260704]\n",
            "4287 [D loss: 1.046964] [G loss: 1.273721]\n",
            "4288 [D loss: 1.075200] [G loss: 1.246312]\n",
            "4289 [D loss: 1.047124] [G loss: 1.253911]\n",
            "4290 [D loss: 1.046276] [G loss: 1.262648]\n",
            "4291 [D loss: 1.045281] [G loss: 1.260507]\n",
            "4292 [D loss: 1.044820] [G loss: 1.255691]\n",
            "4293 [D loss: 1.055280] [G loss: 1.253328]\n",
            "4294 [D loss: 1.046433] [G loss: 1.251071]\n",
            "4295 [D loss: 1.061474] [G loss: 1.256439]\n",
            "4296 [D loss: 1.048317] [G loss: 1.247680]\n",
            "4297 [D loss: 1.051667] [G loss: 1.250886]\n",
            "4298 [D loss: 1.043855] [G loss: 1.254563]\n",
            "4299 [D loss: 1.050636] [G loss: 1.253710]\n",
            "4300 [D loss: 1.074182] [G loss: 1.243765]\n",
            "[[0.8651675  0.3325901  0.7930567  0.10510235]\n",
            " [0.6312862  0.3653367  0.5156159  0.52268124]\n",
            " [0.51322985 0.49249673 0.5011983  0.50161964]\n",
            " [0.5606775  0.4311329  0.5624206  0.5011845 ]\n",
            " [0.69319916 0.39236966 0.60199356 0.35934484]]\n",
            "4301 [D loss: 1.034550] [G loss: 1.261681]\n",
            "4302 [D loss: 1.061274] [G loss: 1.258884]\n",
            "4303 [D loss: 1.059442] [G loss: 1.261837]\n",
            "4304 [D loss: 1.061542] [G loss: 1.255291]\n",
            "4305 [D loss: 1.061922] [G loss: 1.257025]\n",
            "4306 [D loss: 1.039761] [G loss: 1.271451]\n",
            "4307 [D loss: 1.066795] [G loss: 1.240277]\n",
            "4308 [D loss: 1.047467] [G loss: 1.253613]\n",
            "4309 [D loss: 1.073906] [G loss: 1.234774]\n",
            "4310 [D loss: 1.052978] [G loss: 1.268239]\n",
            "4311 [D loss: 1.056692] [G loss: 1.246771]\n",
            "4312 [D loss: 1.033590] [G loss: 1.251983]\n",
            "4313 [D loss: 1.044480] [G loss: 1.269625]\n",
            "4314 [D loss: 1.050545] [G loss: 1.256706]\n",
            "4315 [D loss: 1.055129] [G loss: 1.258469]\n",
            "4316 [D loss: 1.049935] [G loss: 1.257368]\n",
            "4317 [D loss: 1.045476] [G loss: 1.250033]\n",
            "4318 [D loss: 1.052919] [G loss: 1.253778]\n",
            "4319 [D loss: 1.058812] [G loss: 1.261211]\n",
            "4320 [D loss: 1.061128] [G loss: 1.259462]\n",
            "4321 [D loss: 1.069994] [G loss: 1.249948]\n",
            "4322 [D loss: 1.060083] [G loss: 1.245356]\n",
            "4323 [D loss: 1.056750] [G loss: 1.259394]\n",
            "4324 [D loss: 1.072752] [G loss: 1.242022]\n",
            "4325 [D loss: 1.057027] [G loss: 1.248282]\n",
            "4326 [D loss: 1.058609] [G loss: 1.261212]\n",
            "4327 [D loss: 1.065896] [G loss: 1.255068]\n",
            "4328 [D loss: 1.065848] [G loss: 1.246505]\n",
            "4329 [D loss: 1.054043] [G loss: 1.259851]\n",
            "4330 [D loss: 1.044979] [G loss: 1.243447]\n",
            "4331 [D loss: 1.064084] [G loss: 1.253767]\n",
            "4332 [D loss: 1.064757] [G loss: 1.250228]\n",
            "4333 [D loss: 1.066518] [G loss: 1.255211]\n",
            "4334 [D loss: 1.057432] [G loss: 1.259832]\n",
            "4335 [D loss: 1.049809] [G loss: 1.254149]\n",
            "4336 [D loss: 1.054429] [G loss: 1.258182]\n",
            "4337 [D loss: 1.051432] [G loss: 1.261359]\n",
            "4338 [D loss: 1.060872] [G loss: 1.254716]\n",
            "4339 [D loss: 1.062818] [G loss: 1.252505]\n",
            "4340 [D loss: 1.056019] [G loss: 1.246939]\n",
            "4341 [D loss: 1.073730] [G loss: 1.245837]\n",
            "4342 [D loss: 1.042576] [G loss: 1.260526]\n",
            "4343 [D loss: 1.064262] [G loss: 1.259445]\n",
            "4344 [D loss: 1.060258] [G loss: 1.247255]\n",
            "4345 [D loss: 1.065309] [G loss: 1.259213]\n",
            "4346 [D loss: 1.048648] [G loss: 1.260614]\n",
            "4347 [D loss: 1.051122] [G loss: 1.256937]\n",
            "4348 [D loss: 1.061303] [G loss: 1.253076]\n",
            "4349 [D loss: 1.063536] [G loss: 1.246535]\n",
            "4350 [D loss: 1.058416] [G loss: 1.243800]\n",
            "[[0.5021519  0.49820372 0.50167435 0.5013916 ]\n",
            " [0.50310487 0.49814588 0.50220054 0.5007282 ]\n",
            " [0.7039073  0.3136498  0.5495996  0.49718532]\n",
            " [0.5082518  0.49440312 0.50164205 0.5035752 ]\n",
            " [0.67754143 0.39039347 0.5802246  0.4016479 ]]\n",
            "4351 [D loss: 1.057037] [G loss: 1.247150]\n",
            "4352 [D loss: 1.052330] [G loss: 1.256995]\n",
            "4353 [D loss: 1.046230] [G loss: 1.250601]\n",
            "4354 [D loss: 1.082723] [G loss: 1.246448]\n",
            "4355 [D loss: 1.044363] [G loss: 1.251896]\n",
            "4356 [D loss: 1.070102] [G loss: 1.232982]\n",
            "4357 [D loss: 1.057988] [G loss: 1.254486]\n",
            "4358 [D loss: 1.051190] [G loss: 1.232992]\n",
            "4359 [D loss: 1.052079] [G loss: 1.253131]\n",
            "4360 [D loss: 1.077945] [G loss: 1.253423]\n",
            "4361 [D loss: 1.046407] [G loss: 1.248062]\n",
            "4362 [D loss: 1.066384] [G loss: 1.254957]\n",
            "4363 [D loss: 1.053196] [G loss: 1.245665]\n",
            "4364 [D loss: 1.067483] [G loss: 1.254078]\n",
            "4365 [D loss: 1.054591] [G loss: 1.254766]\n",
            "4366 [D loss: 1.063963] [G loss: 1.248975]\n",
            "4367 [D loss: 1.044074] [G loss: 1.258612]\n",
            "4368 [D loss: 1.057837] [G loss: 1.262597]\n",
            "4369 [D loss: 1.045657] [G loss: 1.262147]\n",
            "4370 [D loss: 1.059611] [G loss: 1.247440]\n",
            "4371 [D loss: 1.062409] [G loss: 1.248033]\n",
            "4372 [D loss: 1.054277] [G loss: 1.252575]\n",
            "4373 [D loss: 1.071639] [G loss: 1.248136]\n",
            "4374 [D loss: 1.069110] [G loss: 1.252500]\n",
            "4375 [D loss: 1.053943] [G loss: 1.252550]\n",
            "4376 [D loss: 1.051174] [G loss: 1.250557]\n",
            "4377 [D loss: 1.055954] [G loss: 1.245806]\n",
            "4378 [D loss: 1.045614] [G loss: 1.270032]\n",
            "4379 [D loss: 1.059636] [G loss: 1.248237]\n",
            "4380 [D loss: 1.044915] [G loss: 1.260811]\n",
            "4381 [D loss: 1.065653] [G loss: 1.252780]\n",
            "4382 [D loss: 1.048628] [G loss: 1.256843]\n",
            "4383 [D loss: 1.077476] [G loss: 1.245795]\n",
            "4384 [D loss: 1.055885] [G loss: 1.243996]\n",
            "4385 [D loss: 1.064992] [G loss: 1.254809]\n",
            "4386 [D loss: 1.046029] [G loss: 1.253738]\n",
            "4387 [D loss: 1.060898] [G loss: 1.245519]\n",
            "4388 [D loss: 1.057992] [G loss: 1.256009]\n",
            "4389 [D loss: 1.053770] [G loss: 1.256635]\n",
            "4390 [D loss: 1.055423] [G loss: 1.270007]\n",
            "4391 [D loss: 1.048796] [G loss: 1.250155]\n",
            "4392 [D loss: 1.069951] [G loss: 1.248497]\n",
            "4393 [D loss: 1.050359] [G loss: 1.266212]\n",
            "4394 [D loss: 1.049633] [G loss: 1.253437]\n",
            "4395 [D loss: 1.050505] [G loss: 1.250500]\n",
            "4396 [D loss: 1.066438] [G loss: 1.256039]\n",
            "4397 [D loss: 1.061110] [G loss: 1.247630]\n",
            "4398 [D loss: 1.065533] [G loss: 1.253913]\n",
            "4399 [D loss: 1.071753] [G loss: 1.251868]\n",
            "4400 [D loss: 1.050961] [G loss: 1.256350]\n",
            "[[0.8825475  0.24254194 0.68262994 0.19659373]\n",
            " [0.5128901  0.496884   0.50210047 0.49119467]\n",
            " [0.5021955  0.49817622 0.501688   0.5013734 ]\n",
            " [0.50585806 0.48780075 0.5045257  0.5058061 ]\n",
            " [0.5177575  0.44011557 0.52546257 0.57934505]]\n",
            "4401 [D loss: 1.049876] [G loss: 1.260574]\n",
            "4402 [D loss: 1.068704] [G loss: 1.244890]\n",
            "4403 [D loss: 1.057530] [G loss: 1.257323]\n",
            "4404 [D loss: 1.060602] [G loss: 1.256431]\n",
            "4405 [D loss: 1.063183] [G loss: 1.255657]\n",
            "4406 [D loss: 1.050863] [G loss: 1.257428]\n",
            "4407 [D loss: 1.057733] [G loss: 1.260374]\n",
            "4408 [D loss: 1.059417] [G loss: 1.258519]\n",
            "4409 [D loss: 1.057420] [G loss: 1.237396]\n",
            "4410 [D loss: 1.055437] [G loss: 1.245576]\n",
            "4411 [D loss: 1.054125] [G loss: 1.263420]\n",
            "4412 [D loss: 1.043748] [G loss: 1.260800]\n",
            "4413 [D loss: 1.067414] [G loss: 1.252293]\n",
            "4414 [D loss: 1.051679] [G loss: 1.263924]\n",
            "4415 [D loss: 1.057598] [G loss: 1.254168]\n",
            "4416 [D loss: 1.057224] [G loss: 1.250499]\n",
            "4417 [D loss: 1.062138] [G loss: 1.260541]\n",
            "4418 [D loss: 1.066271] [G loss: 1.241365]\n",
            "4419 [D loss: 1.058832] [G loss: 1.247049]\n",
            "4420 [D loss: 1.051675] [G loss: 1.255478]\n",
            "4421 [D loss: 1.056548] [G loss: 1.259088]\n",
            "4422 [D loss: 1.053015] [G loss: 1.257661]\n",
            "4423 [D loss: 1.064285] [G loss: 1.238133]\n",
            "4424 [D loss: 1.057594] [G loss: 1.250080]\n",
            "4425 [D loss: 1.050905] [G loss: 1.250421]\n",
            "4426 [D loss: 1.042286] [G loss: 1.259602]\n",
            "4427 [D loss: 1.060216] [G loss: 1.252097]\n",
            "4428 [D loss: 1.054598] [G loss: 1.254547]\n",
            "4429 [D loss: 1.056230] [G loss: 1.253407]\n",
            "4430 [D loss: 1.062662] [G loss: 1.240198]\n",
            "4431 [D loss: 1.064280] [G loss: 1.248303]\n",
            "4432 [D loss: 1.037217] [G loss: 1.257628]\n",
            "4433 [D loss: 1.067776] [G loss: 1.252076]\n",
            "4434 [D loss: 1.077363] [G loss: 1.239523]\n",
            "4435 [D loss: 1.070031] [G loss: 1.246401]\n",
            "4436 [D loss: 1.050741] [G loss: 1.246163]\n",
            "4437 [D loss: 1.064705] [G loss: 1.242736]\n",
            "4438 [D loss: 1.065436] [G loss: 1.248398]\n",
            "4439 [D loss: 1.059646] [G loss: 1.253341]\n",
            "4440 [D loss: 1.068921] [G loss: 1.241549]\n",
            "4441 [D loss: 1.078993] [G loss: 1.239036]\n",
            "4442 [D loss: 1.067854] [G loss: 1.248709]\n",
            "4443 [D loss: 1.047697] [G loss: 1.246479]\n",
            "4444 [D loss: 1.055917] [G loss: 1.232120]\n",
            "4445 [D loss: 1.048768] [G loss: 1.257034]\n",
            "4446 [D loss: 1.052133] [G loss: 1.254807]\n",
            "4447 [D loss: 1.060121] [G loss: 1.253282]\n",
            "4448 [D loss: 1.064547] [G loss: 1.243417]\n",
            "4449 [D loss: 1.065291] [G loss: 1.236857]\n",
            "4450 [D loss: 1.046136] [G loss: 1.263207]\n",
            "[[0.6149582  0.43495083 0.49217615 0.46176127]\n",
            " [0.50223875 0.4981484  0.5017015  0.5013561 ]\n",
            " [0.50223875 0.4981484  0.5017015  0.5013561 ]\n",
            " [0.5504768  0.44194922 0.48868087 0.5238643 ]\n",
            " [0.5002208  0.49733433 0.5024799  0.50303715]]\n",
            "4451 [D loss: 1.068694] [G loss: 1.247026]\n",
            "4452 [D loss: 1.056879] [G loss: 1.252675]\n",
            "4453 [D loss: 1.045399] [G loss: 1.254006]\n",
            "4454 [D loss: 1.067472] [G loss: 1.257106]\n",
            "4455 [D loss: 1.056823] [G loss: 1.261279]\n",
            "4456 [D loss: 1.056794] [G loss: 1.255844]\n",
            "4457 [D loss: 1.063145] [G loss: 1.258355]\n",
            "4458 [D loss: 1.066107] [G loss: 1.239672]\n",
            "4459 [D loss: 1.061398] [G loss: 1.247037]\n",
            "4460 [D loss: 1.068059] [G loss: 1.243397]\n",
            "4461 [D loss: 1.069433] [G loss: 1.250538]\n",
            "4462 [D loss: 1.051058] [G loss: 1.256206]\n",
            "4463 [D loss: 1.068719] [G loss: 1.250577]\n",
            "4464 [D loss: 1.046780] [G loss: 1.241687]\n",
            "4465 [D loss: 1.069878] [G loss: 1.245515]\n",
            "4466 [D loss: 1.064044] [G loss: 1.251491]\n",
            "4467 [D loss: 1.055886] [G loss: 1.246883]\n",
            "4468 [D loss: 1.060885] [G loss: 1.259572]\n",
            "4469 [D loss: 1.046516] [G loss: 1.268933]\n",
            "4470 [D loss: 1.062734] [G loss: 1.249998]\n",
            "4471 [D loss: 1.059002] [G loss: 1.241168]\n",
            "4472 [D loss: 1.062368] [G loss: 1.237086]\n",
            "4473 [D loss: 1.041324] [G loss: 1.251389]\n",
            "4474 [D loss: 1.059793] [G loss: 1.244318]\n",
            "4475 [D loss: 1.054107] [G loss: 1.242854]\n",
            "4476 [D loss: 1.060373] [G loss: 1.241229]\n",
            "4477 [D loss: 1.065697] [G loss: 1.246224]\n",
            "4478 [D loss: 1.038756] [G loss: 1.257878]\n",
            "4479 [D loss: 1.058998] [G loss: 1.243643]\n",
            "4480 [D loss: 1.055505] [G loss: 1.255779]\n",
            "4481 [D loss: 1.056828] [G loss: 1.251588]\n",
            "4482 [D loss: 1.051819] [G loss: 1.254374]\n",
            "4483 [D loss: 1.062375] [G loss: 1.253343]\n",
            "4484 [D loss: 1.071570] [G loss: 1.238792]\n",
            "4485 [D loss: 1.070337] [G loss: 1.237173]\n",
            "4486 [D loss: 1.058380] [G loss: 1.241535]\n",
            "4487 [D loss: 1.054804] [G loss: 1.248305]\n",
            "4488 [D loss: 1.052850] [G loss: 1.255641]\n",
            "4489 [D loss: 1.062613] [G loss: 1.249756]\n",
            "4490 [D loss: 1.069036] [G loss: 1.242791]\n",
            "4491 [D loss: 1.055928] [G loss: 1.252710]\n",
            "4492 [D loss: 1.063769] [G loss: 1.247709]\n",
            "4493 [D loss: 1.060108] [G loss: 1.238829]\n",
            "4494 [D loss: 1.059798] [G loss: 1.242038]\n",
            "4495 [D loss: 1.064122] [G loss: 1.250599]\n",
            "4496 [D loss: 1.067100] [G loss: 1.239028]\n",
            "4497 [D loss: 1.055270] [G loss: 1.249033]\n",
            "4498 [D loss: 1.049749] [G loss: 1.250871]\n",
            "4499 [D loss: 1.048074] [G loss: 1.256616]\n",
            "4500 [D loss: 1.070641] [G loss: 1.243105]\n",
            "[[0.5131328  0.4665804  0.50645304 0.54489326]\n",
            " [0.7675245  0.365096   0.64997065 0.28173915]\n",
            " [0.65413934 0.34971738 0.4901473  0.5067128 ]\n",
            " [0.5806558  0.5001641  0.48035398 0.42472425]\n",
            " [0.5294844  0.42174935 0.51804703 0.53892726]]\n",
            "4501 [D loss: 1.055338] [G loss: 1.254851]\n",
            "4502 [D loss: 1.064130] [G loss: 1.243731]\n",
            "4503 [D loss: 1.043357] [G loss: 1.259067]\n",
            "4504 [D loss: 1.058209] [G loss: 1.248962]\n",
            "4505 [D loss: 1.060396] [G loss: 1.253243]\n",
            "4506 [D loss: 1.067954] [G loss: 1.237259]\n",
            "4507 [D loss: 1.053020] [G loss: 1.242948]\n",
            "4508 [D loss: 1.057922] [G loss: 1.246896]\n",
            "4509 [D loss: 1.064528] [G loss: 1.242828]\n",
            "4510 [D loss: 1.050168] [G loss: 1.246037]\n",
            "4511 [D loss: 1.057298] [G loss: 1.247679]\n",
            "4512 [D loss: 1.039124] [G loss: 1.248488]\n",
            "4513 [D loss: 1.068613] [G loss: 1.245583]\n",
            "4514 [D loss: 1.054739] [G loss: 1.240633]\n",
            "4515 [D loss: 1.063284] [G loss: 1.246338]\n",
            "4516 [D loss: 1.060463] [G loss: 1.240150]\n",
            "4517 [D loss: 1.050469] [G loss: 1.251051]\n",
            "4518 [D loss: 1.058429] [G loss: 1.249904]\n",
            "4519 [D loss: 1.043008] [G loss: 1.244870]\n",
            "4520 [D loss: 1.074894] [G loss: 1.243656]\n",
            "4521 [D loss: 1.045772] [G loss: 1.263213]\n",
            "4522 [D loss: 1.063206] [G loss: 1.244928]\n",
            "4523 [D loss: 1.075295] [G loss: 1.254899]\n",
            "4524 [D loss: 1.064920] [G loss: 1.248617]\n",
            "4525 [D loss: 1.069057] [G loss: 1.243705]\n",
            "4526 [D loss: 1.050494] [G loss: 1.251389]\n",
            "4527 [D loss: 1.078457] [G loss: 1.253545]\n",
            "4528 [D loss: 1.058115] [G loss: 1.245599]\n",
            "4529 [D loss: 1.049528] [G loss: 1.242945]\n",
            "4530 [D loss: 1.056344] [G loss: 1.261511]\n",
            "4531 [D loss: 1.059015] [G loss: 1.241037]\n",
            "4532 [D loss: 1.065541] [G loss: 1.252351]\n",
            "4533 [D loss: 1.062841] [G loss: 1.237512]\n",
            "4534 [D loss: 1.077259] [G loss: 1.234138]\n",
            "4535 [D loss: 1.069420] [G loss: 1.256727]\n",
            "4536 [D loss: 1.058213] [G loss: 1.238232]\n",
            "4537 [D loss: 1.052816] [G loss: 1.248773]\n",
            "4538 [D loss: 1.071682] [G loss: 1.239382]\n",
            "4539 [D loss: 1.054727] [G loss: 1.240183]\n",
            "4540 [D loss: 1.062293] [G loss: 1.249783]\n",
            "4541 [D loss: 1.074198] [G loss: 1.250026]\n",
            "4542 [D loss: 1.063464] [G loss: 1.235224]\n",
            "4543 [D loss: 1.076196] [G loss: 1.250671]\n",
            "4544 [D loss: 1.060850] [G loss: 1.258850]\n",
            "4545 [D loss: 1.054908] [G loss: 1.244452]\n",
            "4546 [D loss: 1.065123] [G loss: 1.245244]\n",
            "4547 [D loss: 1.081869] [G loss: 1.228552]\n",
            "4548 [D loss: 1.052716] [G loss: 1.238582]\n",
            "4549 [D loss: 1.060399] [G loss: 1.250311]\n",
            "4550 [D loss: 1.070836] [G loss: 1.246606]\n",
            "[[0.6717519  0.24399737 0.49125215 0.5685517 ]\n",
            " [0.5971263  0.33184683 0.47191232 0.559448  ]\n",
            " [0.6798878  0.3763839  0.6544256  0.32338202]\n",
            " [0.5023321  0.4980947  0.50172883 0.5013112 ]\n",
            " [0.5434211  0.45457673 0.5177376  0.4739083 ]]\n",
            "4551 [D loss: 1.068056] [G loss: 1.241690]\n",
            "4552 [D loss: 1.072205] [G loss: 1.246609]\n",
            "4553 [D loss: 1.058863] [G loss: 1.252399]\n",
            "4554 [D loss: 1.049594] [G loss: 1.246953]\n",
            "4555 [D loss: 1.047485] [G loss: 1.241629]\n",
            "4556 [D loss: 1.052237] [G loss: 1.238688]\n",
            "4557 [D loss: 1.067258] [G loss: 1.243226]\n",
            "4558 [D loss: 1.057443] [G loss: 1.250544]\n",
            "4559 [D loss: 1.056943] [G loss: 1.233569]\n",
            "4560 [D loss: 1.057033] [G loss: 1.243586]\n",
            "4561 [D loss: 1.078771] [G loss: 1.238668]\n",
            "4562 [D loss: 1.063790] [G loss: 1.237391]\n",
            "4563 [D loss: 1.057361] [G loss: 1.243189]\n",
            "4564 [D loss: 1.053481] [G loss: 1.242582]\n",
            "4565 [D loss: 1.054603] [G loss: 1.244041]\n",
            "4566 [D loss: 1.071998] [G loss: 1.244828]\n",
            "4567 [D loss: 1.076472] [G loss: 1.250848]\n",
            "4568 [D loss: 1.069581] [G loss: 1.236502]\n",
            "4569 [D loss: 1.074153] [G loss: 1.241306]\n",
            "4570 [D loss: 1.073377] [G loss: 1.239856]\n",
            "4571 [D loss: 1.052118] [G loss: 1.248686]\n",
            "4572 [D loss: 1.064274] [G loss: 1.247019]\n",
            "4573 [D loss: 1.060043] [G loss: 1.237817]\n",
            "4574 [D loss: 1.057203] [G loss: 1.252301]\n",
            "4575 [D loss: 1.048257] [G loss: 1.245434]\n",
            "4576 [D loss: 1.073544] [G loss: 1.242592]\n",
            "4577 [D loss: 1.050568] [G loss: 1.251704]\n",
            "4578 [D loss: 1.052689] [G loss: 1.238411]\n",
            "4579 [D loss: 1.061741] [G loss: 1.247228]\n",
            "4580 [D loss: 1.063538] [G loss: 1.249517]\n",
            "4581 [D loss: 1.061175] [G loss: 1.248425]\n",
            "4582 [D loss: 1.065522] [G loss: 1.230255]\n",
            "4583 [D loss: 1.054598] [G loss: 1.247905]\n",
            "4584 [D loss: 1.060451] [G loss: 1.241946]\n",
            "4585 [D loss: 1.048751] [G loss: 1.246091]\n",
            "4586 [D loss: 1.058086] [G loss: 1.241348]\n",
            "4587 [D loss: 1.068978] [G loss: 1.240435]\n",
            "4588 [D loss: 1.070949] [G loss: 1.233222]\n",
            "4589 [D loss: 1.062480] [G loss: 1.241557]\n",
            "4590 [D loss: 1.064149] [G loss: 1.246323]\n",
            "4591 [D loss: 1.062709] [G loss: 1.251114]\n",
            "4592 [D loss: 1.052468] [G loss: 1.259185]\n",
            "4593 [D loss: 1.066423] [G loss: 1.240160]\n",
            "4594 [D loss: 1.055420] [G loss: 1.244779]\n",
            "4595 [D loss: 1.050294] [G loss: 1.230885]\n",
            "4596 [D loss: 1.065826] [G loss: 1.241046]\n",
            "4597 [D loss: 1.059622] [G loss: 1.240791]\n",
            "4598 [D loss: 1.061091] [G loss: 1.251511]\n",
            "4599 [D loss: 1.052819] [G loss: 1.249547]\n",
            "4600 [D loss: 1.053419] [G loss: 1.241298]\n",
            "[[0.5382042  0.4116574  0.5482722  0.50838536]\n",
            " [0.6272087  0.32217407 0.5556952  0.545152  ]\n",
            " [0.5874586  0.42097297 0.52355915 0.5425913 ]\n",
            " [0.51273453 0.4725648  0.5103666  0.5141066 ]\n",
            " [0.5663993  0.42208174 0.54222137 0.44499367]]\n",
            "4601 [D loss: 1.074639] [G loss: 1.242668]\n",
            "4602 [D loss: 1.055462] [G loss: 1.253306]\n",
            "4603 [D loss: 1.059889] [G loss: 1.240626]\n",
            "4604 [D loss: 1.062806] [G loss: 1.240224]\n",
            "4605 [D loss: 1.073661] [G loss: 1.236533]\n",
            "4606 [D loss: 1.061512] [G loss: 1.239738]\n",
            "4607 [D loss: 1.067693] [G loss: 1.242625]\n",
            "4608 [D loss: 1.047375] [G loss: 1.243723]\n",
            "4609 [D loss: 1.063383] [G loss: 1.236667]\n",
            "4610 [D loss: 1.060417] [G loss: 1.240585]\n",
            "4611 [D loss: 1.068400] [G loss: 1.252717]\n",
            "4612 [D loss: 1.064804] [G loss: 1.242237]\n",
            "4613 [D loss: 1.070585] [G loss: 1.240187]\n",
            "4614 [D loss: 1.062292] [G loss: 1.242840]\n",
            "4615 [D loss: 1.051182] [G loss: 1.258246]\n",
            "4616 [D loss: 1.064611] [G loss: 1.253059]\n",
            "4617 [D loss: 1.056802] [G loss: 1.253223]\n",
            "4618 [D loss: 1.065436] [G loss: 1.237307]\n",
            "4619 [D loss: 1.067887] [G loss: 1.233429]\n",
            "4620 [D loss: 1.070638] [G loss: 1.244004]\n",
            "4621 [D loss: 1.057442] [G loss: 1.246344]\n",
            "4622 [D loss: 1.068938] [G loss: 1.232264]\n",
            "4623 [D loss: 1.035736] [G loss: 1.246454]\n",
            "4624 [D loss: 1.058707] [G loss: 1.239952]\n",
            "4625 [D loss: 1.058885] [G loss: 1.233658]\n",
            "4626 [D loss: 1.065951] [G loss: 1.247205]\n",
            "4627 [D loss: 1.046775] [G loss: 1.235742]\n",
            "4628 [D loss: 1.062111] [G loss: 1.241757]\n",
            "4629 [D loss: 1.053447] [G loss: 1.256231]\n",
            "4630 [D loss: 1.038964] [G loss: 1.253381]\n",
            "4631 [D loss: 1.069311] [G loss: 1.248790]\n",
            "4632 [D loss: 1.050553] [G loss: 1.247942]\n",
            "4633 [D loss: 1.071399] [G loss: 1.234062]\n",
            "4634 [D loss: 1.061923] [G loss: 1.230242]\n",
            "4635 [D loss: 1.069833] [G loss: 1.237138]\n",
            "4636 [D loss: 1.072134] [G loss: 1.242759]\n",
            "4637 [D loss: 1.057843] [G loss: 1.236976]\n",
            "4638 [D loss: 1.066161] [G loss: 1.243055]\n",
            "4639 [D loss: 1.059034] [G loss: 1.247486]\n",
            "4640 [D loss: 1.071797] [G loss: 1.244194]\n",
            "4641 [D loss: 1.056050] [G loss: 1.249795]\n",
            "4642 [D loss: 1.050636] [G loss: 1.250615]\n",
            "4643 [D loss: 1.056519] [G loss: 1.245167]\n",
            "4644 [D loss: 1.056755] [G loss: 1.238243]\n",
            "4645 [D loss: 1.069604] [G loss: 1.241082]\n",
            "4646 [D loss: 1.070974] [G loss: 1.247086]\n",
            "4647 [D loss: 1.065588] [G loss: 1.239181]\n",
            "4648 [D loss: 1.048265] [G loss: 1.244967]\n",
            "4649 [D loss: 1.058029] [G loss: 1.256221]\n",
            "4650 [D loss: 1.065395] [G loss: 1.247168]\n",
            "[[0.64618576 0.49919212 0.6262882  0.38740337]\n",
            " [0.6849278  0.4008026  0.6069027  0.44560838]\n",
            " [0.6364819  0.2985445  0.43031955 0.56789863]\n",
            " [0.620769   0.46623975 0.55519    0.37151062]\n",
            " [0.5746858  0.4300706  0.5044229  0.5119493 ]]\n",
            "4651 [D loss: 1.071564] [G loss: 1.243592]\n",
            "4652 [D loss: 1.064697] [G loss: 1.235216]\n",
            "4653 [D loss: 1.091094] [G loss: 1.231060]\n",
            "4654 [D loss: 1.052136] [G loss: 1.240883]\n",
            "4655 [D loss: 1.056931] [G loss: 1.247814]\n",
            "4656 [D loss: 1.055869] [G loss: 1.242266]\n",
            "4657 [D loss: 1.069449] [G loss: 1.251087]\n",
            "4658 [D loss: 1.060640] [G loss: 1.249273]\n",
            "4659 [D loss: 1.065075] [G loss: 1.253301]\n",
            "4660 [D loss: 1.064430] [G loss: 1.237038]\n",
            "4661 [D loss: 1.058292] [G loss: 1.234544]\n",
            "4662 [D loss: 1.060925] [G loss: 1.239647]\n",
            "4663 [D loss: 1.062141] [G loss: 1.236411]\n",
            "4664 [D loss: 1.072347] [G loss: 1.246768]\n",
            "4665 [D loss: 1.073467] [G loss: 1.257587]\n",
            "4666 [D loss: 1.075949] [G loss: 1.232805]\n",
            "4667 [D loss: 1.073225] [G loss: 1.233398]\n",
            "4668 [D loss: 1.071715] [G loss: 1.246311]\n",
            "4669 [D loss: 1.053767] [G loss: 1.233402]\n",
            "4670 [D loss: 1.054428] [G loss: 1.247609]\n",
            "4671 [D loss: 1.064298] [G loss: 1.240169]\n",
            "4672 [D loss: 1.059289] [G loss: 1.253354]\n",
            "4673 [D loss: 1.055870] [G loss: 1.244711]\n",
            "4674 [D loss: 1.074418] [G loss: 1.244565]\n",
            "4675 [D loss: 1.073583] [G loss: 1.245418]\n",
            "4676 [D loss: 1.051921] [G loss: 1.252359]\n",
            "4677 [D loss: 1.061306] [G loss: 1.252126]\n",
            "4678 [D loss: 1.064240] [G loss: 1.246120]\n",
            "4679 [D loss: 1.052269] [G loss: 1.245323]\n",
            "4680 [D loss: 1.048769] [G loss: 1.241001]\n",
            "4681 [D loss: 1.072536] [G loss: 1.241597]\n",
            "4682 [D loss: 1.084332] [G loss: 1.228510]\n",
            "4683 [D loss: 1.048537] [G loss: 1.248695]\n",
            "4684 [D loss: 1.057637] [G loss: 1.251307]\n",
            "4685 [D loss: 1.059899] [G loss: 1.242176]\n",
            "4686 [D loss: 1.045842] [G loss: 1.252332]\n",
            "4687 [D loss: 1.072896] [G loss: 1.237466]\n",
            "4688 [D loss: 1.076177] [G loss: 1.240194]\n",
            "4689 [D loss: 1.056572] [G loss: 1.236062]\n",
            "4690 [D loss: 1.070246] [G loss: 1.226432]\n",
            "4691 [D loss: 1.047896] [G loss: 1.247675]\n",
            "4692 [D loss: 1.055362] [G loss: 1.245821]\n",
            "4693 [D loss: 1.061092] [G loss: 1.252898]\n",
            "4694 [D loss: 1.069733] [G loss: 1.241004]\n",
            "4695 [D loss: 1.041835] [G loss: 1.246566]\n",
            "4696 [D loss: 1.057486] [G loss: 1.245251]\n",
            "4697 [D loss: 1.070042] [G loss: 1.247564]\n",
            "4698 [D loss: 1.041433] [G loss: 1.251508]\n",
            "4699 [D loss: 1.061428] [G loss: 1.244912]\n",
            "4700 [D loss: 1.067399] [G loss: 1.234562]\n",
            "[[0.62839943 0.36132395 0.4316515  0.5234814 ]\n",
            " [0.56665367 0.46442327 0.48485285 0.47306126]\n",
            " [0.5092542  0.49303755 0.5076332  0.50279754]\n",
            " [0.5667496  0.44459105 0.49752066 0.47809353]\n",
            " [0.742959   0.40693915 0.6635269  0.28483   ]]\n",
            "4701 [D loss: 1.064740] [G loss: 1.229036]\n",
            "4702 [D loss: 1.054760] [G loss: 1.254828]\n",
            "4703 [D loss: 1.051231] [G loss: 1.244725]\n",
            "4704 [D loss: 1.083538] [G loss: 1.230455]\n",
            "4705 [D loss: 1.048683] [G loss: 1.258911]\n",
            "4706 [D loss: 1.058150] [G loss: 1.239455]\n",
            "4707 [D loss: 1.064831] [G loss: 1.243555]\n",
            "4708 [D loss: 1.061231] [G loss: 1.233446]\n",
            "4709 [D loss: 1.059277] [G loss: 1.234180]\n",
            "4710 [D loss: 1.073535] [G loss: 1.240662]\n",
            "4711 [D loss: 1.056402] [G loss: 1.240453]\n",
            "4712 [D loss: 1.070056] [G loss: 1.238351]\n",
            "4713 [D loss: 1.064864] [G loss: 1.246504]\n",
            "4714 [D loss: 1.072644] [G loss: 1.240200]\n",
            "4715 [D loss: 1.059060] [G loss: 1.245341]\n",
            "4716 [D loss: 1.058714] [G loss: 1.246323]\n",
            "4717 [D loss: 1.062695] [G loss: 1.240370]\n",
            "4718 [D loss: 1.066395] [G loss: 1.244990]\n",
            "4719 [D loss: 1.062294] [G loss: 1.246481]\n",
            "4720 [D loss: 1.065098] [G loss: 1.244655]\n",
            "4721 [D loss: 1.055943] [G loss: 1.253760]\n",
            "4722 [D loss: 1.053890] [G loss: 1.243761]\n",
            "4723 [D loss: 1.076694] [G loss: 1.231894]\n",
            "4724 [D loss: 1.053388] [G loss: 1.244874]\n",
            "4725 [D loss: 1.053154] [G loss: 1.242435]\n",
            "4726 [D loss: 1.063523] [G loss: 1.247805]\n",
            "4727 [D loss: 1.057742] [G loss: 1.232996]\n",
            "4728 [D loss: 1.046006] [G loss: 1.249032]\n",
            "4729 [D loss: 1.059555] [G loss: 1.238975]\n",
            "4730 [D loss: 1.059013] [G loss: 1.241369]\n",
            "4731 [D loss: 1.062564] [G loss: 1.231837]\n",
            "4732 [D loss: 1.057757] [G loss: 1.243660]\n",
            "4733 [D loss: 1.054072] [G loss: 1.229255]\n",
            "4734 [D loss: 1.072535] [G loss: 1.236638]\n",
            "4735 [D loss: 1.073042] [G loss: 1.239036]\n",
            "4736 [D loss: 1.058505] [G loss: 1.244491]\n",
            "4737 [D loss: 1.072335] [G loss: 1.242721]\n",
            "4738 [D loss: 1.060305] [G loss: 1.242924]\n",
            "4739 [D loss: 1.070478] [G loss: 1.242976]\n",
            "4740 [D loss: 1.068599] [G loss: 1.240082]\n",
            "4741 [D loss: 1.061663] [G loss: 1.244733]\n",
            "4742 [D loss: 1.067599] [G loss: 1.236465]\n",
            "4743 [D loss: 1.067355] [G loss: 1.225692]\n",
            "4744 [D loss: 1.066602] [G loss: 1.221352]\n",
            "4745 [D loss: 1.072799] [G loss: 1.242857]\n",
            "4746 [D loss: 1.054564] [G loss: 1.227986]\n",
            "4747 [D loss: 1.039197] [G loss: 1.248975]\n",
            "4748 [D loss: 1.059916] [G loss: 1.242231]\n",
            "4749 [D loss: 1.079126] [G loss: 1.229915]\n",
            "4750 [D loss: 1.056981] [G loss: 1.240386]\n",
            "[[0.6679255  0.30226368 0.54399675 0.41350064]\n",
            " [0.58755696 0.38483483 0.5689563  0.448335  ]\n",
            " [0.65447056 0.32662487 0.52848095 0.58448017]\n",
            " [0.63042504 0.3910191  0.5120427  0.4714766 ]\n",
            " [0.5025271  0.49798152 0.5017786  0.5012135 ]]\n",
            "4751 [D loss: 1.065949] [G loss: 1.230908]\n",
            "4752 [D loss: 1.064857] [G loss: 1.244083]\n",
            "4753 [D loss: 1.052171] [G loss: 1.241108]\n",
            "4754 [D loss: 1.054198] [G loss: 1.246113]\n",
            "4755 [D loss: 1.065472] [G loss: 1.242096]\n",
            "4756 [D loss: 1.076268] [G loss: 1.222304]\n",
            "4757 [D loss: 1.065071] [G loss: 1.246982]\n",
            "4758 [D loss: 1.068151] [G loss: 1.228374]\n",
            "4759 [D loss: 1.064620] [G loss: 1.239998]\n",
            "4760 [D loss: 1.059026] [G loss: 1.234178]\n",
            "4761 [D loss: 1.070795] [G loss: 1.231211]\n",
            "4762 [D loss: 1.053716] [G loss: 1.247242]\n",
            "4763 [D loss: 1.060853] [G loss: 1.242192]\n",
            "4764 [D loss: 1.060393] [G loss: 1.232832]\n",
            "4765 [D loss: 1.056718] [G loss: 1.250803]\n",
            "4766 [D loss: 1.074035] [G loss: 1.227103]\n",
            "4767 [D loss: 1.045289] [G loss: 1.239429]\n",
            "4768 [D loss: 1.062065] [G loss: 1.248212]\n",
            "4769 [D loss: 1.059001] [G loss: 1.239050]\n",
            "4770 [D loss: 1.055806] [G loss: 1.243199]\n",
            "4771 [D loss: 1.057109] [G loss: 1.232457]\n",
            "4772 [D loss: 1.062466] [G loss: 1.233428]\n",
            "4773 [D loss: 1.064256] [G loss: 1.239014]\n",
            "4774 [D loss: 1.054104] [G loss: 1.248598]\n",
            "4775 [D loss: 1.065067] [G loss: 1.235736]\n",
            "4776 [D loss: 1.067921] [G loss: 1.228424]\n",
            "4777 [D loss: 1.071387] [G loss: 1.241195]\n",
            "4778 [D loss: 1.051490] [G loss: 1.240548]\n",
            "4779 [D loss: 1.069132] [G loss: 1.240656]\n",
            "4780 [D loss: 1.064438] [G loss: 1.236149]\n",
            "4781 [D loss: 1.051653] [G loss: 1.237467]\n",
            "4782 [D loss: 1.060060] [G loss: 1.231769]\n",
            "4783 [D loss: 1.068476] [G loss: 1.245488]\n",
            "4784 [D loss: 1.066040] [G loss: 1.245234]\n",
            "4785 [D loss: 1.067510] [G loss: 1.224561]\n",
            "4786 [D loss: 1.075029] [G loss: 1.235890]\n",
            "4787 [D loss: 1.056529] [G loss: 1.230857]\n",
            "4788 [D loss: 1.049649] [G loss: 1.251918]\n",
            "4789 [D loss: 1.069593] [G loss: 1.240070]\n",
            "4790 [D loss: 1.078061] [G loss: 1.233692]\n",
            "4791 [D loss: 1.057726] [G loss: 1.237393]\n",
            "4792 [D loss: 1.060993] [G loss: 1.238095]\n",
            "4793 [D loss: 1.082359] [G loss: 1.242427]\n",
            "4794 [D loss: 1.086857] [G loss: 1.232061]\n",
            "4795 [D loss: 1.064561] [G loss: 1.230736]\n",
            "4796 [D loss: 1.071879] [G loss: 1.241331]\n",
            "4797 [D loss: 1.060691] [G loss: 1.244334]\n",
            "4798 [D loss: 1.065869] [G loss: 1.235595]\n",
            "4799 [D loss: 1.048818] [G loss: 1.238707]\n",
            "4800 [D loss: 1.061824] [G loss: 1.245514]\n",
            "[[0.6652744  0.2119428  0.48678732 0.6176592 ]\n",
            " [0.5551832  0.42479393 0.53740084 0.46603626]\n",
            " [0.5574554  0.47588146 0.5182947  0.47824213]\n",
            " [0.5381426  0.50416136 0.5128186  0.47144216]\n",
            " [0.66540605 0.40130338 0.5820376  0.3973853 ]]\n",
            "4801 [D loss: 1.082547] [G loss: 1.223348]\n",
            "4802 [D loss: 1.051676] [G loss: 1.226772]\n",
            "4803 [D loss: 1.042429] [G loss: 1.245247]\n",
            "4804 [D loss: 1.086122] [G loss: 1.225227]\n",
            "4805 [D loss: 1.073471] [G loss: 1.237305]\n",
            "4806 [D loss: 1.070561] [G loss: 1.228505]\n",
            "4807 [D loss: 1.072175] [G loss: 1.222433]\n",
            "4808 [D loss: 1.054932] [G loss: 1.236989]\n",
            "4809 [D loss: 1.061674] [G loss: 1.244318]\n",
            "4810 [D loss: 1.068557] [G loss: 1.239444]\n",
            "4811 [D loss: 1.068040] [G loss: 1.241946]\n",
            "4812 [D loss: 1.054535] [G loss: 1.234193]\n",
            "4813 [D loss: 1.062604] [G loss: 1.236761]\n",
            "4814 [D loss: 1.080417] [G loss: 1.235938]\n",
            "4815 [D loss: 1.062963] [G loss: 1.248365]\n",
            "4816 [D loss: 1.070481] [G loss: 1.242883]\n",
            "4817 [D loss: 1.057230] [G loss: 1.241860]\n",
            "4818 [D loss: 1.066333] [G loss: 1.248047]\n",
            "4819 [D loss: 1.048898] [G loss: 1.244705]\n",
            "4820 [D loss: 1.053507] [G loss: 1.245524]\n",
            "4821 [D loss: 1.054681] [G loss: 1.220149]\n",
            "4822 [D loss: 1.057187] [G loss: 1.236491]\n",
            "4823 [D loss: 1.062244] [G loss: 1.242866]\n",
            "4824 [D loss: 1.058875] [G loss: 1.245544]\n",
            "4825 [D loss: 1.073549] [G loss: 1.235142]\n",
            "4826 [D loss: 1.076047] [G loss: 1.239784]\n",
            "4827 [D loss: 1.059666] [G loss: 1.236756]\n",
            "4828 [D loss: 1.046453] [G loss: 1.233865]\n",
            "4829 [D loss: 1.050235] [G loss: 1.239230]\n",
            "4830 [D loss: 1.068553] [G loss: 1.248565]\n",
            "4831 [D loss: 1.049184] [G loss: 1.244875]\n",
            "4832 [D loss: 1.053002] [G loss: 1.245195]\n",
            "4833 [D loss: 1.085878] [G loss: 1.229726]\n",
            "4834 [D loss: 1.074306] [G loss: 1.231266]\n",
            "4835 [D loss: 1.065723] [G loss: 1.238376]\n",
            "4836 [D loss: 1.050836] [G loss: 1.228683]\n",
            "4837 [D loss: 1.064174] [G loss: 1.245393]\n",
            "4838 [D loss: 1.072661] [G loss: 1.230700]\n",
            "4839 [D loss: 1.053400] [G loss: 1.239716]\n",
            "4840 [D loss: 1.069332] [G loss: 1.222300]\n",
            "4841 [D loss: 1.038675] [G loss: 1.246749]\n",
            "4842 [D loss: 1.065047] [G loss: 1.244831]\n",
            "4843 [D loss: 1.059528] [G loss: 1.249937]\n",
            "4844 [D loss: 1.083862] [G loss: 1.240310]\n",
            "4845 [D loss: 1.077561] [G loss: 1.232212]\n",
            "4846 [D loss: 1.080522] [G loss: 1.229762]\n",
            "4847 [D loss: 1.077589] [G loss: 1.226332]\n",
            "4848 [D loss: 1.072957] [G loss: 1.233881]\n",
            "4849 [D loss: 1.064762] [G loss: 1.234568]\n",
            "4850 [D loss: 1.075219] [G loss: 1.234043]\n",
            "[[0.8464376  0.27802426 0.6602094  0.27675164]\n",
            " [0.5716869  0.4281225  0.5151135  0.5049827 ]\n",
            " [0.73302895 0.3807082  0.6498265  0.2697173 ]\n",
            " [0.5026281  0.4979245  0.5018005  0.5011602 ]\n",
            " [0.50727034 0.45248747 0.5241638  0.52097684]]\n",
            "4851 [D loss: 1.069473] [G loss: 1.239920]\n",
            "4852 [D loss: 1.063383] [G loss: 1.242054]\n",
            "4853 [D loss: 1.048637] [G loss: 1.232556]\n",
            "4854 [D loss: 1.080214] [G loss: 1.229098]\n",
            "4855 [D loss: 1.061124] [G loss: 1.259080]\n",
            "4856 [D loss: 1.068407] [G loss: 1.229664]\n",
            "4857 [D loss: 1.066427] [G loss: 1.235168]\n",
            "4858 [D loss: 1.071930] [G loss: 1.231947]\n",
            "4859 [D loss: 1.052015] [G loss: 1.247616]\n",
            "4860 [D loss: 1.066935] [G loss: 1.240909]\n",
            "4861 [D loss: 1.062818] [G loss: 1.233538]\n",
            "4862 [D loss: 1.070501] [G loss: 1.227676]\n",
            "4863 [D loss: 1.071574] [G loss: 1.244277]\n",
            "4864 [D loss: 1.040943] [G loss: 1.252721]\n",
            "4865 [D loss: 1.070008] [G loss: 1.241083]\n",
            "4866 [D loss: 1.048428] [G loss: 1.242741]\n",
            "4867 [D loss: 1.046975] [G loss: 1.233145]\n",
            "4868 [D loss: 1.066349] [G loss: 1.230262]\n",
            "4869 [D loss: 1.054273] [G loss: 1.227482]\n",
            "4870 [D loss: 1.061173] [G loss: 1.235296]\n",
            "4871 [D loss: 1.075584] [G loss: 1.232516]\n",
            "4872 [D loss: 1.056317] [G loss: 1.247618]\n",
            "4873 [D loss: 1.072534] [G loss: 1.235635]\n",
            "4874 [D loss: 1.058115] [G loss: 1.239522]\n",
            "4875 [D loss: 1.048137] [G loss: 1.242530]\n",
            "4876 [D loss: 1.056339] [G loss: 1.239283]\n",
            "4877 [D loss: 1.059347] [G loss: 1.245723]\n",
            "4878 [D loss: 1.076269] [G loss: 1.233842]\n",
            "4879 [D loss: 1.063088] [G loss: 1.234445]\n",
            "4880 [D loss: 1.059889] [G loss: 1.243407]\n",
            "4881 [D loss: 1.074202] [G loss: 1.236492]\n",
            "4882 [D loss: 1.067454] [G loss: 1.222726]\n",
            "4883 [D loss: 1.050982] [G loss: 1.245179]\n",
            "4884 [D loss: 1.040842] [G loss: 1.264132]\n",
            "4885 [D loss: 1.079105] [G loss: 1.237661]\n",
            "4886 [D loss: 1.075331] [G loss: 1.235206]\n",
            "4887 [D loss: 1.069569] [G loss: 1.233788]\n",
            "4888 [D loss: 1.067604] [G loss: 1.217497]\n",
            "4889 [D loss: 1.070375] [G loss: 1.226292]\n",
            "4890 [D loss: 1.071690] [G loss: 1.235957]\n",
            "4891 [D loss: 1.060153] [G loss: 1.237406]\n",
            "4892 [D loss: 1.049952] [G loss: 1.246355]\n",
            "4893 [D loss: 1.056732] [G loss: 1.223367]\n",
            "4894 [D loss: 1.061162] [G loss: 1.235613]\n",
            "4895 [D loss: 1.083627] [G loss: 1.217045]\n",
            "4896 [D loss: 1.072205] [G loss: 1.233814]\n",
            "4897 [D loss: 1.073492] [G loss: 1.232393]\n",
            "4898 [D loss: 1.047433] [G loss: 1.243672]\n",
            "4899 [D loss: 1.054635] [G loss: 1.240889]\n",
            "4900 [D loss: 1.055690] [G loss: 1.246159]\n",
            "[[0.5035671  0.49182945 0.5030865  0.503496  ]\n",
            " [0.50267893 0.49789554 0.5018108  0.5011338 ]\n",
            " [0.70714915 0.32229412 0.54739183 0.45420486]\n",
            " [0.57233775 0.43174666 0.52038944 0.5432863 ]\n",
            " [0.6126506  0.42413834 0.4829975  0.44132173]]\n",
            "4901 [D loss: 1.068623] [G loss: 1.230215]\n",
            "4902 [D loss: 1.054323] [G loss: 1.238019]\n",
            "4903 [D loss: 1.072998] [G loss: 1.233288]\n",
            "4904 [D loss: 1.072209] [G loss: 1.225854]\n",
            "4905 [D loss: 1.071358] [G loss: 1.233896]\n",
            "4906 [D loss: 1.071027] [G loss: 1.237250]\n",
            "4907 [D loss: 1.074835] [G loss: 1.222092]\n",
            "4908 [D loss: 1.055522] [G loss: 1.241684]\n",
            "4909 [D loss: 1.084263] [G loss: 1.224712]\n",
            "4910 [D loss: 1.062318] [G loss: 1.232125]\n",
            "4911 [D loss: 1.064623] [G loss: 1.241592]\n",
            "4912 [D loss: 1.076992] [G loss: 1.227439]\n",
            "4913 [D loss: 1.070506] [G loss: 1.232589]\n",
            "4914 [D loss: 1.053814] [G loss: 1.233101]\n",
            "4915 [D loss: 1.054115] [G loss: 1.241586]\n",
            "4916 [D loss: 1.065743] [G loss: 1.231611]\n",
            "4917 [D loss: 1.069558] [G loss: 1.243331]\n",
            "4918 [D loss: 1.064763] [G loss: 1.230884]\n",
            "4919 [D loss: 1.075029] [G loss: 1.229676]\n",
            "4920 [D loss: 1.067952] [G loss: 1.229641]\n",
            "4921 [D loss: 1.066372] [G loss: 1.236296]\n",
            "4922 [D loss: 1.065211] [G loss: 1.247196]\n",
            "4923 [D loss: 1.084951] [G loss: 1.225115]\n",
            "4924 [D loss: 1.065493] [G loss: 1.229009]\n",
            "4925 [D loss: 1.066021] [G loss: 1.237121]\n",
            "4926 [D loss: 1.058549] [G loss: 1.250592]\n",
            "4927 [D loss: 1.072703] [G loss: 1.232276]\n",
            "4928 [D loss: 1.064641] [G loss: 1.233766]\n",
            "4929 [D loss: 1.062689] [G loss: 1.235659]\n",
            "4930 [D loss: 1.066049] [G loss: 1.245711]\n",
            "4931 [D loss: 1.069468] [G loss: 1.235647]\n",
            "4932 [D loss: 1.059701] [G loss: 1.248818]\n",
            "4933 [D loss: 1.054915] [G loss: 1.234384]\n",
            "4934 [D loss: 1.057541] [G loss: 1.234189]\n",
            "4935 [D loss: 1.051217] [G loss: 1.236626]\n",
            "4936 [D loss: 1.060131] [G loss: 1.242117]\n",
            "4937 [D loss: 1.051188] [G loss: 1.247691]\n",
            "4938 [D loss: 1.068542] [G loss: 1.229211]\n",
            "4939 [D loss: 1.056892] [G loss: 1.239070]\n",
            "4940 [D loss: 1.063727] [G loss: 1.227864]\n",
            "4941 [D loss: 1.073217] [G loss: 1.240976]\n",
            "4942 [D loss: 1.067166] [G loss: 1.234153]\n",
            "4943 [D loss: 1.067296] [G loss: 1.239916]\n",
            "4944 [D loss: 1.056705] [G loss: 1.231107]\n",
            "4945 [D loss: 1.062809] [G loss: 1.224494]\n",
            "4946 [D loss: 1.069814] [G loss: 1.225480]\n",
            "4947 [D loss: 1.063358] [G loss: 1.239198]\n",
            "4948 [D loss: 1.068997] [G loss: 1.235653]\n",
            "4949 [D loss: 1.054192] [G loss: 1.225270]\n",
            "4950 [D loss: 1.044727] [G loss: 1.241358]\n",
            "[[0.5039822  0.48878908 0.503864   0.5046115 ]\n",
            " [0.7658285  0.33827546 0.5501898  0.32747278]\n",
            " [0.54873866 0.49733937 0.51058424 0.4702185 ]\n",
            " [0.5027297  0.49786732 0.5018214  0.5011066 ]\n",
            " [0.6271699  0.44105995 0.49290732 0.43505064]]\n",
            "4951 [D loss: 1.083964] [G loss: 1.223466]\n",
            "4952 [D loss: 1.070057] [G loss: 1.231740]\n",
            "4953 [D loss: 1.050143] [G loss: 1.237753]\n",
            "4954 [D loss: 1.091421] [G loss: 1.221343]\n",
            "4955 [D loss: 1.051399] [G loss: 1.236319]\n",
            "4956 [D loss: 1.070886] [G loss: 1.232330]\n",
            "4957 [D loss: 1.057363] [G loss: 1.239919]\n",
            "4958 [D loss: 1.056789] [G loss: 1.232426]\n",
            "4959 [D loss: 1.054378] [G loss: 1.248808]\n",
            "4960 [D loss: 1.064954] [G loss: 1.238691]\n",
            "4961 [D loss: 1.060711] [G loss: 1.249349]\n",
            "4962 [D loss: 1.047168] [G loss: 1.238674]\n",
            "4963 [D loss: 1.060718] [G loss: 1.233829]\n",
            "4964 [D loss: 1.067908] [G loss: 1.252225]\n",
            "4965 [D loss: 1.064981] [G loss: 1.236775]\n",
            "4966 [D loss: 1.060354] [G loss: 1.228007]\n",
            "4967 [D loss: 1.061187] [G loss: 1.226274]\n",
            "4968 [D loss: 1.057766] [G loss: 1.234134]\n",
            "4969 [D loss: 1.068009] [G loss: 1.228831]\n",
            "4970 [D loss: 1.060012] [G loss: 1.238600]\n",
            "4971 [D loss: 1.070176] [G loss: 1.232791]\n",
            "4972 [D loss: 1.056984] [G loss: 1.243070]\n",
            "4973 [D loss: 1.059357] [G loss: 1.220169]\n",
            "4974 [D loss: 1.048813] [G loss: 1.241366]\n",
            "4975 [D loss: 1.056527] [G loss: 1.227534]\n",
            "4976 [D loss: 1.075876] [G loss: 1.221997]\n",
            "4977 [D loss: 1.061878] [G loss: 1.220564]\n",
            "4978 [D loss: 1.062306] [G loss: 1.247261]\n",
            "4979 [D loss: 1.081814] [G loss: 1.240658]\n",
            "4980 [D loss: 1.066301] [G loss: 1.225304]\n",
            "4981 [D loss: 1.061535] [G loss: 1.227234]\n",
            "4982 [D loss: 1.055917] [G loss: 1.235690]\n",
            "4983 [D loss: 1.087363] [G loss: 1.235893]\n",
            "4984 [D loss: 1.056460] [G loss: 1.234463]\n",
            "4985 [D loss: 1.073491] [G loss: 1.228805]\n",
            "4986 [D loss: 1.073026] [G loss: 1.231840]\n",
            "4987 [D loss: 1.064918] [G loss: 1.230322]\n",
            "4988 [D loss: 1.084742] [G loss: 1.225752]\n",
            "4989 [D loss: 1.051024] [G loss: 1.239319]\n",
            "4990 [D loss: 1.075081] [G loss: 1.228357]\n",
            "4991 [D loss: 1.063376] [G loss: 1.228218]\n",
            "4992 [D loss: 1.079573] [G loss: 1.234977]\n",
            "4993 [D loss: 1.081242] [G loss: 1.220349]\n",
            "4994 [D loss: 1.064064] [G loss: 1.235681]\n",
            "4995 [D loss: 1.056534] [G loss: 1.238629]\n",
            "4996 [D loss: 1.066387] [G loss: 1.242407]\n",
            "4997 [D loss: 1.081249] [G loss: 1.237636]\n",
            "4998 [D loss: 1.042538] [G loss: 1.243264]\n",
            "4999 [D loss: 1.054584] [G loss: 1.243890]\n",
            "5000 [D loss: 1.051759] [G loss: 1.229854]\n",
            "[[0.5613787  0.4427913  0.487799   0.5194118 ]\n",
            " [0.6331873  0.363098   0.54634684 0.5526232 ]\n",
            " [0.5407692  0.5035996  0.50098115 0.46758088]\n",
            " [0.5038526  0.48987845 0.5035313  0.5042991 ]\n",
            " [0.67310464 0.44076535 0.6499755  0.2989159 ]]\n",
            "5001 [D loss: 1.061918] [G loss: 1.233968]\n",
            "5002 [D loss: 1.057393] [G loss: 1.244926]\n",
            "5003 [D loss: 1.075633] [G loss: 1.224152]\n",
            "5004 [D loss: 1.075339] [G loss: 1.235032]\n",
            "5005 [D loss: 1.050116] [G loss: 1.248478]\n",
            "5006 [D loss: 1.071700] [G loss: 1.228204]\n",
            "5007 [D loss: 1.074586] [G loss: 1.222147]\n",
            "5008 [D loss: 1.057223] [G loss: 1.235241]\n",
            "5009 [D loss: 1.065869] [G loss: 1.249255]\n",
            "5010 [D loss: 1.069443] [G loss: 1.236236]\n",
            "5011 [D loss: 1.060463] [G loss: 1.238641]\n",
            "5012 [D loss: 1.051240] [G loss: 1.239507]\n",
            "5013 [D loss: 1.059070] [G loss: 1.231363]\n",
            "5014 [D loss: 1.061311] [G loss: 1.228942]\n",
            "5015 [D loss: 1.061283] [G loss: 1.234411]\n",
            "5016 [D loss: 1.057909] [G loss: 1.232240]\n",
            "5017 [D loss: 1.073465] [G loss: 1.226641]\n",
            "5018 [D loss: 1.065112] [G loss: 1.225071]\n",
            "5019 [D loss: 1.082456] [G loss: 1.227098]\n",
            "5020 [D loss: 1.066386] [G loss: 1.228910]\n",
            "5021 [D loss: 1.070753] [G loss: 1.221637]\n",
            "5022 [D loss: 1.067544] [G loss: 1.234263]\n",
            "5023 [D loss: 1.061796] [G loss: 1.233434]\n",
            "5024 [D loss: 1.062557] [G loss: 1.237870]\n",
            "5025 [D loss: 1.071259] [G loss: 1.227078]\n",
            "5026 [D loss: 1.067596] [G loss: 1.240205]\n",
            "5027 [D loss: 1.076309] [G loss: 1.231140]\n",
            "5028 [D loss: 1.080147] [G loss: 1.227362]\n",
            "5029 [D loss: 1.065305] [G loss: 1.227946]\n",
            "5030 [D loss: 1.059573] [G loss: 1.240936]\n",
            "5031 [D loss: 1.077398] [G loss: 1.242472]\n",
            "5032 [D loss: 1.070898] [G loss: 1.232670]\n",
            "5033 [D loss: 1.072143] [G loss: 1.220631]\n",
            "5034 [D loss: 1.080178] [G loss: 1.216805]\n",
            "5035 [D loss: 1.038215] [G loss: 1.244135]\n",
            "5036 [D loss: 1.069706] [G loss: 1.229577]\n",
            "5037 [D loss: 1.079813] [G loss: 1.233387]\n",
            "5038 [D loss: 1.065925] [G loss: 1.234666]\n",
            "5039 [D loss: 1.072834] [G loss: 1.224344]\n",
            "5040 [D loss: 1.063081] [G loss: 1.228447]\n",
            "5041 [D loss: 1.068007] [G loss: 1.215807]\n",
            "5042 [D loss: 1.072840] [G loss: 1.229472]\n",
            "5043 [D loss: 1.073024] [G loss: 1.231436]\n",
            "5044 [D loss: 1.059675] [G loss: 1.234773]\n",
            "5045 [D loss: 1.065423] [G loss: 1.240013]\n",
            "5046 [D loss: 1.061020] [G loss: 1.220617]\n",
            "5047 [D loss: 1.065523] [G loss: 1.226277]\n",
            "5048 [D loss: 1.091109] [G loss: 1.218789]\n",
            "5049 [D loss: 1.060170] [G loss: 1.232286]\n",
            "5050 [D loss: 1.072410] [G loss: 1.231776]\n",
            "[[0.6159474  0.39254162 0.6171271  0.4449849 ]\n",
            " [0.597446   0.39788327 0.5023608  0.5193558 ]\n",
            " [0.54868    0.47281253 0.4939979  0.47830212]\n",
            " [0.5191816  0.47337246 0.510906   0.4974504 ]\n",
            " [0.58111715 0.43668568 0.53318626 0.46691635]]\n",
            "5051 [D loss: 1.076095] [G loss: 1.223131]\n",
            "5052 [D loss: 1.072527] [G loss: 1.225448]\n",
            "5053 [D loss: 1.076410] [G loss: 1.231515]\n",
            "5054 [D loss: 1.077886] [G loss: 1.227571]\n",
            "5055 [D loss: 1.079499] [G loss: 1.217677]\n",
            "5056 [D loss: 1.054959] [G loss: 1.231217]\n",
            "5057 [D loss: 1.065137] [G loss: 1.239033]\n",
            "5058 [D loss: 1.062848] [G loss: 1.225763]\n",
            "5059 [D loss: 1.056833] [G loss: 1.239753]\n",
            "5060 [D loss: 1.062017] [G loss: 1.220981]\n",
            "5061 [D loss: 1.077426] [G loss: 1.225671]\n",
            "5062 [D loss: 1.086182] [G loss: 1.224014]\n",
            "5063 [D loss: 1.062376] [G loss: 1.230383]\n",
            "5064 [D loss: 1.061322] [G loss: 1.234907]\n",
            "5065 [D loss: 1.086744] [G loss: 1.208307]\n",
            "5066 [D loss: 1.058830] [G loss: 1.236800]\n",
            "5067 [D loss: 1.067341] [G loss: 1.226838]\n",
            "5068 [D loss: 1.060428] [G loss: 1.231871]\n",
            "5069 [D loss: 1.055902] [G loss: 1.224747]\n",
            "5070 [D loss: 1.069403] [G loss: 1.234812]\n",
            "5071 [D loss: 1.069327] [G loss: 1.218945]\n",
            "5072 [D loss: 1.081437] [G loss: 1.237452]\n",
            "5073 [D loss: 1.071120] [G loss: 1.229080]\n",
            "5074 [D loss: 1.075707] [G loss: 1.219252]\n",
            "5075 [D loss: 1.062721] [G loss: 1.226689]\n",
            "5076 [D loss: 1.074668] [G loss: 1.226388]\n",
            "5077 [D loss: 1.069058] [G loss: 1.222919]\n",
            "5078 [D loss: 1.081017] [G loss: 1.219917]\n",
            "5079 [D loss: 1.064891] [G loss: 1.248842]\n",
            "5080 [D loss: 1.085210] [G loss: 1.216220]\n",
            "5081 [D loss: 1.070975] [G loss: 1.228676]\n",
            "5082 [D loss: 1.052452] [G loss: 1.244199]\n",
            "5083 [D loss: 1.057011] [G loss: 1.227409]\n",
            "5084 [D loss: 1.073135] [G loss: 1.224057]\n",
            "5085 [D loss: 1.065488] [G loss: 1.226885]\n",
            "5086 [D loss: 1.066518] [G loss: 1.232851]\n",
            "5087 [D loss: 1.077287] [G loss: 1.225786]\n",
            "5088 [D loss: 1.072854] [G loss: 1.225394]\n",
            "5089 [D loss: 1.077252] [G loss: 1.208876]\n",
            "5090 [D loss: 1.047165] [G loss: 1.243280]\n",
            "5091 [D loss: 1.068482] [G loss: 1.227110]\n",
            "5092 [D loss: 1.075820] [G loss: 1.231675]\n",
            "5093 [D loss: 1.060198] [G loss: 1.242362]\n",
            "5094 [D loss: 1.066499] [G loss: 1.210398]\n",
            "5095 [D loss: 1.069246] [G loss: 1.229000]\n",
            "5096 [D loss: 1.079192] [G loss: 1.225710]\n",
            "5097 [D loss: 1.071397] [G loss: 1.228520]\n",
            "5098 [D loss: 1.060006] [G loss: 1.223230]\n",
            "5099 [D loss: 1.074822] [G loss: 1.220960]\n",
            "5100 [D loss: 1.075556] [G loss: 1.224623]\n",
            "[[0.5831207  0.47591352 0.46956897 0.46826553]\n",
            " [0.8315512  0.33174276 0.7746828  0.13707186]\n",
            " [0.5028837  0.49778157 0.501852   0.5010243 ]\n",
            " [0.75352275 0.3551362  0.64300793 0.31119576]\n",
            " [0.65214103 0.43650016 0.6558877  0.43078002]]\n",
            "5101 [D loss: 1.071414] [G loss: 1.224891]\n",
            "5102 [D loss: 1.070284] [G loss: 1.218956]\n",
            "5103 [D loss: 1.065861] [G loss: 1.221737]\n",
            "5104 [D loss: 1.061684] [G loss: 1.227172]\n",
            "5105 [D loss: 1.052297] [G loss: 1.228295]\n",
            "5106 [D loss: 1.063055] [G loss: 1.235495]\n",
            "5107 [D loss: 1.075473] [G loss: 1.215281]\n",
            "5108 [D loss: 1.092556] [G loss: 1.211649]\n",
            "5109 [D loss: 1.069372] [G loss: 1.231643]\n",
            "5110 [D loss: 1.052799] [G loss: 1.231421]\n",
            "5111 [D loss: 1.071636] [G loss: 1.247825]\n",
            "5112 [D loss: 1.074519] [G loss: 1.222074]\n",
            "5113 [D loss: 1.074542] [G loss: 1.232069]\n",
            "5114 [D loss: 1.070429] [G loss: 1.224242]\n",
            "5115 [D loss: 1.060428] [G loss: 1.231300]\n",
            "5116 [D loss: 1.052739] [G loss: 1.226445]\n",
            "5117 [D loss: 1.066458] [G loss: 1.235160]\n",
            "5118 [D loss: 1.054951] [G loss: 1.235050]\n",
            "5119 [D loss: 1.076157] [G loss: 1.223127]\n",
            "5120 [D loss: 1.078708] [G loss: 1.228921]\n",
            "5121 [D loss: 1.064904] [G loss: 1.241410]\n",
            "5122 [D loss: 1.077093] [G loss: 1.227974]\n",
            "5123 [D loss: 1.070007] [G loss: 1.230379]\n",
            "5124 [D loss: 1.077822] [G loss: 1.220063]\n",
            "5125 [D loss: 1.079856] [G loss: 1.228497]\n",
            "5126 [D loss: 1.069866] [G loss: 1.219055]\n",
            "5127 [D loss: 1.060639] [G loss: 1.233078]\n",
            "5128 [D loss: 1.067481] [G loss: 1.224289]\n",
            "5129 [D loss: 1.059062] [G loss: 1.232896]\n",
            "5130 [D loss: 1.065260] [G loss: 1.230122]\n",
            "5131 [D loss: 1.055380] [G loss: 1.227023]\n",
            "5132 [D loss: 1.074268] [G loss: 1.226561]\n",
            "5133 [D loss: 1.069175] [G loss: 1.235778]\n",
            "5134 [D loss: 1.071246] [G loss: 1.230774]\n",
            "5135 [D loss: 1.066728] [G loss: 1.233358]\n",
            "5136 [D loss: 1.060722] [G loss: 1.240787]\n",
            "5137 [D loss: 1.056445] [G loss: 1.242523]\n",
            "5138 [D loss: 1.069161] [G loss: 1.220177]\n",
            "5139 [D loss: 1.068574] [G loss: 1.228053]\n",
            "5140 [D loss: 1.076170] [G loss: 1.227169]\n",
            "5141 [D loss: 1.088521] [G loss: 1.213598]\n",
            "5142 [D loss: 1.066931] [G loss: 1.228428]\n",
            "5143 [D loss: 1.068210] [G loss: 1.222401]\n",
            "5144 [D loss: 1.077833] [G loss: 1.238735]\n",
            "5145 [D loss: 1.067312] [G loss: 1.220460]\n",
            "5146 [D loss: 1.060464] [G loss: 1.223389]\n",
            "5147 [D loss: 1.048570] [G loss: 1.238309]\n",
            "5148 [D loss: 1.070794] [G loss: 1.224881]\n",
            "5149 [D loss: 1.071364] [G loss: 1.223423]\n",
            "5150 [D loss: 1.073491] [G loss: 1.218586]\n",
            "[[0.51189166 0.49175784 0.507904   0.500586  ]\n",
            " [0.5880191  0.37549925 0.5583962  0.4662579 ]\n",
            " [0.6055231  0.2800086  0.5290463  0.63016826]\n",
            " [0.50299513 0.43965146 0.5626355  0.5203381 ]\n",
            " [0.8876824  0.33520898 0.799583   0.09400843]]\n",
            "5151 [D loss: 1.089384] [G loss: 1.239651]\n",
            "5152 [D loss: 1.068142] [G loss: 1.215945]\n",
            "5153 [D loss: 1.081435] [G loss: 1.226267]\n",
            "5154 [D loss: 1.076832] [G loss: 1.213237]\n",
            "5155 [D loss: 1.058616] [G loss: 1.233710]\n",
            "5156 [D loss: 1.077025] [G loss: 1.221136]\n",
            "5157 [D loss: 1.034298] [G loss: 1.232253]\n",
            "5158 [D loss: 1.057309] [G loss: 1.221616]\n",
            "5159 [D loss: 1.048171] [G loss: 1.231840]\n",
            "5160 [D loss: 1.059766] [G loss: 1.232511]\n",
            "5161 [D loss: 1.052870] [G loss: 1.228914]\n",
            "5162 [D loss: 1.054031] [G loss: 1.234585]\n",
            "5163 [D loss: 1.067279] [G loss: 1.228575]\n",
            "5164 [D loss: 1.077460] [G loss: 1.220055]\n",
            "5165 [D loss: 1.066249] [G loss: 1.232158]\n",
            "5166 [D loss: 1.076368] [G loss: 1.233953]\n",
            "5167 [D loss: 1.083090] [G loss: 1.225599]\n",
            "5168 [D loss: 1.063254] [G loss: 1.227494]\n",
            "5169 [D loss: 1.076555] [G loss: 1.221669]\n",
            "5170 [D loss: 1.065160] [G loss: 1.235356]\n",
            "5171 [D loss: 1.071040] [G loss: 1.217492]\n",
            "5172 [D loss: 1.064849] [G loss: 1.229796]\n",
            "5173 [D loss: 1.068368] [G loss: 1.224139]\n",
            "5174 [D loss: 1.068268] [G loss: 1.227610]\n",
            "5175 [D loss: 1.072756] [G loss: 1.220799]\n",
            "5176 [D loss: 1.084983] [G loss: 1.214643]\n",
            "5177 [D loss: 1.067487] [G loss: 1.230990]\n",
            "5178 [D loss: 1.077259] [G loss: 1.226975]\n",
            "5179 [D loss: 1.054741] [G loss: 1.239054]\n",
            "5180 [D loss: 1.061354] [G loss: 1.237909]\n",
            "5181 [D loss: 1.061158] [G loss: 1.227763]\n",
            "5182 [D loss: 1.055713] [G loss: 1.223848]\n",
            "5183 [D loss: 1.066267] [G loss: 1.228138]\n",
            "5184 [D loss: 1.056315] [G loss: 1.245769]\n",
            "5185 [D loss: 1.066715] [G loss: 1.234422]\n",
            "5186 [D loss: 1.057342] [G loss: 1.235274]\n",
            "5187 [D loss: 1.065687] [G loss: 1.236332]\n",
            "5188 [D loss: 1.055254] [G loss: 1.225435]\n",
            "5189 [D loss: 1.071882] [G loss: 1.220169]\n",
            "5190 [D loss: 1.067823] [G loss: 1.216461]\n",
            "5191 [D loss: 1.060460] [G loss: 1.231810]\n",
            "5192 [D loss: 1.062427] [G loss: 1.240809]\n",
            "5193 [D loss: 1.060916] [G loss: 1.235419]\n",
            "5194 [D loss: 1.058549] [G loss: 1.243524]\n",
            "5195 [D loss: 1.075562] [G loss: 1.224229]\n",
            "5196 [D loss: 1.065086] [G loss: 1.236605]\n",
            "5197 [D loss: 1.088407] [G loss: 1.224702]\n",
            "5198 [D loss: 1.074238] [G loss: 1.227206]\n",
            "5199 [D loss: 1.061596] [G loss: 1.227140]\n",
            "5200 [D loss: 1.067197] [G loss: 1.218962]\n",
            "[[0.745925   0.4772286  0.67831725 0.2660128 ]\n",
            " [0.77779603 0.17214462 0.51487833 0.5634329 ]\n",
            " [0.5768161  0.34415975 0.5065198  0.5821056 ]\n",
            " [0.6208832  0.37000278 0.50582266 0.5201072 ]\n",
            " [0.772164   0.3714227  0.6794826  0.23984353]]\n",
            "5201 [D loss: 1.078640] [G loss: 1.218757]\n",
            "5202 [D loss: 1.075714] [G loss: 1.226427]\n",
            "5203 [D loss: 1.059339] [G loss: 1.232639]\n",
            "5204 [D loss: 1.064739] [G loss: 1.238113]\n",
            "5205 [D loss: 1.066834] [G loss: 1.220549]\n",
            "5206 [D loss: 1.057050] [G loss: 1.220795]\n",
            "5207 [D loss: 1.072052] [G loss: 1.224135]\n",
            "5208 [D loss: 1.072085] [G loss: 1.223514]\n",
            "5209 [D loss: 1.083974] [G loss: 1.213056]\n",
            "5210 [D loss: 1.068069] [G loss: 1.239038]\n",
            "5211 [D loss: 1.071485] [G loss: 1.223843]\n",
            "5212 [D loss: 1.064467] [G loss: 1.239590]\n",
            "5213 [D loss: 1.063845] [G loss: 1.225062]\n",
            "5214 [D loss: 1.069565] [G loss: 1.228473]\n",
            "5215 [D loss: 1.085245] [G loss: 1.222511]\n",
            "5216 [D loss: 1.045357] [G loss: 1.229609]\n",
            "5217 [D loss: 1.060662] [G loss: 1.221740]\n",
            "5218 [D loss: 1.072427] [G loss: 1.218827]\n",
            "5219 [D loss: 1.072805] [G loss: 1.212348]\n",
            "5220 [D loss: 1.078669] [G loss: 1.218193]\n",
            "5221 [D loss: 1.039584] [G loss: 1.235829]\n",
            "5222 [D loss: 1.064693] [G loss: 1.224233]\n",
            "5223 [D loss: 1.077517] [G loss: 1.210213]\n",
            "5224 [D loss: 1.052097] [G loss: 1.243927]\n",
            "5225 [D loss: 1.074101] [G loss: 1.232428]\n",
            "5226 [D loss: 1.055765] [G loss: 1.232025]\n",
            "5227 [D loss: 1.050070] [G loss: 1.234493]\n",
            "5228 [D loss: 1.067377] [G loss: 1.222510]\n",
            "5229 [D loss: 1.074847] [G loss: 1.222074]\n",
            "5230 [D loss: 1.059916] [G loss: 1.230792]\n",
            "5231 [D loss: 1.057115] [G loss: 1.234015]\n",
            "5232 [D loss: 1.086045] [G loss: 1.208350]\n",
            "5233 [D loss: 1.064213] [G loss: 1.229208]\n",
            "5234 [D loss: 1.051173] [G loss: 1.233742]\n",
            "5235 [D loss: 1.083530] [G loss: 1.220577]\n",
            "5236 [D loss: 1.068029] [G loss: 1.230038]\n",
            "5237 [D loss: 1.070085] [G loss: 1.225403]\n",
            "5238 [D loss: 1.070815] [G loss: 1.220697]\n",
            "5239 [D loss: 1.065603] [G loss: 1.232062]\n",
            "5240 [D loss: 1.057663] [G loss: 1.227571]\n",
            "5241 [D loss: 1.068755] [G loss: 1.227687]\n",
            "5242 [D loss: 1.057068] [G loss: 1.238695]\n",
            "5243 [D loss: 1.069363] [G loss: 1.224273]\n",
            "5244 [D loss: 1.072548] [G loss: 1.207195]\n",
            "5245 [D loss: 1.077675] [G loss: 1.214184]\n",
            "5246 [D loss: 1.083802] [G loss: 1.203985]\n",
            "5247 [D loss: 1.067568] [G loss: 1.221503]\n",
            "5248 [D loss: 1.068716] [G loss: 1.217806]\n",
            "5249 [D loss: 1.071792] [G loss: 1.221758]\n",
            "5250 [D loss: 1.071794] [G loss: 1.229218]\n",
            "[[0.5144453  0.480116   0.5081278  0.5110071 ]\n",
            " [0.83746433 0.30370706 0.7009893  0.22399893]\n",
            " [0.7144498  0.41432354 0.6337071  0.31464937]\n",
            " [0.5724741  0.42308244 0.51872593 0.50365216]\n",
            " [0.51323605 0.479326   0.50980496 0.51339066]]\n",
            "5251 [D loss: 1.084472] [G loss: 1.223899]\n",
            "5252 [D loss: 1.067658] [G loss: 1.230926]\n",
            "5253 [D loss: 1.057456] [G loss: 1.236555]\n",
            "5254 [D loss: 1.070494] [G loss: 1.225811]\n",
            "5255 [D loss: 1.057861] [G loss: 1.237185]\n",
            "5256 [D loss: 1.062732] [G loss: 1.227372]\n",
            "5257 [D loss: 1.063460] [G loss: 1.227478]\n",
            "5258 [D loss: 1.067722] [G loss: 1.209601]\n",
            "5259 [D loss: 1.069732] [G loss: 1.216581]\n",
            "5260 [D loss: 1.074718] [G loss: 1.223135]\n",
            "5261 [D loss: 1.070673] [G loss: 1.216113]\n",
            "5262 [D loss: 1.073824] [G loss: 1.214672]\n",
            "5263 [D loss: 1.065234] [G loss: 1.226389]\n",
            "5264 [D loss: 1.067517] [G loss: 1.231656]\n",
            "5265 [D loss: 1.061739] [G loss: 1.239006]\n",
            "5266 [D loss: 1.068559] [G loss: 1.226291]\n",
            "5267 [D loss: 1.063197] [G loss: 1.226100]\n",
            "5268 [D loss: 1.061489] [G loss: 1.227841]\n",
            "5269 [D loss: 1.061903] [G loss: 1.225848]\n",
            "5270 [D loss: 1.074627] [G loss: 1.207353]\n",
            "5271 [D loss: 1.082615] [G loss: 1.219053]\n",
            "5272 [D loss: 1.061537] [G loss: 1.234533]\n",
            "5273 [D loss: 1.071279] [G loss: 1.219435]\n",
            "5274 [D loss: 1.071320] [G loss: 1.222921]\n",
            "5275 [D loss: 1.081303] [G loss: 1.222904]\n",
            "5276 [D loss: 1.054149] [G loss: 1.225201]\n",
            "5277 [D loss: 1.077662] [G loss: 1.221173]\n",
            "5278 [D loss: 1.077932] [G loss: 1.235312]\n",
            "5279 [D loss: 1.071738] [G loss: 1.230555]\n",
            "5280 [D loss: 1.083360] [G loss: 1.203036]\n",
            "5281 [D loss: 1.076670] [G loss: 1.215337]\n",
            "5282 [D loss: 1.075564] [G loss: 1.227166]\n",
            "5283 [D loss: 1.055464] [G loss: 1.221493]\n",
            "5284 [D loss: 1.066929] [G loss: 1.222215]\n",
            "5285 [D loss: 1.061016] [G loss: 1.227691]\n",
            "5286 [D loss: 1.078857] [G loss: 1.226445]\n",
            "5287 [D loss: 1.072633] [G loss: 1.234374]\n",
            "5288 [D loss: 1.054951] [G loss: 1.219832]\n",
            "5289 [D loss: 1.078061] [G loss: 1.232440]\n",
            "5290 [D loss: 1.067318] [G loss: 1.223990]\n",
            "5291 [D loss: 1.063868] [G loss: 1.221526]\n",
            "5292 [D loss: 1.080043] [G loss: 1.216084]\n",
            "5293 [D loss: 1.074753] [G loss: 1.206825]\n",
            "5294 [D loss: 1.075024] [G loss: 1.226374]\n",
            "5295 [D loss: 1.079448] [G loss: 1.231114]\n",
            "5296 [D loss: 1.073869] [G loss: 1.230128]\n",
            "5297 [D loss: 1.079501] [G loss: 1.223398]\n",
            "5298 [D loss: 1.075378] [G loss: 1.216628]\n",
            "5299 [D loss: 1.079493] [G loss: 1.210937]\n",
            "5300 [D loss: 1.070772] [G loss: 1.229402]\n",
            "[[0.50308466 0.4976688  0.5018924  0.5009198 ]\n",
            " [0.75298077 0.31041244 0.6004996  0.4004343 ]\n",
            " [0.5591558  0.46520436 0.5250731  0.48767346]\n",
            " [0.5037138  0.49706852 0.5023757  0.50112206]\n",
            " [0.50116265 0.49492428 0.50254416 0.5068152 ]]\n",
            "5301 [D loss: 1.072810] [G loss: 1.223968]\n",
            "5302 [D loss: 1.060702] [G loss: 1.220457]\n",
            "5303 [D loss: 1.055957] [G loss: 1.224708]\n",
            "5304 [D loss: 1.061964] [G loss: 1.216309]\n",
            "5305 [D loss: 1.068223] [G loss: 1.224097]\n",
            "5306 [D loss: 1.072692] [G loss: 1.232992]\n",
            "5307 [D loss: 1.077781] [G loss: 1.227306]\n",
            "5308 [D loss: 1.069513] [G loss: 1.222919]\n",
            "5309 [D loss: 1.059026] [G loss: 1.222502]\n",
            "5310 [D loss: 1.050669] [G loss: 1.242728]\n",
            "5311 [D loss: 1.060929] [G loss: 1.231688]\n",
            "5312 [D loss: 1.085474] [G loss: 1.217922]\n",
            "5313 [D loss: 1.086891] [G loss: 1.215824]\n",
            "5314 [D loss: 1.058956] [G loss: 1.221129]\n",
            "5315 [D loss: 1.067771] [G loss: 1.221329]\n",
            "5316 [D loss: 1.073258] [G loss: 1.214624]\n",
            "5317 [D loss: 1.078281] [G loss: 1.213152]\n",
            "5318 [D loss: 1.044412] [G loss: 1.224903]\n",
            "5319 [D loss: 1.075627] [G loss: 1.220714]\n",
            "5320 [D loss: 1.079130] [G loss: 1.221312]\n",
            "5321 [D loss: 1.069046] [G loss: 1.219772]\n",
            "5322 [D loss: 1.066391] [G loss: 1.220702]\n",
            "5323 [D loss: 1.078001] [G loss: 1.223203]\n",
            "5324 [D loss: 1.078007] [G loss: 1.221791]\n",
            "5325 [D loss: 1.063656] [G loss: 1.233332]\n",
            "5326 [D loss: 1.069079] [G loss: 1.228888]\n",
            "5327 [D loss: 1.057778] [G loss: 1.233973]\n",
            "5328 [D loss: 1.077768] [G loss: 1.218791]\n",
            "5329 [D loss: 1.081978] [G loss: 1.212607]\n",
            "5330 [D loss: 1.061977] [G loss: 1.219599]\n",
            "5331 [D loss: 1.069208] [G loss: 1.216861]\n",
            "5332 [D loss: 1.071732] [G loss: 1.217643]\n",
            "5333 [D loss: 1.073610] [G loss: 1.223493]\n",
            "5334 [D loss: 1.066172] [G loss: 1.228031]\n",
            "5335 [D loss: 1.080270] [G loss: 1.221340]\n",
            "5336 [D loss: 1.069770] [G loss: 1.227911]\n",
            "5337 [D loss: 1.068388] [G loss: 1.231125]\n",
            "5338 [D loss: 1.076344] [G loss: 1.234040]\n",
            "5339 [D loss: 1.075482] [G loss: 1.215870]\n",
            "5340 [D loss: 1.060777] [G loss: 1.218003]\n",
            "5341 [D loss: 1.071090] [G loss: 1.222856]\n",
            "5342 [D loss: 1.062728] [G loss: 1.233651]\n",
            "5343 [D loss: 1.079196] [G loss: 1.215464]\n",
            "5344 [D loss: 1.074276] [G loss: 1.217026]\n",
            "5345 [D loss: 1.075343] [G loss: 1.211342]\n",
            "5346 [D loss: 1.066304] [G loss: 1.217301]\n",
            "5347 [D loss: 1.074054] [G loss: 1.222644]\n",
            "5348 [D loss: 1.079935] [G loss: 1.214770]\n",
            "5349 [D loss: 1.072400] [G loss: 1.219014]\n",
            "5350 [D loss: 1.066851] [G loss: 1.223370]\n",
            "[[0.80438226 0.32524255 0.69622314 0.24550223]\n",
            " [0.52905464 0.42517567 0.53297263 0.53576756]\n",
            " [0.59942734 0.43519416 0.52725893 0.47858432]\n",
            " [0.5261445  0.48465407 0.5045086  0.4947119 ]\n",
            " [0.9011352  0.28185758 0.818979   0.1001858 ]]\n",
            "5351 [D loss: 1.077424] [G loss: 1.215706]\n",
            "5352 [D loss: 1.072009] [G loss: 1.230064]\n",
            "5353 [D loss: 1.074153] [G loss: 1.208042]\n",
            "5354 [D loss: 1.080757] [G loss: 1.212162]\n",
            "5355 [D loss: 1.085504] [G loss: 1.208907]\n",
            "5356 [D loss: 1.064419] [G loss: 1.214825]\n",
            "5357 [D loss: 1.082541] [G loss: 1.211222]\n",
            "5358 [D loss: 1.080304] [G loss: 1.218685]\n",
            "5359 [D loss: 1.089400] [G loss: 1.218198]\n",
            "5360 [D loss: 1.082920] [G loss: 1.215235]\n",
            "5361 [D loss: 1.069670] [G loss: 1.220417]\n",
            "5362 [D loss: 1.075468] [G loss: 1.223732]\n",
            "5363 [D loss: 1.072104] [G loss: 1.209036]\n",
            "5364 [D loss: 1.075660] [G loss: 1.206557]\n",
            "5365 [D loss: 1.066210] [G loss: 1.213627]\n",
            "5366 [D loss: 1.074589] [G loss: 1.221703]\n",
            "5367 [D loss: 1.081205] [G loss: 1.209493]\n",
            "5368 [D loss: 1.072401] [G loss: 1.220045]\n",
            "5369 [D loss: 1.082841] [G loss: 1.192760]\n",
            "5370 [D loss: 1.073877] [G loss: 1.223984]\n",
            "5371 [D loss: 1.091917] [G loss: 1.203324]\n",
            "5372 [D loss: 1.071143] [G loss: 1.210496]\n",
            "5373 [D loss: 1.074841] [G loss: 1.228136]\n",
            "5374 [D loss: 1.068084] [G loss: 1.229183]\n",
            "5375 [D loss: 1.069686] [G loss: 1.212363]\n",
            "5376 [D loss: 1.064335] [G loss: 1.236989]\n",
            "5377 [D loss: 1.076972] [G loss: 1.208325]\n",
            "5378 [D loss: 1.064138] [G loss: 1.227795]\n",
            "5379 [D loss: 1.077401] [G loss: 1.210842]\n",
            "5380 [D loss: 1.078811] [G loss: 1.216426]\n",
            "5381 [D loss: 1.075117] [G loss: 1.228941]\n",
            "5382 [D loss: 1.067183] [G loss: 1.228636]\n",
            "5383 [D loss: 1.077624] [G loss: 1.214380]\n",
            "5384 [D loss: 1.063764] [G loss: 1.226607]\n",
            "5385 [D loss: 1.078463] [G loss: 1.229351]\n",
            "5386 [D loss: 1.077198] [G loss: 1.217190]\n",
            "5387 [D loss: 1.085212] [G loss: 1.218120]\n",
            "5388 [D loss: 1.075354] [G loss: 1.214757]\n",
            "5389 [D loss: 1.068720] [G loss: 1.211551]\n",
            "5390 [D loss: 1.069872] [G loss: 1.227380]\n",
            "5391 [D loss: 1.075736] [G loss: 1.214732]\n",
            "5392 [D loss: 1.064533] [G loss: 1.235120]\n",
            "5393 [D loss: 1.060177] [G loss: 1.222938]\n",
            "5394 [D loss: 1.071392] [G loss: 1.215218]\n",
            "5395 [D loss: 1.076930] [G loss: 1.215874]\n",
            "5396 [D loss: 1.090935] [G loss: 1.219168]\n",
            "5397 [D loss: 1.075393] [G loss: 1.229137]\n",
            "5398 [D loss: 1.073326] [G loss: 1.217153]\n",
            "5399 [D loss: 1.076726] [G loss: 1.221626]\n",
            "5400 [D loss: 1.060574] [G loss: 1.234270]\n",
            "[[0.6322424  0.39300996 0.50795215 0.49312147]\n",
            " [0.7527763  0.30180356 0.5768277  0.4574937 ]\n",
            " [0.6049452  0.30198365 0.57061154 0.5555091 ]\n",
            " [0.638548   0.34968933 0.5417891  0.44342998]\n",
            " [0.6583857  0.40981436 0.57037324 0.4653303 ]]\n",
            "5401 [D loss: 1.076674] [G loss: 1.226437]\n",
            "5402 [D loss: 1.071019] [G loss: 1.227189]\n",
            "5403 [D loss: 1.072583] [G loss: 1.214187]\n",
            "5404 [D loss: 1.061468] [G loss: 1.223045]\n",
            "5405 [D loss: 1.070824] [G loss: 1.229304]\n",
            "5406 [D loss: 1.069115] [G loss: 1.208891]\n",
            "5407 [D loss: 1.073875] [G loss: 1.225236]\n",
            "5408 [D loss: 1.072400] [G loss: 1.209438]\n",
            "5409 [D loss: 1.070741] [G loss: 1.220760]\n",
            "5410 [D loss: 1.063843] [G loss: 1.217279]\n",
            "5411 [D loss: 1.049451] [G loss: 1.219863]\n",
            "5412 [D loss: 1.067461] [G loss: 1.226231]\n",
            "5413 [D loss: 1.058547] [G loss: 1.231424]\n",
            "5414 [D loss: 1.075464] [G loss: 1.210769]\n",
            "5415 [D loss: 1.071897] [G loss: 1.230411]\n",
            "5416 [D loss: 1.064559] [G loss: 1.219706]\n",
            "5417 [D loss: 1.076324] [G loss: 1.213021]\n",
            "5418 [D loss: 1.079987] [G loss: 1.236084]\n",
            "5419 [D loss: 1.079858] [G loss: 1.217841]\n",
            "5420 [D loss: 1.071732] [G loss: 1.212060]\n",
            "5421 [D loss: 1.082325] [G loss: 1.214770]\n",
            "5422 [D loss: 1.073798] [G loss: 1.212951]\n",
            "5423 [D loss: 1.065764] [G loss: 1.229432]\n",
            "5424 [D loss: 1.080595] [G loss: 1.208762]\n",
            "5425 [D loss: 1.072341] [G loss: 1.211099]\n",
            "5426 [D loss: 1.071796] [G loss: 1.212378]\n",
            "5427 [D loss: 1.078045] [G loss: 1.214548]\n",
            "5428 [D loss: 1.075965] [G loss: 1.213585]\n",
            "5429 [D loss: 1.064759] [G loss: 1.233548]\n",
            "5430 [D loss: 1.075861] [G loss: 1.222664]\n",
            "5431 [D loss: 1.053723] [G loss: 1.227396]\n",
            "5432 [D loss: 1.082593] [G loss: 1.214450]\n",
            "5433 [D loss: 1.068927] [G loss: 1.212438]\n",
            "5434 [D loss: 1.079057] [G loss: 1.211380]\n",
            "5435 [D loss: 1.068612] [G loss: 1.217705]\n",
            "5436 [D loss: 1.078848] [G loss: 1.213691]\n",
            "5437 [D loss: 1.088083] [G loss: 1.216592]\n",
            "5438 [D loss: 1.070157] [G loss: 1.207398]\n",
            "5439 [D loss: 1.080917] [G loss: 1.212911]\n",
            "5440 [D loss: 1.069113] [G loss: 1.217963]\n",
            "5441 [D loss: 1.072549] [G loss: 1.232965]\n",
            "5442 [D loss: 1.066700] [G loss: 1.216143]\n",
            "5443 [D loss: 1.087310] [G loss: 1.207010]\n",
            "5444 [D loss: 1.064888] [G loss: 1.213851]\n",
            "5445 [D loss: 1.077887] [G loss: 1.230112]\n",
            "5446 [D loss: 1.056115] [G loss: 1.219489]\n",
            "5447 [D loss: 1.078934] [G loss: 1.223681]\n",
            "5448 [D loss: 1.081709] [G loss: 1.197559]\n",
            "5449 [D loss: 1.071163] [G loss: 1.210064]\n",
            "5450 [D loss: 1.054955] [G loss: 1.232023]\n",
            "[[0.5277537  0.48254308 0.5135869  0.49654692]\n",
            " [0.57467604 0.44706783 0.47410527 0.5065055 ]\n",
            " [0.8243089  0.2998874  0.6382214  0.32254833]\n",
            " [0.67273927 0.4131048  0.519461   0.44892216]\n",
            " [0.629603   0.47338685 0.50125605 0.42433247]]\n",
            "5451 [D loss: 1.078529] [G loss: 1.205646]\n",
            "5452 [D loss: 1.085740] [G loss: 1.203035]\n",
            "5453 [D loss: 1.075345] [G loss: 1.210353]\n",
            "5454 [D loss: 1.082105] [G loss: 1.208583]\n",
            "5455 [D loss: 1.079188] [G loss: 1.212883]\n",
            "5456 [D loss: 1.059137] [G loss: 1.218549]\n",
            "5457 [D loss: 1.068071] [G loss: 1.237864]\n",
            "5458 [D loss: 1.076942] [G loss: 1.223987]\n",
            "5459 [D loss: 1.080421] [G loss: 1.211313]\n",
            "5460 [D loss: 1.068556] [G loss: 1.219589]\n",
            "5461 [D loss: 1.073428] [G loss: 1.214110]\n",
            "5462 [D loss: 1.080300] [G loss: 1.210643]\n",
            "5463 [D loss: 1.067924] [G loss: 1.229261]\n",
            "5464 [D loss: 1.089197] [G loss: 1.216761]\n",
            "5465 [D loss: 1.080576] [G loss: 1.220937]\n",
            "5466 [D loss: 1.071286] [G loss: 1.225429]\n",
            "5467 [D loss: 1.069912] [G loss: 1.212455]\n",
            "5468 [D loss: 1.065317] [G loss: 1.220892]\n",
            "5469 [D loss: 1.078602] [G loss: 1.217400]\n",
            "5470 [D loss: 1.073428] [G loss: 1.216638]\n",
            "5471 [D loss: 1.062218] [G loss: 1.218058]\n",
            "5472 [D loss: 1.081737] [G loss: 1.199136]\n",
            "5473 [D loss: 1.094392] [G loss: 1.195466]\n",
            "5474 [D loss: 1.067482] [G loss: 1.224309]\n",
            "5475 [D loss: 1.062747] [G loss: 1.211909]\n",
            "5476 [D loss: 1.060717] [G loss: 1.223747]\n",
            "5477 [D loss: 1.067263] [G loss: 1.225520]\n",
            "5478 [D loss: 1.068846] [G loss: 1.231985]\n",
            "5479 [D loss: 1.061670] [G loss: 1.235163]\n",
            "5480 [D loss: 1.070236] [G loss: 1.230276]\n",
            "5481 [D loss: 1.076161] [G loss: 1.204627]\n",
            "5482 [D loss: 1.079615] [G loss: 1.205246]\n",
            "5483 [D loss: 1.072655] [G loss: 1.212421]\n",
            "5484 [D loss: 1.069344] [G loss: 1.226271]\n",
            "5485 [D loss: 1.056088] [G loss: 1.220030]\n",
            "5486 [D loss: 1.085728] [G loss: 1.222911]\n",
            "5487 [D loss: 1.062404] [G loss: 1.234319]\n",
            "5488 [D loss: 1.054494] [G loss: 1.212180]\n",
            "5489 [D loss: 1.056905] [G loss: 1.217189]\n",
            "5490 [D loss: 1.051476] [G loss: 1.220924]\n",
            "5491 [D loss: 1.076974] [G loss: 1.218050]\n",
            "5492 [D loss: 1.078296] [G loss: 1.216968]\n",
            "5493 [D loss: 1.052813] [G loss: 1.232990]\n",
            "5494 [D loss: 1.076541] [G loss: 1.214814]\n",
            "5495 [D loss: 1.060131] [G loss: 1.225380]\n",
            "5496 [D loss: 1.084125] [G loss: 1.220105]\n",
            "5497 [D loss: 1.068761] [G loss: 1.221962]\n",
            "5498 [D loss: 1.071918] [G loss: 1.217227]\n",
            "5499 [D loss: 1.083435] [G loss: 1.198430]\n",
            "5500 [D loss: 1.084394] [G loss: 1.217133]\n",
            "[[0.5032621  0.49756426 0.50193346 0.5008468 ]\n",
            " [0.5032621  0.49756426 0.50193346 0.5008468 ]\n",
            " [0.59505624 0.44902635 0.510008   0.45798466]\n",
            " [0.71646774 0.39000985 0.6551375  0.29478708]\n",
            " [0.8343781  0.34451085 0.76544005 0.15077482]]\n",
            "5501 [D loss: 1.087329] [G loss: 1.201203]\n",
            "5502 [D loss: 1.061074] [G loss: 1.214685]\n",
            "5503 [D loss: 1.080854] [G loss: 1.205396]\n",
            "5504 [D loss: 1.074180] [G loss: 1.246590]\n",
            "5505 [D loss: 1.060815] [G loss: 1.218041]\n",
            "5506 [D loss: 1.056418] [G loss: 1.225811]\n",
            "5507 [D loss: 1.071499] [G loss: 1.224668]\n",
            "5508 [D loss: 1.078882] [G loss: 1.213330]\n",
            "5509 [D loss: 1.080969] [G loss: 1.214516]\n",
            "5510 [D loss: 1.069765] [G loss: 1.221174]\n",
            "5511 [D loss: 1.072177] [G loss: 1.232520]\n",
            "5512 [D loss: 1.082280] [G loss: 1.206799]\n",
            "5513 [D loss: 1.052980] [G loss: 1.224904]\n",
            "5514 [D loss: 1.072736] [G loss: 1.219780]\n",
            "5515 [D loss: 1.075420] [G loss: 1.205864]\n",
            "5516 [D loss: 1.062404] [G loss: 1.225610]\n",
            "5517 [D loss: 1.070475] [G loss: 1.235459]\n",
            "5518 [D loss: 1.076085] [G loss: 1.228941]\n",
            "5519 [D loss: 1.088443] [G loss: 1.204868]\n",
            "5520 [D loss: 1.081651] [G loss: 1.213208]\n",
            "5521 [D loss: 1.071033] [G loss: 1.218573]\n",
            "5522 [D loss: 1.063725] [G loss: 1.232219]\n",
            "5523 [D loss: 1.062634] [G loss: 1.228309]\n",
            "5524 [D loss: 1.057016] [G loss: 1.223089]\n",
            "5525 [D loss: 1.075971] [G loss: 1.207405]\n",
            "5526 [D loss: 1.070867] [G loss: 1.224446]\n",
            "5527 [D loss: 1.071972] [G loss: 1.221045]\n",
            "5528 [D loss: 1.063192] [G loss: 1.215799]\n",
            "5529 [D loss: 1.081902] [G loss: 1.204306]\n",
            "5530 [D loss: 1.070609] [G loss: 1.218793]\n",
            "5531 [D loss: 1.070694] [G loss: 1.219572]\n",
            "5532 [D loss: 1.080723] [G loss: 1.206959]\n",
            "5533 [D loss: 1.083405] [G loss: 1.211999]\n",
            "5534 [D loss: 1.072681] [G loss: 1.201947]\n",
            "5535 [D loss: 1.080227] [G loss: 1.214488]\n",
            "5536 [D loss: 1.071798] [G loss: 1.227590]\n",
            "5537 [D loss: 1.071361] [G loss: 1.220321]\n",
            "5538 [D loss: 1.076503] [G loss: 1.210520]\n",
            "5539 [D loss: 1.046729] [G loss: 1.221049]\n",
            "5540 [D loss: 1.072629] [G loss: 1.217126]\n",
            "5541 [D loss: 1.057528] [G loss: 1.213620]\n",
            "5542 [D loss: 1.074374] [G loss: 1.217248]\n",
            "5543 [D loss: 1.086338] [G loss: 1.207475]\n",
            "5544 [D loss: 1.074518] [G loss: 1.217570]\n",
            "5545 [D loss: 1.076314] [G loss: 1.215780]\n",
            "5546 [D loss: 1.081766] [G loss: 1.229982]\n",
            "5547 [D loss: 1.082798] [G loss: 1.207405]\n",
            "5548 [D loss: 1.070396] [G loss: 1.209936]\n",
            "5549 [D loss: 1.069418] [G loss: 1.210875]\n",
            "5550 [D loss: 1.063310] [G loss: 1.217702]\n",
            "[[0.7299022  0.30066454 0.54293495 0.42642194]\n",
            " [0.5033064  0.497538   0.50194365 0.5008288 ]\n",
            " [0.50737655 0.48612523 0.5057382  0.5146958 ]\n",
            " [0.75512975 0.36779544 0.71818686 0.21115455]\n",
            " [0.52083975 0.43212822 0.56545395 0.54591   ]]\n",
            "5551 [D loss: 1.073833] [G loss: 1.221865]\n",
            "5552 [D loss: 1.068319] [G loss: 1.202214]\n",
            "5553 [D loss: 1.073834] [G loss: 1.217223]\n",
            "5554 [D loss: 1.085501] [G loss: 1.216406]\n",
            "5555 [D loss: 1.064778] [G loss: 1.213015]\n",
            "5556 [D loss: 1.068157] [G loss: 1.215406]\n",
            "5557 [D loss: 1.078143] [G loss: 1.207746]\n",
            "5558 [D loss: 1.084394] [G loss: 1.208156]\n",
            "5559 [D loss: 1.074883] [G loss: 1.213202]\n",
            "5560 [D loss: 1.062522] [G loss: 1.214566]\n",
            "5561 [D loss: 1.072940] [G loss: 1.209331]\n",
            "5562 [D loss: 1.071406] [G loss: 1.223679]\n",
            "5563 [D loss: 1.072647] [G loss: 1.212980]\n",
            "5564 [D loss: 1.080858] [G loss: 1.220648]\n",
            "5565 [D loss: 1.084878] [G loss: 1.215058]\n",
            "5566 [D loss: 1.078138] [G loss: 1.219223]\n",
            "5567 [D loss: 1.047821] [G loss: 1.231308]\n",
            "5568 [D loss: 1.081122] [G loss: 1.201375]\n",
            "5569 [D loss: 1.066432] [G loss: 1.226234]\n",
            "5570 [D loss: 1.078694] [G loss: 1.217953]\n",
            "5571 [D loss: 1.069331] [G loss: 1.212876]\n",
            "5572 [D loss: 1.060243] [G loss: 1.216734]\n",
            "5573 [D loss: 1.078112] [G loss: 1.212870]\n",
            "5574 [D loss: 1.073671] [G loss: 1.227874]\n",
            "5575 [D loss: 1.066068] [G loss: 1.197006]\n",
            "5576 [D loss: 1.073245] [G loss: 1.213142]\n",
            "5577 [D loss: 1.075510] [G loss: 1.220139]\n",
            "5578 [D loss: 1.087986] [G loss: 1.209615]\n",
            "5579 [D loss: 1.066293] [G loss: 1.227654]\n",
            "5580 [D loss: 1.076198] [G loss: 1.202068]\n",
            "5581 [D loss: 1.090948] [G loss: 1.204460]\n",
            "5582 [D loss: 1.081527] [G loss: 1.201586]\n",
            "5583 [D loss: 1.086704] [G loss: 1.206808]\n",
            "5584 [D loss: 1.085212] [G loss: 1.196858]\n",
            "5585 [D loss: 1.070619] [G loss: 1.222840]\n",
            "5586 [D loss: 1.075388] [G loss: 1.222326]\n",
            "5587 [D loss: 1.068844] [G loss: 1.209893]\n",
            "5588 [D loss: 1.079087] [G loss: 1.211703]\n",
            "5589 [D loss: 1.067721] [G loss: 1.211112]\n",
            "5590 [D loss: 1.071624] [G loss: 1.222462]\n",
            "5591 [D loss: 1.075401] [G loss: 1.220606]\n",
            "5592 [D loss: 1.077862] [G loss: 1.209353]\n",
            "5593 [D loss: 1.066369] [G loss: 1.229049]\n",
            "5594 [D loss: 1.056531] [G loss: 1.217199]\n",
            "5595 [D loss: 1.089853] [G loss: 1.220059]\n",
            "5596 [D loss: 1.079638] [G loss: 1.213021]\n",
            "5597 [D loss: 1.082180] [G loss: 1.217676]\n",
            "5598 [D loss: 1.084913] [G loss: 1.206307]\n",
            "5599 [D loss: 1.076150] [G loss: 1.218797]\n",
            "5600 [D loss: 1.063384] [G loss: 1.223917]\n",
            "[[0.5033508  0.49751168 0.5019539  0.5008106 ]\n",
            " [0.5148955  0.48535016 0.508318   0.49748605]\n",
            " [0.5033508  0.49751168 0.5019539  0.5008106 ]\n",
            " [0.56622136 0.45200714 0.47643873 0.5109768 ]\n",
            " [0.5703741  0.43702117 0.5375289  0.45015365]]\n",
            "5601 [D loss: 1.074325] [G loss: 1.225639]\n",
            "5602 [D loss: 1.078921] [G loss: 1.206644]\n",
            "5603 [D loss: 1.074901] [G loss: 1.212262]\n",
            "5604 [D loss: 1.076959] [G loss: 1.217890]\n",
            "5605 [D loss: 1.071883] [G loss: 1.218169]\n",
            "5606 [D loss: 1.078408] [G loss: 1.211953]\n",
            "5607 [D loss: 1.064400] [G loss: 1.203055]\n",
            "5608 [D loss: 1.071372] [G loss: 1.220289]\n",
            "5609 [D loss: 1.069383] [G loss: 1.223277]\n",
            "5610 [D loss: 1.074454] [G loss: 1.217601]\n",
            "5611 [D loss: 1.061069] [G loss: 1.222473]\n",
            "5612 [D loss: 1.068637] [G loss: 1.208401]\n",
            "5613 [D loss: 1.069985] [G loss: 1.211081]\n",
            "5614 [D loss: 1.070055] [G loss: 1.201116]\n",
            "5615 [D loss: 1.054899] [G loss: 1.221205]\n",
            "5616 [D loss: 1.074934] [G loss: 1.205601]\n",
            "5617 [D loss: 1.055331] [G loss: 1.222076]\n",
            "5618 [D loss: 1.076085] [G loss: 1.212156]\n",
            "5619 [D loss: 1.083929] [G loss: 1.208420]\n",
            "5620 [D loss: 1.065019] [G loss: 1.219888]\n",
            "5621 [D loss: 1.077276] [G loss: 1.204466]\n",
            "5622 [D loss: 1.079440] [G loss: 1.211795]\n",
            "5623 [D loss: 1.062116] [G loss: 1.212008]\n",
            "5624 [D loss: 1.071524] [G loss: 1.212405]\n",
            "5625 [D loss: 1.067209] [G loss: 1.215781]\n",
            "5626 [D loss: 1.074595] [G loss: 1.203525]\n",
            "5627 [D loss: 1.066987] [G loss: 1.203298]\n",
            "5628 [D loss: 1.074930] [G loss: 1.238403]\n",
            "5629 [D loss: 1.073652] [G loss: 1.212794]\n",
            "5630 [D loss: 1.090283] [G loss: 1.208788]\n",
            "5631 [D loss: 1.072450] [G loss: 1.216871]\n",
            "5632 [D loss: 1.058124] [G loss: 1.211231]\n",
            "5633 [D loss: 1.055013] [G loss: 1.232083]\n",
            "5634 [D loss: 1.069038] [G loss: 1.202398]\n",
            "5635 [D loss: 1.063235] [G loss: 1.218781]\n",
            "5636 [D loss: 1.080190] [G loss: 1.214435]\n",
            "5637 [D loss: 1.078700] [G loss: 1.203545]\n",
            "5638 [D loss: 1.066947] [G loss: 1.208986]\n",
            "5639 [D loss: 1.086032] [G loss: 1.203499]\n",
            "5640 [D loss: 1.071851] [G loss: 1.218296]\n",
            "5641 [D loss: 1.085580] [G loss: 1.197581]\n",
            "5642 [D loss: 1.081290] [G loss: 1.199993]\n",
            "5643 [D loss: 1.068083] [G loss: 1.216796]\n",
            "5644 [D loss: 1.085361] [G loss: 1.205548]\n",
            "5645 [D loss: 1.092795] [G loss: 1.204125]\n",
            "5646 [D loss: 1.070972] [G loss: 1.203235]\n",
            "5647 [D loss: 1.072956] [G loss: 1.211949]\n",
            "5648 [D loss: 1.093965] [G loss: 1.203367]\n",
            "5649 [D loss: 1.074325] [G loss: 1.218796]\n",
            "5650 [D loss: 1.081388] [G loss: 1.210441]\n",
            "[[0.6581007  0.4325086  0.6013533  0.34719196]\n",
            " [0.8554275  0.2371819  0.68480027 0.2859415 ]\n",
            " [0.6165576  0.34954473 0.6305415  0.44502145]\n",
            " [0.5062335  0.47929075 0.5062266  0.50837964]\n",
            " [0.50339544 0.49748546 0.501964   0.50079226]]\n",
            "5651 [D loss: 1.064979] [G loss: 1.212504]\n",
            "5652 [D loss: 1.058964] [G loss: 1.225592]\n",
            "5653 [D loss: 1.083790] [G loss: 1.208201]\n",
            "5654 [D loss: 1.079583] [G loss: 1.212152]\n",
            "5655 [D loss: 1.090171] [G loss: 1.205815]\n",
            "5656 [D loss: 1.062779] [G loss: 1.210059]\n",
            "5657 [D loss: 1.067799] [G loss: 1.209459]\n",
            "5658 [D loss: 1.064271] [G loss: 1.202794]\n",
            "5659 [D loss: 1.072913] [G loss: 1.201930]\n",
            "5660 [D loss: 1.089749] [G loss: 1.211477]\n",
            "5661 [D loss: 1.084296] [G loss: 1.208421]\n",
            "5662 [D loss: 1.077598] [G loss: 1.212687]\n",
            "5663 [D loss: 1.083632] [G loss: 1.193683]\n",
            "5664 [D loss: 1.078174] [G loss: 1.222879]\n",
            "5665 [D loss: 1.060356] [G loss: 1.221289]\n",
            "5666 [D loss: 1.080227] [G loss: 1.206172]\n",
            "5667 [D loss: 1.082301] [G loss: 1.212832]\n",
            "5668 [D loss: 1.081896] [G loss: 1.198366]\n",
            "5669 [D loss: 1.074960] [G loss: 1.220711]\n",
            "5670 [D loss: 1.081354] [G loss: 1.216254]\n",
            "5671 [D loss: 1.065515] [G loss: 1.210978]\n",
            "5672 [D loss: 1.090791] [G loss: 1.197364]\n",
            "5673 [D loss: 1.091387] [G loss: 1.194924]\n",
            "5674 [D loss: 1.085127] [G loss: 1.194231]\n",
            "5675 [D loss: 1.079493] [G loss: 1.201033]\n",
            "5676 [D loss: 1.083818] [G loss: 1.207458]\n",
            "5677 [D loss: 1.078102] [G loss: 1.201748]\n",
            "5678 [D loss: 1.054781] [G loss: 1.231258]\n",
            "5679 [D loss: 1.093471] [G loss: 1.203394]\n",
            "5680 [D loss: 1.074145] [G loss: 1.228371]\n",
            "5681 [D loss: 1.091347] [G loss: 1.195958]\n",
            "5682 [D loss: 1.081507] [G loss: 1.196352]\n",
            "5683 [D loss: 1.068797] [G loss: 1.214049]\n",
            "5684 [D loss: 1.088532] [G loss: 1.210652]\n",
            "5685 [D loss: 1.067350] [G loss: 1.211845]\n",
            "5686 [D loss: 1.078428] [G loss: 1.210988]\n",
            "5687 [D loss: 1.066684] [G loss: 1.224254]\n",
            "5688 [D loss: 1.076310] [G loss: 1.215976]\n",
            "5689 [D loss: 1.068439] [G loss: 1.216317]\n",
            "5690 [D loss: 1.069921] [G loss: 1.211297]\n",
            "5691 [D loss: 1.070456] [G loss: 1.201468]\n",
            "5692 [D loss: 1.071178] [G loss: 1.225193]\n",
            "5693 [D loss: 1.068695] [G loss: 1.204852]\n",
            "5694 [D loss: 1.079375] [G loss: 1.207946]\n",
            "5695 [D loss: 1.075402] [G loss: 1.200279]\n",
            "5696 [D loss: 1.081939] [G loss: 1.219779]\n",
            "5697 [D loss: 1.081597] [G loss: 1.219983]\n",
            "5698 [D loss: 1.078578] [G loss: 1.205642]\n",
            "5699 [D loss: 1.076415] [G loss: 1.208067]\n",
            "5700 [D loss: 1.070396] [G loss: 1.207344]\n",
            "[[0.5113908  0.4929037  0.50720966 0.5032149 ]\n",
            " [0.60539126 0.4420473  0.46671638 0.47021556]\n",
            " [0.5034399  0.49745905 0.5019743  0.5007741 ]\n",
            " [0.60799015 0.36226052 0.5215235  0.5352747 ]\n",
            " [0.55683964 0.4681796  0.50308764 0.46794862]]\n",
            "5701 [D loss: 1.074727] [G loss: 1.202609]\n",
            "5702 [D loss: 1.079321] [G loss: 1.212302]\n",
            "5703 [D loss: 1.061889] [G loss: 1.216933]\n",
            "5704 [D loss: 1.070896] [G loss: 1.211045]\n",
            "5705 [D loss: 1.078087] [G loss: 1.213444]\n",
            "5706 [D loss: 1.077308] [G loss: 1.199776]\n",
            "5707 [D loss: 1.088279] [G loss: 1.192446]\n",
            "5708 [D loss: 1.080756] [G loss: 1.210526]\n",
            "5709 [D loss: 1.083835] [G loss: 1.200743]\n",
            "5710 [D loss: 1.073282] [G loss: 1.222878]\n",
            "5711 [D loss: 1.063735] [G loss: 1.213915]\n",
            "5712 [D loss: 1.076674] [G loss: 1.194500]\n",
            "5713 [D loss: 1.065306] [G loss: 1.223963]\n",
            "5714 [D loss: 1.079532] [G loss: 1.217622]\n",
            "5715 [D loss: 1.086340] [G loss: 1.201243]\n",
            "5716 [D loss: 1.075288] [G loss: 1.205996]\n",
            "5717 [D loss: 1.073059] [G loss: 1.208975]\n",
            "5718 [D loss: 1.078467] [G loss: 1.212992]\n",
            "5719 [D loss: 1.081920] [G loss: 1.215533]\n",
            "5720 [D loss: 1.073259] [G loss: 1.220173]\n",
            "5721 [D loss: 1.071187] [G loss: 1.220600]\n",
            "5722 [D loss: 1.085000] [G loss: 1.208345]\n",
            "5723 [D loss: 1.082111] [G loss: 1.192687]\n",
            "5724 [D loss: 1.079945] [G loss: 1.222461]\n",
            "5725 [D loss: 1.074627] [G loss: 1.213013]\n",
            "5726 [D loss: 1.080086] [G loss: 1.216349]\n",
            "5727 [D loss: 1.079483] [G loss: 1.219973]\n",
            "5728 [D loss: 1.067200] [G loss: 1.205114]\n",
            "5729 [D loss: 1.071917] [G loss: 1.217896]\n",
            "5730 [D loss: 1.080946] [G loss: 1.203230]\n",
            "5731 [D loss: 1.073490] [G loss: 1.202546]\n",
            "5732 [D loss: 1.073303] [G loss: 1.210418]\n",
            "5733 [D loss: 1.081905] [G loss: 1.208437]\n",
            "5734 [D loss: 1.059468] [G loss: 1.213581]\n",
            "5735 [D loss: 1.068564] [G loss: 1.209662]\n",
            "5736 [D loss: 1.090295] [G loss: 1.200094]\n",
            "5737 [D loss: 1.079352] [G loss: 1.202440]\n",
            "5738 [D loss: 1.075597] [G loss: 1.205142]\n",
            "5739 [D loss: 1.092588] [G loss: 1.197107]\n",
            "5740 [D loss: 1.082288] [G loss: 1.208495]\n",
            "5741 [D loss: 1.071567] [G loss: 1.218131]\n",
            "5742 [D loss: 1.079641] [G loss: 1.203056]\n",
            "5743 [D loss: 1.073591] [G loss: 1.203952]\n",
            "5744 [D loss: 1.067747] [G loss: 1.223785]\n",
            "5745 [D loss: 1.078129] [G loss: 1.215165]\n",
            "5746 [D loss: 1.063162] [G loss: 1.212465]\n",
            "5747 [D loss: 1.072738] [G loss: 1.207696]\n",
            "5748 [D loss: 1.064978] [G loss: 1.221753]\n",
            "5749 [D loss: 1.077663] [G loss: 1.222390]\n",
            "5750 [D loss: 1.087231] [G loss: 1.206043]\n",
            "[[0.720258   0.42793155 0.6315031  0.27856046]\n",
            " [0.62926894 0.4028201  0.5493565  0.47424045]\n",
            " [0.51416385 0.46069282 0.50703967 0.5263821 ]\n",
            " [0.891079   0.14240886 0.6460337  0.34987506]\n",
            " [0.5856303  0.3968752  0.5627086  0.55984783]]\n",
            "5751 [D loss: 1.070122] [G loss: 1.217252]\n",
            "5752 [D loss: 1.065342] [G loss: 1.210332]\n",
            "5753 [D loss: 1.077930] [G loss: 1.213795]\n",
            "5754 [D loss: 1.076463] [G loss: 1.207337]\n",
            "5755 [D loss: 1.083468] [G loss: 1.200648]\n",
            "5756 [D loss: 1.060584] [G loss: 1.216741]\n",
            "5757 [D loss: 1.086075] [G loss: 1.203877]\n",
            "5758 [D loss: 1.074569] [G loss: 1.210034]\n",
            "5759 [D loss: 1.072559] [G loss: 1.200901]\n",
            "5760 [D loss: 1.053453] [G loss: 1.215244]\n",
            "5761 [D loss: 1.081866] [G loss: 1.211970]\n",
            "5762 [D loss: 1.056210] [G loss: 1.213063]\n",
            "5763 [D loss: 1.069371] [G loss: 1.217650]\n",
            "5764 [D loss: 1.073609] [G loss: 1.197165]\n",
            "5765 [D loss: 1.076185] [G loss: 1.213056]\n",
            "5766 [D loss: 1.083543] [G loss: 1.212032]\n",
            "5767 [D loss: 1.081566] [G loss: 1.196277]\n",
            "5768 [D loss: 1.057137] [G loss: 1.212176]\n",
            "5769 [D loss: 1.070514] [G loss: 1.206541]\n",
            "5770 [D loss: 1.084672] [G loss: 1.210202]\n",
            "5771 [D loss: 1.084689] [G loss: 1.224798]\n",
            "5772 [D loss: 1.065426] [G loss: 1.215342]\n",
            "5773 [D loss: 1.072125] [G loss: 1.217043]\n",
            "5774 [D loss: 1.093660] [G loss: 1.198927]\n",
            "5775 [D loss: 1.074889] [G loss: 1.206328]\n",
            "5776 [D loss: 1.102450] [G loss: 1.199659]\n",
            "5777 [D loss: 1.075967] [G loss: 1.221179]\n",
            "5778 [D loss: 1.067574] [G loss: 1.208623]\n",
            "5779 [D loss: 1.065649] [G loss: 1.219225]\n",
            "5780 [D loss: 1.056780] [G loss: 1.221921]\n",
            "5781 [D loss: 1.072610] [G loss: 1.203283]\n",
            "5782 [D loss: 1.082812] [G loss: 1.195462]\n",
            "5783 [D loss: 1.089095] [G loss: 1.192959]\n",
            "5784 [D loss: 1.054503] [G loss: 1.218369]\n",
            "5785 [D loss: 1.087495] [G loss: 1.208205]\n",
            "5786 [D loss: 1.060001] [G loss: 1.207470]\n",
            "5787 [D loss: 1.088067] [G loss: 1.201393]\n",
            "5788 [D loss: 1.076222] [G loss: 1.214635]\n",
            "5789 [D loss: 1.086844] [G loss: 1.204404]\n",
            "5790 [D loss: 1.074594] [G loss: 1.211046]\n",
            "5791 [D loss: 1.073922] [G loss: 1.213256]\n",
            "5792 [D loss: 1.051368] [G loss: 1.208210]\n",
            "5793 [D loss: 1.080101] [G loss: 1.208058]\n",
            "5794 [D loss: 1.066318] [G loss: 1.207472]\n",
            "5795 [D loss: 1.063541] [G loss: 1.220902]\n",
            "5796 [D loss: 1.058589] [G loss: 1.196904]\n",
            "5797 [D loss: 1.066562] [G loss: 1.223867]\n",
            "5798 [D loss: 1.066329] [G loss: 1.211209]\n",
            "5799 [D loss: 1.074198] [G loss: 1.211094]\n",
            "5800 [D loss: 1.090549] [G loss: 1.196066]\n",
            "[[0.51729053 0.4867136  0.5046129  0.5033492 ]\n",
            " [0.55285615 0.4459417  0.55691737 0.49816155]\n",
            " [0.5775226  0.44000804 0.4988283  0.47272974]\n",
            " [0.50352895 0.49740702 0.50199467 0.500737  ]\n",
            " [0.5364353  0.43462515 0.5346307  0.5617001 ]]\n",
            "5801 [D loss: 1.079372] [G loss: 1.188087]\n",
            "5802 [D loss: 1.077306] [G loss: 1.219061]\n",
            "5803 [D loss: 1.089866] [G loss: 1.201851]\n",
            "5804 [D loss: 1.069932] [G loss: 1.220673]\n",
            "5805 [D loss: 1.087520] [G loss: 1.214622]\n",
            "5806 [D loss: 1.063425] [G loss: 1.218880]\n",
            "5807 [D loss: 1.090327] [G loss: 1.204779]\n",
            "5808 [D loss: 1.071497] [G loss: 1.207952]\n",
            "5809 [D loss: 1.075202] [G loss: 1.220285]\n",
            "5810 [D loss: 1.085998] [G loss: 1.198990]\n",
            "5811 [D loss: 1.060423] [G loss: 1.212128]\n",
            "5812 [D loss: 1.082147] [G loss: 1.216426]\n",
            "5813 [D loss: 1.083638] [G loss: 1.208578]\n",
            "5814 [D loss: 1.062847] [G loss: 1.219510]\n",
            "5815 [D loss: 1.070046] [G loss: 1.196151]\n",
            "5816 [D loss: 1.066153] [G loss: 1.218833]\n",
            "5817 [D loss: 1.096742] [G loss: 1.205916]\n",
            "5818 [D loss: 1.072604] [G loss: 1.208605]\n",
            "5819 [D loss: 1.085570] [G loss: 1.212723]\n",
            "5820 [D loss: 1.074492] [G loss: 1.197782]\n",
            "5821 [D loss: 1.077002] [G loss: 1.215667]\n",
            "5822 [D loss: 1.081331] [G loss: 1.210142]\n",
            "5823 [D loss: 1.077626] [G loss: 1.211588]\n",
            "5824 [D loss: 1.081937] [G loss: 1.206474]\n",
            "5825 [D loss: 1.057544] [G loss: 1.210149]\n",
            "5826 [D loss: 1.071204] [G loss: 1.211833]\n",
            "5827 [D loss: 1.064039] [G loss: 1.192853]\n",
            "5828 [D loss: 1.064624] [G loss: 1.208274]\n",
            "5829 [D loss: 1.081457] [G loss: 1.214682]\n",
            "5830 [D loss: 1.072557] [G loss: 1.210951]\n",
            "5831 [D loss: 1.079275] [G loss: 1.210763]\n",
            "5832 [D loss: 1.079916] [G loss: 1.216399]\n",
            "5833 [D loss: 1.078058] [G loss: 1.218244]\n",
            "5834 [D loss: 1.083970] [G loss: 1.212042]\n",
            "5835 [D loss: 1.085988] [G loss: 1.209166]\n",
            "5836 [D loss: 1.068562] [G loss: 1.204297]\n",
            "5837 [D loss: 1.071607] [G loss: 1.213177]\n",
            "5838 [D loss: 1.071472] [G loss: 1.208143]\n",
            "5839 [D loss: 1.066814] [G loss: 1.212769]\n",
            "5840 [D loss: 1.064397] [G loss: 1.220028]\n",
            "5841 [D loss: 1.064613] [G loss: 1.201140]\n",
            "5842 [D loss: 1.077292] [G loss: 1.197345]\n",
            "5843 [D loss: 1.084393] [G loss: 1.198458]\n",
            "5844 [D loss: 1.076049] [G loss: 1.210150]\n",
            "5845 [D loss: 1.093659] [G loss: 1.199704]\n",
            "5846 [D loss: 1.077482] [G loss: 1.204167]\n",
            "5847 [D loss: 1.060516] [G loss: 1.203063]\n",
            "5848 [D loss: 1.075373] [G loss: 1.211001]\n",
            "5849 [D loss: 1.077229] [G loss: 1.211784]\n",
            "5850 [D loss: 1.067528] [G loss: 1.194834]\n",
            "[[0.5390995  0.46917087 0.5056469  0.5109705 ]\n",
            " [0.5173348  0.4777853  0.50899804 0.49802148]\n",
            " [0.6780476  0.26932514 0.55892193 0.53385115]\n",
            " [0.8852202  0.1934259  0.7206231  0.26253402]\n",
            " [0.518653   0.41959047 0.58591294 0.5364413 ]]\n",
            "5851 [D loss: 1.071248] [G loss: 1.214242]\n",
            "5852 [D loss: 1.075532] [G loss: 1.214728]\n",
            "5853 [D loss: 1.076774] [G loss: 1.205425]\n",
            "5854 [D loss: 1.072807] [G loss: 1.208412]\n",
            "5855 [D loss: 1.074594] [G loss: 1.202302]\n",
            "5856 [D loss: 1.066945] [G loss: 1.207478]\n",
            "5857 [D loss: 1.074434] [G loss: 1.203710]\n",
            "5858 [D loss: 1.069521] [G loss: 1.203419]\n",
            "5859 [D loss: 1.085582] [G loss: 1.206950]\n",
            "5860 [D loss: 1.059258] [G loss: 1.201164]\n",
            "5861 [D loss: 1.081356] [G loss: 1.197402]\n",
            "5862 [D loss: 1.079625] [G loss: 1.216491]\n",
            "5863 [D loss: 1.075792] [G loss: 1.194224]\n",
            "5864 [D loss: 1.056748] [G loss: 1.218362]\n",
            "5865 [D loss: 1.091773] [G loss: 1.197160]\n",
            "5866 [D loss: 1.080450] [G loss: 1.200372]\n",
            "5867 [D loss: 1.056540] [G loss: 1.221081]\n",
            "5868 [D loss: 1.080511] [G loss: 1.208010]\n",
            "5869 [D loss: 1.090512] [G loss: 1.206137]\n",
            "5870 [D loss: 1.084589] [G loss: 1.201762]\n",
            "5871 [D loss: 1.081462] [G loss: 1.207126]\n",
            "5872 [D loss: 1.071964] [G loss: 1.214002]\n",
            "5873 [D loss: 1.074152] [G loss: 1.211424]\n",
            "5874 [D loss: 1.065940] [G loss: 1.209828]\n",
            "5875 [D loss: 1.061025] [G loss: 1.209953]\n",
            "5876 [D loss: 1.087768] [G loss: 1.212659]\n",
            "5877 [D loss: 1.088595] [G loss: 1.197833]\n",
            "5878 [D loss: 1.054502] [G loss: 1.231119]\n",
            "5879 [D loss: 1.081287] [G loss: 1.207823]\n",
            "5880 [D loss: 1.080414] [G loss: 1.201579]\n",
            "5881 [D loss: 1.086825] [G loss: 1.200534]\n",
            "5882 [D loss: 1.078535] [G loss: 1.204594]\n",
            "5883 [D loss: 1.078307] [G loss: 1.204492]\n",
            "5884 [D loss: 1.101633] [G loss: 1.188188]\n",
            "5885 [D loss: 1.076849] [G loss: 1.213767]\n",
            "5886 [D loss: 1.062923] [G loss: 1.210667]\n",
            "5887 [D loss: 1.071517] [G loss: 1.198019]\n",
            "5888 [D loss: 1.077948] [G loss: 1.207161]\n",
            "5889 [D loss: 1.080109] [G loss: 1.212235]\n",
            "5890 [D loss: 1.088022] [G loss: 1.205888]\n",
            "5891 [D loss: 1.084291] [G loss: 1.186882]\n",
            "5892 [D loss: 1.080416] [G loss: 1.200406]\n",
            "5893 [D loss: 1.083362] [G loss: 1.219534]\n",
            "5894 [D loss: 1.076573] [G loss: 1.188025]\n",
            "5895 [D loss: 1.074882] [G loss: 1.208978]\n",
            "5896 [D loss: 1.062981] [G loss: 1.205448]\n",
            "5897 [D loss: 1.077481] [G loss: 1.202870]\n",
            "5898 [D loss: 1.071942] [G loss: 1.199756]\n",
            "5899 [D loss: 1.088829] [G loss: 1.197894]\n",
            "5900 [D loss: 1.073538] [G loss: 1.194812]\n",
            "[[0.60201985 0.30371904 0.48168766 0.5850742 ]\n",
            " [0.5043621  0.49582317 0.5026094  0.501073  ]\n",
            " [0.5551153  0.40949336 0.5039096  0.5476247 ]\n",
            " [0.69498235 0.29370087 0.5520555  0.49718037]\n",
            " [0.5036149  0.4973568  0.5020165  0.50069946]]\n",
            "5901 [D loss: 1.073573] [G loss: 1.190337]\n",
            "5902 [D loss: 1.063966] [G loss: 1.209949]\n",
            "5903 [D loss: 1.070975] [G loss: 1.195337]\n",
            "5904 [D loss: 1.074212] [G loss: 1.209774]\n",
            "5905 [D loss: 1.075229] [G loss: 1.214971]\n",
            "5906 [D loss: 1.065501] [G loss: 1.199840]\n",
            "5907 [D loss: 1.083275] [G loss: 1.209153]\n",
            "5908 [D loss: 1.070597] [G loss: 1.216445]\n",
            "5909 [D loss: 1.083660] [G loss: 1.205798]\n",
            "5910 [D loss: 1.090399] [G loss: 1.188919]\n",
            "5911 [D loss: 1.077781] [G loss: 1.207100]\n",
            "5912 [D loss: 1.084046] [G loss: 1.211823]\n",
            "5913 [D loss: 1.076567] [G loss: 1.206625]\n",
            "5914 [D loss: 1.076650] [G loss: 1.215103]\n",
            "5915 [D loss: 1.061580] [G loss: 1.204414]\n",
            "5916 [D loss: 1.071977] [G loss: 1.209921]\n",
            "5917 [D loss: 1.072722] [G loss: 1.219518]\n",
            "5918 [D loss: 1.066858] [G loss: 1.205266]\n",
            "5919 [D loss: 1.076253] [G loss: 1.206069]\n",
            "5920 [D loss: 1.070674] [G loss: 1.210143]\n",
            "5921 [D loss: 1.068454] [G loss: 1.213971]\n",
            "5922 [D loss: 1.062562] [G loss: 1.216095]\n",
            "5923 [D loss: 1.089041] [G loss: 1.195609]\n",
            "5924 [D loss: 1.083530] [G loss: 1.210265]\n",
            "5925 [D loss: 1.081376] [G loss: 1.200896]\n",
            "5926 [D loss: 1.083152] [G loss: 1.202458]\n",
            "5927 [D loss: 1.077749] [G loss: 1.205115]\n",
            "5928 [D loss: 1.083206] [G loss: 1.205604]\n",
            "5929 [D loss: 1.092808] [G loss: 1.185433]\n",
            "5930 [D loss: 1.081715] [G loss: 1.201452]\n",
            "5931 [D loss: 1.094993] [G loss: 1.198110]\n",
            "5932 [D loss: 1.076928] [G loss: 1.186047]\n",
            "5933 [D loss: 1.069234] [G loss: 1.208775]\n",
            "5934 [D loss: 1.056329] [G loss: 1.220509]\n",
            "5935 [D loss: 1.065708] [G loss: 1.207356]\n",
            "5936 [D loss: 1.082167] [G loss: 1.197120]\n",
            "5937 [D loss: 1.075489] [G loss: 1.195487]\n",
            "5938 [D loss: 1.069682] [G loss: 1.221173]\n",
            "5939 [D loss: 1.072449] [G loss: 1.206015]\n",
            "5940 [D loss: 1.068568] [G loss: 1.210592]\n",
            "5941 [D loss: 1.089904] [G loss: 1.206110]\n",
            "5942 [D loss: 1.076468] [G loss: 1.195943]\n",
            "5943 [D loss: 1.068789] [G loss: 1.204755]\n",
            "5944 [D loss: 1.083733] [G loss: 1.194036]\n",
            "5945 [D loss: 1.054819] [G loss: 1.222540]\n",
            "5946 [D loss: 1.088230] [G loss: 1.184460]\n",
            "5947 [D loss: 1.076311] [G loss: 1.210313]\n",
            "5948 [D loss: 1.090774] [G loss: 1.191523]\n",
            "5949 [D loss: 1.082198] [G loss: 1.205794]\n",
            "5950 [D loss: 1.078525] [G loss: 1.192515]\n",
            "[[0.62452054 0.3597523  0.5176367  0.5260798 ]\n",
            " [0.5322129  0.36704823 0.5516026  0.5991345 ]\n",
            " [0.75487715 0.34491944 0.6314288  0.333136  ]\n",
            " [0.69779295 0.50153637 0.62881553 0.3344848 ]\n",
            " [0.50743204 0.49871963 0.50537795 0.49893564]]\n",
            "5951 [D loss: 1.068667] [G loss: 1.220036]\n",
            "5952 [D loss: 1.074523] [G loss: 1.207155]\n",
            "5953 [D loss: 1.060687] [G loss: 1.228497]\n",
            "5954 [D loss: 1.071296] [G loss: 1.213053]\n",
            "5955 [D loss: 1.090934] [G loss: 1.198158]\n",
            "5956 [D loss: 1.065373] [G loss: 1.209541]\n",
            "5957 [D loss: 1.083827] [G loss: 1.188075]\n",
            "5958 [D loss: 1.079732] [G loss: 1.205870]\n",
            "5959 [D loss: 1.065672] [G loss: 1.207501]\n",
            "5960 [D loss: 1.090312] [G loss: 1.195144]\n",
            "5961 [D loss: 1.079732] [G loss: 1.187882]\n",
            "5962 [D loss: 1.086619] [G loss: 1.190723]\n",
            "5963 [D loss: 1.086141] [G loss: 1.199026]\n",
            "5964 [D loss: 1.073811] [G loss: 1.196092]\n",
            "5965 [D loss: 1.104636] [G loss: 1.195822]\n",
            "5966 [D loss: 1.080996] [G loss: 1.202859]\n",
            "5967 [D loss: 1.084315] [G loss: 1.199267]\n",
            "5968 [D loss: 1.068123] [G loss: 1.199778]\n",
            "5969 [D loss: 1.085287] [G loss: 1.186952]\n",
            "5970 [D loss: 1.086708] [G loss: 1.201309]\n",
            "5971 [D loss: 1.080312] [G loss: 1.216712]\n",
            "5972 [D loss: 1.069180] [G loss: 1.189332]\n",
            "5973 [D loss: 1.087041] [G loss: 1.208509]\n",
            "5974 [D loss: 1.082621] [G loss: 1.200810]\n",
            "5975 [D loss: 1.065449] [G loss: 1.220705]\n",
            "5976 [D loss: 1.078084] [G loss: 1.203356]\n",
            "5977 [D loss: 1.079253] [G loss: 1.213394]\n",
            "5978 [D loss: 1.067980] [G loss: 1.213227]\n",
            "5979 [D loss: 1.081618] [G loss: 1.201682]\n",
            "5980 [D loss: 1.090108] [G loss: 1.208308]\n",
            "5981 [D loss: 1.068608] [G loss: 1.219894]\n",
            "5982 [D loss: 1.080850] [G loss: 1.209120]\n",
            "5983 [D loss: 1.083477] [G loss: 1.201176]\n",
            "5984 [D loss: 1.083576] [G loss: 1.210529]\n",
            "5985 [D loss: 1.072689] [G loss: 1.205115]\n",
            "5986 [D loss: 1.067082] [G loss: 1.207007]\n",
            "5987 [D loss: 1.083512] [G loss: 1.204221]\n",
            "5988 [D loss: 1.076902] [G loss: 1.196870]\n",
            "5989 [D loss: 1.077398] [G loss: 1.202256]\n",
            "5990 [D loss: 1.080929] [G loss: 1.191159]\n",
            "5991 [D loss: 1.081848] [G loss: 1.196679]\n",
            "5992 [D loss: 1.084904] [G loss: 1.189402]\n",
            "5993 [D loss: 1.090387] [G loss: 1.193353]\n",
            "5994 [D loss: 1.099474] [G loss: 1.187676]\n",
            "5995 [D loss: 1.079810] [G loss: 1.211310]\n",
            "5996 [D loss: 1.075869] [G loss: 1.201872]\n",
            "5997 [D loss: 1.061095] [G loss: 1.211905]\n",
            "5998 [D loss: 1.055807] [G loss: 1.205289]\n",
            "5999 [D loss: 1.067915] [G loss: 1.207807]\n",
            "6000 [D loss: 1.055712] [G loss: 1.206020]\n",
            "[[0.5037008  0.49730685 0.50203854 0.50066185]\n",
            " [0.6360446  0.2943482  0.5005271  0.55578744]\n",
            " [0.6754363  0.4112761  0.5885655  0.40265235]\n",
            " [0.548594   0.4478058  0.5124746  0.5168217 ]\n",
            " [0.900417   0.20921893 0.7778595  0.2056364 ]]\n",
            "6001 [D loss: 1.095489] [G loss: 1.193260]\n",
            "6002 [D loss: 1.073885] [G loss: 1.204059]\n",
            "6003 [D loss: 1.074496] [G loss: 1.208124]\n",
            "6004 [D loss: 1.088292] [G loss: 1.196058]\n",
            "6005 [D loss: 1.083457] [G loss: 1.188422]\n",
            "6006 [D loss: 1.079159] [G loss: 1.179077]\n",
            "6007 [D loss: 1.084063] [G loss: 1.195966]\n",
            "6008 [D loss: 1.095767] [G loss: 1.192734]\n",
            "6009 [D loss: 1.081002] [G loss: 1.224662]\n",
            "6010 [D loss: 1.083901] [G loss: 1.206499]\n",
            "6011 [D loss: 1.089208] [G loss: 1.200333]\n",
            "6012 [D loss: 1.077947] [G loss: 1.193259]\n",
            "6013 [D loss: 1.086006] [G loss: 1.196376]\n",
            "6014 [D loss: 1.079266] [G loss: 1.215801]\n",
            "6015 [D loss: 1.083177] [G loss: 1.189161]\n",
            "6016 [D loss: 1.080920] [G loss: 1.192543]\n",
            "6017 [D loss: 1.086413] [G loss: 1.201641]\n",
            "6018 [D loss: 1.063535] [G loss: 1.209482]\n",
            "6019 [D loss: 1.097507] [G loss: 1.194996]\n",
            "6020 [D loss: 1.085527] [G loss: 1.204841]\n",
            "6021 [D loss: 1.069361] [G loss: 1.212817]\n",
            "6022 [D loss: 1.087650] [G loss: 1.194434]\n",
            "6023 [D loss: 1.080743] [G loss: 1.199481]\n",
            "6024 [D loss: 1.062298] [G loss: 1.205479]\n",
            "6025 [D loss: 1.075466] [G loss: 1.213432]\n",
            "6026 [D loss: 1.074514] [G loss: 1.221184]\n",
            "6027 [D loss: 1.081863] [G loss: 1.201169]\n",
            "6028 [D loss: 1.077770] [G loss: 1.206450]\n",
            "6029 [D loss: 1.078701] [G loss: 1.207331]\n",
            "6030 [D loss: 1.074295] [G loss: 1.192444]\n",
            "6031 [D loss: 1.091562] [G loss: 1.192130]\n",
            "6032 [D loss: 1.079173] [G loss: 1.210321]\n",
            "6033 [D loss: 1.074718] [G loss: 1.206695]\n",
            "6034 [D loss: 1.088642] [G loss: 1.210770]\n",
            "6035 [D loss: 1.072319] [G loss: 1.201831]\n",
            "6036 [D loss: 1.085280] [G loss: 1.193835]\n",
            "6037 [D loss: 1.070889] [G loss: 1.199210]\n",
            "6038 [D loss: 1.084850] [G loss: 1.197096]\n",
            "6039 [D loss: 1.069681] [G loss: 1.213032]\n",
            "6040 [D loss: 1.101849] [G loss: 1.197650]\n",
            "6041 [D loss: 1.076906] [G loss: 1.190134]\n",
            "6042 [D loss: 1.091460] [G loss: 1.186904]\n",
            "6043 [D loss: 1.054209] [G loss: 1.209869]\n",
            "6044 [D loss: 1.088950] [G loss: 1.194106]\n",
            "6045 [D loss: 1.084029] [G loss: 1.193208]\n",
            "6046 [D loss: 1.092391] [G loss: 1.205920]\n",
            "6047 [D loss: 1.079120] [G loss: 1.207784]\n",
            "6048 [D loss: 1.061070] [G loss: 1.209170]\n",
            "6049 [D loss: 1.098479] [G loss: 1.175206]\n",
            "6050 [D loss: 1.080562] [G loss: 1.217050]\n",
            "[[0.5515298  0.40847605 0.5467727  0.5545324 ]\n",
            " [0.81356347 0.3070702  0.6609702  0.30659515]\n",
            " [0.61041725 0.35025117 0.45931703 0.5308221 ]\n",
            " [0.50374395 0.49728188 0.50204945 0.50064284]\n",
            " [0.7968143  0.24768303 0.7076394  0.23131941]]\n",
            "6051 [D loss: 1.076036] [G loss: 1.211195]\n",
            "6052 [D loss: 1.056922] [G loss: 1.208835]\n",
            "6053 [D loss: 1.070736] [G loss: 1.211762]\n",
            "6054 [D loss: 1.066587] [G loss: 1.209532]\n",
            "6055 [D loss: 1.086253] [G loss: 1.217765]\n",
            "6056 [D loss: 1.080561] [G loss: 1.195182]\n",
            "6057 [D loss: 1.071937] [G loss: 1.210386]\n",
            "6058 [D loss: 1.079911] [G loss: 1.200154]\n",
            "6059 [D loss: 1.079026] [G loss: 1.200715]\n",
            "6060 [D loss: 1.084569] [G loss: 1.198512]\n",
            "6061 [D loss: 1.073017] [G loss: 1.202558]\n",
            "6062 [D loss: 1.079725] [G loss: 1.194826]\n",
            "6063 [D loss: 1.088535] [G loss: 1.183159]\n",
            "6064 [D loss: 1.075185] [G loss: 1.212454]\n",
            "6065 [D loss: 1.087930] [G loss: 1.208276]\n",
            "6066 [D loss: 1.080456] [G loss: 1.195667]\n",
            "6067 [D loss: 1.072536] [G loss: 1.192672]\n",
            "6068 [D loss: 1.084731] [G loss: 1.194304]\n",
            "6069 [D loss: 1.088805] [G loss: 1.200578]\n",
            "6070 [D loss: 1.070794] [G loss: 1.210111]\n",
            "6071 [D loss: 1.079739] [G loss: 1.199451]\n",
            "6072 [D loss: 1.080545] [G loss: 1.192719]\n",
            "6073 [D loss: 1.081346] [G loss: 1.205924]\n",
            "6074 [D loss: 1.096261] [G loss: 1.189407]\n",
            "6075 [D loss: 1.090232] [G loss: 1.186130]\n",
            "6076 [D loss: 1.087715] [G loss: 1.192538]\n",
            "6077 [D loss: 1.087489] [G loss: 1.186460]\n",
            "6078 [D loss: 1.067893] [G loss: 1.211375]\n",
            "6079 [D loss: 1.084041] [G loss: 1.187578]\n",
            "6080 [D loss: 1.091220] [G loss: 1.187983]\n",
            "6081 [D loss: 1.067543] [G loss: 1.213094]\n",
            "6082 [D loss: 1.071524] [G loss: 1.202573]\n",
            "6083 [D loss: 1.069951] [G loss: 1.193389]\n",
            "6084 [D loss: 1.083554] [G loss: 1.210579]\n",
            "6085 [D loss: 1.068675] [G loss: 1.213621]\n",
            "6086 [D loss: 1.073572] [G loss: 1.213774]\n",
            "6087 [D loss: 1.069180] [G loss: 1.188289]\n",
            "6088 [D loss: 1.087364] [G loss: 1.192564]\n",
            "6089 [D loss: 1.065512] [G loss: 1.213102]\n",
            "6090 [D loss: 1.096542] [G loss: 1.192123]\n",
            "6091 [D loss: 1.075720] [G loss: 1.189135]\n",
            "6092 [D loss: 1.072704] [G loss: 1.196958]\n",
            "6093 [D loss: 1.057637] [G loss: 1.208332]\n",
            "6094 [D loss: 1.082999] [G loss: 1.195770]\n",
            "6095 [D loss: 1.078194] [G loss: 1.208139]\n",
            "6096 [D loss: 1.073669] [G loss: 1.218370]\n",
            "6097 [D loss: 1.089389] [G loss: 1.205200]\n",
            "6098 [D loss: 1.086808] [G loss: 1.187051]\n",
            "6099 [D loss: 1.072234] [G loss: 1.193421]\n",
            "6100 [D loss: 1.086165] [G loss: 1.206456]\n",
            "[[0.59193236 0.29670626 0.57433873 0.6244412 ]\n",
            " [0.73601544 0.37018105 0.6618797  0.30032128]\n",
            " [0.7093355  0.3916904  0.6438611  0.305451  ]\n",
            " [0.5745047  0.43209141 0.52063984 0.48711085]\n",
            " [0.6170233  0.3435509  0.5723109  0.47268045]]\n",
            "6101 [D loss: 1.093061] [G loss: 1.191044]\n",
            "6102 [D loss: 1.081669] [G loss: 1.192814]\n",
            "6103 [D loss: 1.102685] [G loss: 1.186067]\n",
            "6104 [D loss: 1.097015] [G loss: 1.192621]\n",
            "6105 [D loss: 1.080109] [G loss: 1.212735]\n",
            "6106 [D loss: 1.085543] [G loss: 1.189311]\n",
            "6107 [D loss: 1.107036] [G loss: 1.188302]\n",
            "6108 [D loss: 1.087587] [G loss: 1.187452]\n",
            "6109 [D loss: 1.078122] [G loss: 1.192655]\n",
            "6110 [D loss: 1.081805] [G loss: 1.191820]\n",
            "6111 [D loss: 1.087552] [G loss: 1.188047]\n",
            "6112 [D loss: 1.093103] [G loss: 1.182262]\n",
            "6113 [D loss: 1.078167] [G loss: 1.208477]\n",
            "6114 [D loss: 1.078128] [G loss: 1.181681]\n",
            "6115 [D loss: 1.079249] [G loss: 1.196063]\n",
            "6116 [D loss: 1.064347] [G loss: 1.204794]\n",
            "6117 [D loss: 1.075609] [G loss: 1.208371]\n",
            "6118 [D loss: 1.081094] [G loss: 1.192604]\n",
            "6119 [D loss: 1.084414] [G loss: 1.194651]\n",
            "6120 [D loss: 1.081075] [G loss: 1.195524]\n",
            "6121 [D loss: 1.090631] [G loss: 1.191142]\n",
            "6122 [D loss: 1.090801] [G loss: 1.197719]\n",
            "6123 [D loss: 1.081447] [G loss: 1.194966]\n",
            "6124 [D loss: 1.081481] [G loss: 1.188875]\n",
            "6125 [D loss: 1.076463] [G loss: 1.220600]\n",
            "6126 [D loss: 1.085189] [G loss: 1.189573]\n",
            "6127 [D loss: 1.076731] [G loss: 1.202127]\n",
            "6128 [D loss: 1.071575] [G loss: 1.199463]\n",
            "6129 [D loss: 1.083961] [G loss: 1.196749]\n",
            "6130 [D loss: 1.075448] [G loss: 1.194289]\n",
            "6131 [D loss: 1.081788] [G loss: 1.197719]\n",
            "6132 [D loss: 1.091485] [G loss: 1.195721]\n",
            "6133 [D loss: 1.078468] [G loss: 1.191839]\n",
            "6134 [D loss: 1.080905] [G loss: 1.198625]\n",
            "6135 [D loss: 1.074162] [G loss: 1.201771]\n",
            "6136 [D loss: 1.084114] [G loss: 1.197176]\n",
            "6137 [D loss: 1.062591] [G loss: 1.212343]\n",
            "6138 [D loss: 1.084461] [G loss: 1.207341]\n",
            "6139 [D loss: 1.088265] [G loss: 1.196611]\n",
            "6140 [D loss: 1.081171] [G loss: 1.194890]\n",
            "6141 [D loss: 1.080362] [G loss: 1.199543]\n",
            "6142 [D loss: 1.083492] [G loss: 1.187942]\n",
            "6143 [D loss: 1.078746] [G loss: 1.206791]\n",
            "6144 [D loss: 1.068487] [G loss: 1.213113]\n",
            "6145 [D loss: 1.086807] [G loss: 1.199846]\n",
            "6146 [D loss: 1.059345] [G loss: 1.224057]\n",
            "6147 [D loss: 1.088555] [G loss: 1.181721]\n",
            "6148 [D loss: 1.097237] [G loss: 1.194310]\n",
            "6149 [D loss: 1.082406] [G loss: 1.213940]\n",
            "6150 [D loss: 1.084325] [G loss: 1.199321]\n",
            "[[0.5322251  0.4658028  0.514606   0.4922572 ]\n",
            " [0.5038294  0.49723175 0.50207216 0.50060546]\n",
            " [0.50710315 0.4730302  0.50771934 0.5107009 ]\n",
            " [0.5038294  0.49723175 0.50207216 0.50060546]\n",
            " [0.56329155 0.33690733 0.5386712  0.6077745 ]]\n",
            "6151 [D loss: 1.070305] [G loss: 1.196144]\n",
            "6152 [D loss: 1.058492] [G loss: 1.204064]\n",
            "6153 [D loss: 1.080276] [G loss: 1.206324]\n",
            "6154 [D loss: 1.084950] [G loss: 1.195970]\n",
            "6155 [D loss: 1.064771] [G loss: 1.211235]\n",
            "6156 [D loss: 1.073662] [G loss: 1.212154]\n",
            "6157 [D loss: 1.089195] [G loss: 1.191190]\n",
            "6158 [D loss: 1.066085] [G loss: 1.191706]\n",
            "6159 [D loss: 1.083726] [G loss: 1.209084]\n",
            "6160 [D loss: 1.090610] [G loss: 1.210905]\n",
            "6161 [D loss: 1.094849] [G loss: 1.198388]\n",
            "6162 [D loss: 1.071812] [G loss: 1.199463]\n",
            "6163 [D loss: 1.076877] [G loss: 1.182877]\n",
            "6164 [D loss: 1.087837] [G loss: 1.202988]\n",
            "6165 [D loss: 1.076723] [G loss: 1.201187]\n",
            "6166 [D loss: 1.090755] [G loss: 1.191145]\n",
            "6167 [D loss: 1.069338] [G loss: 1.206319]\n",
            "6168 [D loss: 1.071269] [G loss: 1.184969]\n",
            "6169 [D loss: 1.089235] [G loss: 1.196493]\n",
            "6170 [D loss: 1.075068] [G loss: 1.198558]\n",
            "6171 [D loss: 1.078096] [G loss: 1.199798]\n",
            "6172 [D loss: 1.072296] [G loss: 1.181267]\n",
            "6173 [D loss: 1.084339] [G loss: 1.198434]\n",
            "6174 [D loss: 1.063613] [G loss: 1.195177]\n",
            "6175 [D loss: 1.087794] [G loss: 1.201645]\n",
            "6176 [D loss: 1.038247] [G loss: 1.229720]\n",
            "6177 [D loss: 1.071702] [G loss: 1.204054]\n",
            "6178 [D loss: 1.075876] [G loss: 1.198426]\n",
            "6179 [D loss: 1.066597] [G loss: 1.219410]\n",
            "6180 [D loss: 1.086353] [G loss: 1.189294]\n",
            "6181 [D loss: 1.081569] [G loss: 1.184002]\n",
            "6182 [D loss: 1.084871] [G loss: 1.206271]\n",
            "6183 [D loss: 1.083716] [G loss: 1.197532]\n",
            "6184 [D loss: 1.085449] [G loss: 1.206979]\n",
            "6185 [D loss: 1.082736] [G loss: 1.197171]\n",
            "6186 [D loss: 1.088977] [G loss: 1.186025]\n",
            "6187 [D loss: 1.088455] [G loss: 1.194675]\n",
            "6188 [D loss: 1.067326] [G loss: 1.233446]\n",
            "6189 [D loss: 1.068091] [G loss: 1.205011]\n",
            "6190 [D loss: 1.067777] [G loss: 1.209418]\n",
            "6191 [D loss: 1.082356] [G loss: 1.204725]\n",
            "6192 [D loss: 1.070903] [G loss: 1.192933]\n",
            "6193 [D loss: 1.084789] [G loss: 1.197134]\n",
            "6194 [D loss: 1.084099] [G loss: 1.200778]\n",
            "6195 [D loss: 1.093839] [G loss: 1.182292]\n",
            "6196 [D loss: 1.085085] [G loss: 1.183000]\n",
            "6197 [D loss: 1.085143] [G loss: 1.203413]\n",
            "6198 [D loss: 1.084139] [G loss: 1.198022]\n",
            "6199 [D loss: 1.064753] [G loss: 1.207209]\n",
            "6200 [D loss: 1.094725] [G loss: 1.197191]\n",
            "[[0.5038699  0.49720475 0.5020855  0.5005909 ]\n",
            " [0.5038699  0.49720475 0.5020855  0.5005909 ]\n",
            " [0.8247281  0.23483619 0.6404411  0.43374777]\n",
            " [0.77690166 0.29366794 0.662191   0.3247094 ]\n",
            " [0.86008316 0.21939169 0.68731356 0.30510205]]\n",
            "6201 [D loss: 1.088965] [G loss: 1.183669]\n",
            "6202 [D loss: 1.069859] [G loss: 1.207399]\n",
            "6203 [D loss: 1.082492] [G loss: 1.188565]\n",
            "6204 [D loss: 1.063276] [G loss: 1.206405]\n",
            "6205 [D loss: 1.083177] [G loss: 1.196704]\n",
            "6206 [D loss: 1.093978] [G loss: 1.191797]\n",
            "6207 [D loss: 1.081429] [G loss: 1.203695]\n",
            "6208 [D loss: 1.081800] [G loss: 1.205977]\n",
            "6209 [D loss: 1.083010] [G loss: 1.187729]\n",
            "6210 [D loss: 1.090034] [G loss: 1.199215]\n",
            "6211 [D loss: 1.066947] [G loss: 1.211700]\n",
            "6212 [D loss: 1.071440] [G loss: 1.210258]\n",
            "6213 [D loss: 1.076033] [G loss: 1.192183]\n",
            "6214 [D loss: 1.098902] [G loss: 1.190589]\n",
            "6215 [D loss: 1.059532] [G loss: 1.213061]\n",
            "6216 [D loss: 1.082318] [G loss: 1.194701]\n",
            "6217 [D loss: 1.071075] [G loss: 1.202971]\n",
            "6218 [D loss: 1.091952] [G loss: 1.192431]\n",
            "6219 [D loss: 1.088997] [G loss: 1.203619]\n",
            "6220 [D loss: 1.079474] [G loss: 1.200296]\n",
            "6221 [D loss: 1.088637] [G loss: 1.207139]\n",
            "6222 [D loss: 1.078582] [G loss: 1.195104]\n",
            "6223 [D loss: 1.097616] [G loss: 1.178423]\n",
            "6224 [D loss: 1.100441] [G loss: 1.190128]\n",
            "6225 [D loss: 1.074365] [G loss: 1.200999]\n",
            "6226 [D loss: 1.084179] [G loss: 1.192797]\n",
            "6227 [D loss: 1.090866] [G loss: 1.187803]\n",
            "6228 [D loss: 1.085031] [G loss: 1.200414]\n",
            "6229 [D loss: 1.102236] [G loss: 1.181006]\n",
            "6230 [D loss: 1.092597] [G loss: 1.192869]\n",
            "6231 [D loss: 1.077228] [G loss: 1.198748]\n",
            "6232 [D loss: 1.075370] [G loss: 1.184967]\n",
            "6233 [D loss: 1.089900] [G loss: 1.193558]\n",
            "6234 [D loss: 1.088460] [G loss: 1.187918]\n",
            "6235 [D loss: 1.074968] [G loss: 1.194174]\n",
            "6236 [D loss: 1.080131] [G loss: 1.198122]\n",
            "6237 [D loss: 1.083484] [G loss: 1.194822]\n",
            "6238 [D loss: 1.089031] [G loss: 1.187980]\n",
            "6239 [D loss: 1.085299] [G loss: 1.191125]\n",
            "6240 [D loss: 1.073180] [G loss: 1.208465]\n",
            "6241 [D loss: 1.057472] [G loss: 1.207698]\n",
            "6242 [D loss: 1.073209] [G loss: 1.190772]\n",
            "6243 [D loss: 1.081989] [G loss: 1.199573]\n",
            "6244 [D loss: 1.079334] [G loss: 1.191267]\n",
            "6245 [D loss: 1.081473] [G loss: 1.192041]\n",
            "6246 [D loss: 1.084295] [G loss: 1.184811]\n",
            "6247 [D loss: 1.090932] [G loss: 1.193252]\n",
            "6248 [D loss: 1.092033] [G loss: 1.186060]\n",
            "6249 [D loss: 1.085749] [G loss: 1.210407]\n",
            "6250 [D loss: 1.068782] [G loss: 1.191157]\n",
            "[[0.5039101  0.497178   0.5020992  0.50057614]\n",
            " [0.6736913  0.41452035 0.6366088  0.30891162]\n",
            " [0.5766831  0.40925717 0.5642249  0.52765197]\n",
            " [0.5039101  0.497178   0.5020992  0.50057614]\n",
            " [0.72926104 0.45267895 0.6970973  0.30008605]]\n",
            "6251 [D loss: 1.073536] [G loss: 1.193014]\n",
            "6252 [D loss: 1.087509] [G loss: 1.188593]\n",
            "6253 [D loss: 1.072572] [G loss: 1.194790]\n",
            "6254 [D loss: 1.082895] [G loss: 1.191308]\n",
            "6255 [D loss: 1.082463] [G loss: 1.208556]\n",
            "6256 [D loss: 1.098851] [G loss: 1.190788]\n",
            "6257 [D loss: 1.080548] [G loss: 1.201336]\n",
            "6258 [D loss: 1.073221] [G loss: 1.194420]\n",
            "6259 [D loss: 1.094665] [G loss: 1.177572]\n",
            "6260 [D loss: 1.096555] [G loss: 1.181927]\n",
            "6261 [D loss: 1.080257] [G loss: 1.203854]\n",
            "6262 [D loss: 1.085850] [G loss: 1.200370]\n",
            "6263 [D loss: 1.088766] [G loss: 1.186766]\n",
            "6264 [D loss: 1.098407] [G loss: 1.197602]\n",
            "6265 [D loss: 1.073276] [G loss: 1.206929]\n",
            "6266 [D loss: 1.101584] [G loss: 1.187917]\n",
            "6267 [D loss: 1.092348] [G loss: 1.174901]\n",
            "6268 [D loss: 1.070744] [G loss: 1.198680]\n",
            "6269 [D loss: 1.091373] [G loss: 1.186210]\n",
            "6270 [D loss: 1.090334] [G loss: 1.185411]\n",
            "6271 [D loss: 1.075736] [G loss: 1.193596]\n",
            "6272 [D loss: 1.096125] [G loss: 1.175290]\n",
            "6273 [D loss: 1.097772] [G loss: 1.181876]\n",
            "6274 [D loss: 1.085399] [G loss: 1.178060]\n",
            "6275 [D loss: 1.084098] [G loss: 1.173257]\n",
            "6276 [D loss: 1.088317] [G loss: 1.192369]\n",
            "6277 [D loss: 1.076354] [G loss: 1.183295]\n",
            "6278 [D loss: 1.104182] [G loss: 1.180308]\n",
            "6279 [D loss: 1.079985] [G loss: 1.192335]\n",
            "6280 [D loss: 1.083449] [G loss: 1.182060]\n",
            "6281 [D loss: 1.082802] [G loss: 1.191601]\n",
            "6282 [D loss: 1.086768] [G loss: 1.208479]\n",
            "6283 [D loss: 1.100513] [G loss: 1.179843]\n",
            "6284 [D loss: 1.073663] [G loss: 1.206329]\n",
            "6285 [D loss: 1.081633] [G loss: 1.197389]\n",
            "6286 [D loss: 1.097947] [G loss: 1.203374]\n",
            "6287 [D loss: 1.076722] [G loss: 1.192551]\n",
            "6288 [D loss: 1.076990] [G loss: 1.198688]\n",
            "6289 [D loss: 1.054851] [G loss: 1.203156]\n",
            "6290 [D loss: 1.085275] [G loss: 1.192833]\n",
            "6291 [D loss: 1.089722] [G loss: 1.200779]\n",
            "6292 [D loss: 1.085107] [G loss: 1.207201]\n",
            "6293 [D loss: 1.070802] [G loss: 1.191741]\n",
            "6294 [D loss: 1.095212] [G loss: 1.200682]\n",
            "6295 [D loss: 1.080723] [G loss: 1.199307]\n",
            "6296 [D loss: 1.095526] [G loss: 1.184193]\n",
            "6297 [D loss: 1.068825] [G loss: 1.195366]\n",
            "6298 [D loss: 1.079424] [G loss: 1.192029]\n",
            "6299 [D loss: 1.097543] [G loss: 1.179262]\n",
            "6300 [D loss: 1.068174] [G loss: 1.199873]\n",
            "[[0.6115352  0.39546588 0.5613084  0.5052578 ]\n",
            " [0.680677   0.39330623 0.71069145 0.39981678]\n",
            " [0.7185549  0.28193194 0.5791248  0.5182238 ]\n",
            " [0.73814994 0.37604004 0.64509386 0.3155122 ]\n",
            " [0.5504443  0.33997703 0.52011687 0.6149271 ]]\n",
            "6301 [D loss: 1.091528] [G loss: 1.198042]\n",
            "6302 [D loss: 1.074117] [G loss: 1.198225]\n",
            "6303 [D loss: 1.093548] [G loss: 1.198352]\n",
            "6304 [D loss: 1.073668] [G loss: 1.188639]\n",
            "6305 [D loss: 1.086629] [G loss: 1.201995]\n",
            "6306 [D loss: 1.063681] [G loss: 1.205930]\n",
            "6307 [D loss: 1.092628] [G loss: 1.212598]\n",
            "6308 [D loss: 1.091599] [G loss: 1.182600]\n",
            "6309 [D loss: 1.103253] [G loss: 1.177106]\n",
            "6310 [D loss: 1.084848] [G loss: 1.186912]\n",
            "6311 [D loss: 1.084489] [G loss: 1.187969]\n",
            "6312 [D loss: 1.092272] [G loss: 1.185976]\n",
            "6313 [D loss: 1.077748] [G loss: 1.187262]\n",
            "6314 [D loss: 1.070369] [G loss: 1.191802]\n",
            "6315 [D loss: 1.072717] [G loss: 1.204613]\n",
            "6316 [D loss: 1.085522] [G loss: 1.183342]\n",
            "6317 [D loss: 1.069806] [G loss: 1.204458]\n",
            "6318 [D loss: 1.080919] [G loss: 1.193028]\n",
            "6319 [D loss: 1.096509] [G loss: 1.175971]\n",
            "6320 [D loss: 1.079817] [G loss: 1.187286]\n",
            "6321 [D loss: 1.081697] [G loss: 1.176266]\n",
            "6322 [D loss: 1.076485] [G loss: 1.193170]\n",
            "6323 [D loss: 1.081153] [G loss: 1.193401]\n",
            "6324 [D loss: 1.082545] [G loss: 1.184731]\n",
            "6325 [D loss: 1.097819] [G loss: 1.176801]\n",
            "6326 [D loss: 1.070719] [G loss: 1.208181]\n",
            "6327 [D loss: 1.096613] [G loss: 1.172501]\n",
            "6328 [D loss: 1.062442] [G loss: 1.206028]\n",
            "6329 [D loss: 1.091571] [G loss: 1.169981]\n",
            "6330 [D loss: 1.070027] [G loss: 1.207946]\n",
            "6331 [D loss: 1.076671] [G loss: 1.202634]\n",
            "6332 [D loss: 1.089179] [G loss: 1.192554]\n",
            "6333 [D loss: 1.096871] [G loss: 1.179829]\n",
            "6334 [D loss: 1.077476] [G loss: 1.192828]\n",
            "6335 [D loss: 1.094904] [G loss: 1.187468]\n",
            "6336 [D loss: 1.067609] [G loss: 1.193903]\n",
            "6337 [D loss: 1.085502] [G loss: 1.182107]\n",
            "6338 [D loss: 1.075460] [G loss: 1.182753]\n",
            "6339 [D loss: 1.094678] [G loss: 1.181539]\n",
            "6340 [D loss: 1.094946] [G loss: 1.198049]\n",
            "6341 [D loss: 1.050761] [G loss: 1.217342]\n",
            "6342 [D loss: 1.083117] [G loss: 1.184532]\n",
            "6343 [D loss: 1.068690] [G loss: 1.197900]\n",
            "6344 [D loss: 1.090162] [G loss: 1.181993]\n",
            "6345 [D loss: 1.079992] [G loss: 1.191174]\n",
            "6346 [D loss: 1.075675] [G loss: 1.188228]\n",
            "6347 [D loss: 1.071671] [G loss: 1.193857]\n",
            "6348 [D loss: 1.082885] [G loss: 1.195904]\n",
            "6349 [D loss: 1.074739] [G loss: 1.184723]\n",
            "6350 [D loss: 1.074972] [G loss: 1.201775]\n",
            "[[0.50510687 0.49761614 0.5014713  0.49912515]\n",
            " [0.49886462 0.4809686  0.5253894  0.51008207]\n",
            " [0.9458702  0.13851039 0.7459841  0.21687332]\n",
            " [0.83889854 0.284355   0.6964218  0.20418732]\n",
            " [0.52973205 0.47261202 0.5102636  0.49778658]]\n",
            "6351 [D loss: 1.081760] [G loss: 1.191017]\n",
            "6352 [D loss: 1.090034] [G loss: 1.195870]\n",
            "6353 [D loss: 1.090404] [G loss: 1.188743]\n",
            "6354 [D loss: 1.075522] [G loss: 1.217890]\n",
            "6355 [D loss: 1.092330] [G loss: 1.187665]\n",
            "6356 [D loss: 1.082455] [G loss: 1.198088]\n",
            "6357 [D loss: 1.084464] [G loss: 1.197750]\n",
            "6358 [D loss: 1.091436] [G loss: 1.187645]\n",
            "6359 [D loss: 1.076868] [G loss: 1.192987]\n",
            "6360 [D loss: 1.089513] [G loss: 1.194327]\n",
            "6361 [D loss: 1.083144] [G loss: 1.194813]\n",
            "6362 [D loss: 1.081978] [G loss: 1.191279]\n",
            "6363 [D loss: 1.078270] [G loss: 1.192656]\n",
            "6364 [D loss: 1.081604] [G loss: 1.194874]\n",
            "6365 [D loss: 1.084011] [G loss: 1.180209]\n",
            "6366 [D loss: 1.094368] [G loss: 1.188894]\n",
            "6367 [D loss: 1.101443] [G loss: 1.179745]\n",
            "6368 [D loss: 1.071980] [G loss: 1.204996]\n",
            "6369 [D loss: 1.093945] [G loss: 1.192060]\n",
            "6370 [D loss: 1.081108] [G loss: 1.206415]\n",
            "6371 [D loss: 1.082800] [G loss: 1.195217]\n",
            "6372 [D loss: 1.090925] [G loss: 1.170322]\n",
            "6373 [D loss: 1.076269] [G loss: 1.194666]\n",
            "6374 [D loss: 1.077750] [G loss: 1.187996]\n",
            "6375 [D loss: 1.073709] [G loss: 1.190000]\n",
            "6376 [D loss: 1.088589] [G loss: 1.194546]\n",
            "6377 [D loss: 1.084665] [G loss: 1.192290]\n",
            "6378 [D loss: 1.090346] [G loss: 1.182898]\n",
            "6379 [D loss: 1.079039] [G loss: 1.186206]\n",
            "6380 [D loss: 1.069967] [G loss: 1.194896]\n",
            "6381 [D loss: 1.075144] [G loss: 1.180655]\n",
            "6382 [D loss: 1.075686] [G loss: 1.204347]\n",
            "6383 [D loss: 1.087883] [G loss: 1.188078]\n",
            "6384 [D loss: 1.102824] [G loss: 1.183753]\n",
            "6385 [D loss: 1.079873] [G loss: 1.186909]\n",
            "6386 [D loss: 1.098160] [G loss: 1.175316]\n",
            "6387 [D loss: 1.088557] [G loss: 1.201103]\n",
            "6388 [D loss: 1.089387] [G loss: 1.176448]\n",
            "6389 [D loss: 1.086773] [G loss: 1.187791]\n",
            "6390 [D loss: 1.084086] [G loss: 1.181207]\n",
            "6391 [D loss: 1.088996] [G loss: 1.191809]\n",
            "6392 [D loss: 1.081515] [G loss: 1.185250]\n",
            "6393 [D loss: 1.073048] [G loss: 1.204807]\n",
            "6394 [D loss: 1.098349] [G loss: 1.187218]\n",
            "6395 [D loss: 1.089425] [G loss: 1.187155]\n",
            "6396 [D loss: 1.078942] [G loss: 1.190945]\n",
            "6397 [D loss: 1.095974] [G loss: 1.185604]\n",
            "6398 [D loss: 1.092541] [G loss: 1.178897]\n",
            "6399 [D loss: 1.089016] [G loss: 1.186067]\n",
            "6400 [D loss: 1.082724] [G loss: 1.198797]\n",
            "[[0.5071492  0.49848932 0.50500005 0.49904886]\n",
            " [0.53621227 0.44101128 0.5267065  0.53529257]\n",
            " [0.79401004 0.30848733 0.65004575 0.3454594 ]\n",
            " [0.5096529  0.4689579  0.5330321  0.5249792 ]\n",
            " [0.7855312  0.27847093 0.609281   0.38424852]]\n",
            "6401 [D loss: 1.095505] [G loss: 1.183701]\n",
            "6402 [D loss: 1.068680] [G loss: 1.196823]\n",
            "6403 [D loss: 1.084643] [G loss: 1.195229]\n",
            "6404 [D loss: 1.070655] [G loss: 1.194049]\n",
            "6405 [D loss: 1.072334] [G loss: 1.200068]\n",
            "6406 [D loss: 1.092474] [G loss: 1.192661]\n",
            "6407 [D loss: 1.091022] [G loss: 1.178384]\n",
            "6408 [D loss: 1.090987] [G loss: 1.189523]\n",
            "6409 [D loss: 1.090353] [G loss: 1.189897]\n",
            "6410 [D loss: 1.088552] [G loss: 1.198821]\n",
            "6411 [D loss: 1.089925] [G loss: 1.185624]\n",
            "6412 [D loss: 1.086140] [G loss: 1.213284]\n",
            "6413 [D loss: 1.071532] [G loss: 1.214684]\n",
            "6414 [D loss: 1.103467] [G loss: 1.178614]\n",
            "6415 [D loss: 1.086678] [G loss: 1.189200]\n",
            "6416 [D loss: 1.085828] [G loss: 1.185447]\n",
            "6417 [D loss: 1.095909] [G loss: 1.195784]\n",
            "6418 [D loss: 1.073762] [G loss: 1.190128]\n",
            "6419 [D loss: 1.072376] [G loss: 1.186316]\n",
            "6420 [D loss: 1.071535] [G loss: 1.188086]\n",
            "6421 [D loss: 1.098491] [G loss: 1.179597]\n",
            "6422 [D loss: 1.084567] [G loss: 1.173664]\n",
            "6423 [D loss: 1.079358] [G loss: 1.189056]\n",
            "6424 [D loss: 1.086999] [G loss: 1.194160]\n",
            "6425 [D loss: 1.097019] [G loss: 1.184128]\n",
            "6426 [D loss: 1.089463] [G loss: 1.183300]\n",
            "6427 [D loss: 1.093588] [G loss: 1.186235]\n",
            "6428 [D loss: 1.077899] [G loss: 1.196850]\n",
            "6429 [D loss: 1.085285] [G loss: 1.199183]\n",
            "6430 [D loss: 1.095535] [G loss: 1.186803]\n",
            "6431 [D loss: 1.084429] [G loss: 1.189247]\n",
            "6432 [D loss: 1.099859] [G loss: 1.180938]\n",
            "6433 [D loss: 1.084446] [G loss: 1.193729]\n",
            "6434 [D loss: 1.076810] [G loss: 1.195953]\n",
            "6435 [D loss: 1.073457] [G loss: 1.189664]\n",
            "6436 [D loss: 1.072904] [G loss: 1.182627]\n",
            "6437 [D loss: 1.080988] [G loss: 1.182073]\n",
            "6438 [D loss: 1.083352] [G loss: 1.191460]\n",
            "6439 [D loss: 1.064399] [G loss: 1.206056]\n",
            "6440 [D loss: 1.079037] [G loss: 1.182597]\n",
            "6441 [D loss: 1.095164] [G loss: 1.173367]\n",
            "6442 [D loss: 1.070177] [G loss: 1.200547]\n",
            "6443 [D loss: 1.081468] [G loss: 1.175517]\n",
            "6444 [D loss: 1.072104] [G loss: 1.199686]\n",
            "6445 [D loss: 1.080302] [G loss: 1.181806]\n",
            "6446 [D loss: 1.093431] [G loss: 1.184341]\n",
            "6447 [D loss: 1.095587] [G loss: 1.173434]\n",
            "6448 [D loss: 1.096468] [G loss: 1.181062]\n",
            "6449 [D loss: 1.096285] [G loss: 1.181624]\n",
            "6450 [D loss: 1.074288] [G loss: 1.186447]\n",
            "[[0.5253928  0.48384017 0.5017228  0.50446236]\n",
            " [0.50407165 0.49707016 0.5021531  0.5005176 ]\n",
            " [0.5658308  0.49199262 0.4891166  0.44741878]\n",
            " [0.50407165 0.49707016 0.5021531  0.5005176 ]\n",
            " [0.50764966 0.48153016 0.513317   0.49978027]]\n",
            "6451 [D loss: 1.066672] [G loss: 1.211869]\n",
            "6452 [D loss: 1.085149] [G loss: 1.191756]\n",
            "6453 [D loss: 1.092439] [G loss: 1.186422]\n",
            "6454 [D loss: 1.087215] [G loss: 1.184051]\n",
            "6455 [D loss: 1.082221] [G loss: 1.171893]\n",
            "6456 [D loss: 1.099789] [G loss: 1.179678]\n",
            "6457 [D loss: 1.079450] [G loss: 1.184462]\n",
            "6458 [D loss: 1.088139] [G loss: 1.174450]\n",
            "6459 [D loss: 1.077110] [G loss: 1.193672]\n",
            "6460 [D loss: 1.084788] [G loss: 1.187628]\n",
            "6461 [D loss: 1.104612] [G loss: 1.185361]\n",
            "6462 [D loss: 1.115123] [G loss: 1.171202]\n",
            "6463 [D loss: 1.082989] [G loss: 1.181509]\n",
            "6464 [D loss: 1.095281] [G loss: 1.179390]\n",
            "6465 [D loss: 1.082846] [G loss: 1.194000]\n",
            "6466 [D loss: 1.092963] [G loss: 1.198653]\n",
            "6467 [D loss: 1.092923] [G loss: 1.180440]\n",
            "6468 [D loss: 1.064307] [G loss: 1.188676]\n",
            "6469 [D loss: 1.072648] [G loss: 1.184066]\n",
            "6470 [D loss: 1.080483] [G loss: 1.198248]\n",
            "6471 [D loss: 1.080606] [G loss: 1.203706]\n",
            "6472 [D loss: 1.068840] [G loss: 1.194875]\n",
            "6473 [D loss: 1.098561] [G loss: 1.183662]\n",
            "6474 [D loss: 1.078289] [G loss: 1.194447]\n",
            "6475 [D loss: 1.103555] [G loss: 1.185375]\n",
            "6476 [D loss: 1.068829] [G loss: 1.184454]\n",
            "6477 [D loss: 1.083098] [G loss: 1.200159]\n",
            "6478 [D loss: 1.076928] [G loss: 1.195422]\n",
            "6479 [D loss: 1.093213] [G loss: 1.169068]\n",
            "6480 [D loss: 1.091681] [G loss: 1.193044]\n",
            "6481 [D loss: 1.081874] [G loss: 1.203190]\n",
            "6482 [D loss: 1.079828] [G loss: 1.186711]\n",
            "6483 [D loss: 1.096184] [G loss: 1.179247]\n",
            "6484 [D loss: 1.104353] [G loss: 1.176960]\n",
            "6485 [D loss: 1.098660] [G loss: 1.195779]\n",
            "6486 [D loss: 1.068315] [G loss: 1.202460]\n",
            "6487 [D loss: 1.078928] [G loss: 1.191281]\n",
            "6488 [D loss: 1.077404] [G loss: 1.199109]\n",
            "6489 [D loss: 1.103775] [G loss: 1.168750]\n",
            "6490 [D loss: 1.084313] [G loss: 1.179911]\n",
            "6491 [D loss: 1.085350] [G loss: 1.182832]\n",
            "6492 [D loss: 1.088853] [G loss: 1.171152]\n",
            "6493 [D loss: 1.089185] [G loss: 1.179269]\n",
            "6494 [D loss: 1.089169] [G loss: 1.179487]\n",
            "6495 [D loss: 1.096656] [G loss: 1.174479]\n",
            "6496 [D loss: 1.084915] [G loss: 1.170939]\n",
            "6497 [D loss: 1.075491] [G loss: 1.184065]\n",
            "6498 [D loss: 1.085272] [G loss: 1.176394]\n",
            "6499 [D loss: 1.087995] [G loss: 1.188761]\n",
            "6500 [D loss: 1.082970] [G loss: 1.188814]\n",
            "[[0.7449259  0.3907034  0.70744234 0.25649813]\n",
            " [0.75779015 0.32334852 0.6577447  0.29090393]\n",
            " [0.504112   0.4970433  0.50216675 0.5005027 ]\n",
            " [0.5077904  0.47144324 0.50827336 0.511035  ]\n",
            " [0.5528159  0.4515517  0.51598424 0.51984185]]\n",
            "6501 [D loss: 1.093356] [G loss: 1.183897]\n",
            "6502 [D loss: 1.083683] [G loss: 1.191066]\n",
            "6503 [D loss: 1.109903] [G loss: 1.175658]\n",
            "6504 [D loss: 1.074539] [G loss: 1.202466]\n",
            "6505 [D loss: 1.069594] [G loss: 1.190719]\n",
            "6506 [D loss: 1.082298] [G loss: 1.181304]\n",
            "6507 [D loss: 1.082364] [G loss: 1.178490]\n",
            "6508 [D loss: 1.086307] [G loss: 1.189902]\n",
            "6509 [D loss: 1.095180] [G loss: 1.192033]\n",
            "6510 [D loss: 1.082834] [G loss: 1.179573]\n",
            "6511 [D loss: 1.087571] [G loss: 1.183389]\n",
            "6512 [D loss: 1.070242] [G loss: 1.201587]\n",
            "6513 [D loss: 1.076490] [G loss: 1.195478]\n",
            "6514 [D loss: 1.081618] [G loss: 1.182939]\n",
            "6515 [D loss: 1.075230] [G loss: 1.181271]\n",
            "6516 [D loss: 1.084406] [G loss: 1.191516]\n",
            "6517 [D loss: 1.083849] [G loss: 1.172766]\n",
            "6518 [D loss: 1.084303] [G loss: 1.188772]\n",
            "6519 [D loss: 1.077686] [G loss: 1.205182]\n",
            "6520 [D loss: 1.093242] [G loss: 1.177654]\n",
            "6521 [D loss: 1.089996] [G loss: 1.191685]\n",
            "6522 [D loss: 1.082277] [G loss: 1.188799]\n",
            "6523 [D loss: 1.093154] [G loss: 1.187378]\n",
            "6524 [D loss: 1.097869] [G loss: 1.178633]\n",
            "6525 [D loss: 1.085862] [G loss: 1.170603]\n",
            "6526 [D loss: 1.101102] [G loss: 1.160043]\n",
            "6527 [D loss: 1.078790] [G loss: 1.181456]\n",
            "6528 [D loss: 1.088708] [G loss: 1.179859]\n",
            "6529 [D loss: 1.077460] [G loss: 1.187869]\n",
            "6530 [D loss: 1.061945] [G loss: 1.190286]\n",
            "6531 [D loss: 1.088207] [G loss: 1.180936]\n",
            "6532 [D loss: 1.078572] [G loss: 1.188644]\n",
            "6533 [D loss: 1.088694] [G loss: 1.181546]\n",
            "6534 [D loss: 1.091236] [G loss: 1.192393]\n",
            "6535 [D loss: 1.079493] [G loss: 1.171633]\n",
            "6536 [D loss: 1.091903] [G loss: 1.177319]\n",
            "6537 [D loss: 1.087989] [G loss: 1.173590]\n",
            "6538 [D loss: 1.078506] [G loss: 1.180814]\n",
            "6539 [D loss: 1.073281] [G loss: 1.194916]\n",
            "6540 [D loss: 1.081127] [G loss: 1.196796]\n",
            "6541 [D loss: 1.078445] [G loss: 1.174911]\n",
            "6542 [D loss: 1.083863] [G loss: 1.173823]\n",
            "6543 [D loss: 1.103661] [G loss: 1.170489]\n",
            "6544 [D loss: 1.080916] [G loss: 1.199034]\n",
            "6545 [D loss: 1.084572] [G loss: 1.196851]\n",
            "6546 [D loss: 1.082398] [G loss: 1.185100]\n",
            "6547 [D loss: 1.080780] [G loss: 1.190873]\n",
            "6548 [D loss: 1.086068] [G loss: 1.164789]\n",
            "6549 [D loss: 1.097629] [G loss: 1.182687]\n",
            "6550 [D loss: 1.090945] [G loss: 1.184032]\n",
            "[[0.6459739  0.38143802 0.5759443  0.39094076]\n",
            " [0.5342864  0.47352836 0.5116221  0.50015604]\n",
            " [0.6143023  0.44217774 0.51332414 0.4463642 ]\n",
            " [0.55354685 0.42348218 0.5312609  0.5342267 ]\n",
            " [0.6805456  0.31734192 0.51529014 0.5024519 ]]\n",
            "6551 [D loss: 1.077516] [G loss: 1.209975]\n",
            "6552 [D loss: 1.090615] [G loss: 1.181437]\n",
            "6553 [D loss: 1.081931] [G loss: 1.177810]\n",
            "6554 [D loss: 1.070773] [G loss: 1.192315]\n",
            "6555 [D loss: 1.071710] [G loss: 1.193661]\n",
            "6556 [D loss: 1.081526] [G loss: 1.181951]\n",
            "6557 [D loss: 1.076165] [G loss: 1.196998]\n",
            "6558 [D loss: 1.100624] [G loss: 1.169305]\n",
            "6559 [D loss: 1.096316] [G loss: 1.181815]\n",
            "6560 [D loss: 1.085542] [G loss: 1.188063]\n",
            "6561 [D loss: 1.086749] [G loss: 1.182972]\n",
            "6562 [D loss: 1.094549] [G loss: 1.189041]\n",
            "6563 [D loss: 1.082755] [G loss: 1.189477]\n",
            "6564 [D loss: 1.067965] [G loss: 1.207318]\n",
            "6565 [D loss: 1.063844] [G loss: 1.186843]\n",
            "6566 [D loss: 1.093434] [G loss: 1.192731]\n",
            "6567 [D loss: 1.085674] [G loss: 1.209053]\n",
            "6568 [D loss: 1.099794] [G loss: 1.181169]\n",
            "6569 [D loss: 1.076500] [G loss: 1.185444]\n",
            "6570 [D loss: 1.090418] [G loss: 1.170578]\n",
            "6571 [D loss: 1.081111] [G loss: 1.189807]\n",
            "6572 [D loss: 1.085739] [G loss: 1.199183]\n",
            "6573 [D loss: 1.076993] [G loss: 1.189052]\n",
            "6574 [D loss: 1.082069] [G loss: 1.184284]\n",
            "6575 [D loss: 1.077662] [G loss: 1.176873]\n",
            "6576 [D loss: 1.095892] [G loss: 1.195241]\n",
            "6577 [D loss: 1.102098] [G loss: 1.176473]\n",
            "6578 [D loss: 1.085006] [G loss: 1.194842]\n",
            "6579 [D loss: 1.096587] [G loss: 1.179628]\n",
            "6580 [D loss: 1.077117] [G loss: 1.194304]\n",
            "6581 [D loss: 1.082383] [G loss: 1.179252]\n",
            "6582 [D loss: 1.074700] [G loss: 1.189168]\n",
            "6583 [D loss: 1.073685] [G loss: 1.187592]\n",
            "6584 [D loss: 1.091194] [G loss: 1.181197]\n",
            "6585 [D loss: 1.075066] [G loss: 1.192608]\n",
            "6586 [D loss: 1.104154] [G loss: 1.182318]\n",
            "6587 [D loss: 1.073967] [G loss: 1.199577]\n",
            "6588 [D loss: 1.079451] [G loss: 1.179191]\n",
            "6589 [D loss: 1.089496] [G loss: 1.183813]\n",
            "6590 [D loss: 1.075801] [G loss: 1.198710]\n",
            "6591 [D loss: 1.095730] [G loss: 1.181051]\n",
            "6592 [D loss: 1.095195] [G loss: 1.176813]\n",
            "6593 [D loss: 1.081360] [G loss: 1.179654]\n",
            "6594 [D loss: 1.094316] [G loss: 1.185845]\n",
            "6595 [D loss: 1.093866] [G loss: 1.184948]\n",
            "6596 [D loss: 1.092296] [G loss: 1.185133]\n",
            "6597 [D loss: 1.094165] [G loss: 1.178234]\n",
            "6598 [D loss: 1.091357] [G loss: 1.194625]\n",
            "6599 [D loss: 1.082018] [G loss: 1.189423]\n",
            "6600 [D loss: 1.091685] [G loss: 1.187503]\n",
            "[[0.68346983 0.3617572  0.62468785 0.3395379 ]\n",
            " [0.50419277 0.49698967 0.502194   0.50047296]\n",
            " [0.50419277 0.49698967 0.502194   0.50047296]\n",
            " [0.8920018  0.21310231 0.7457225  0.24649982]\n",
            " [0.5859652  0.43259612 0.51029307 0.5035806 ]]\n",
            "6601 [D loss: 1.093607] [G loss: 1.175045]\n",
            "6602 [D loss: 1.079811] [G loss: 1.208563]\n",
            "6603 [D loss: 1.086198] [G loss: 1.190779]\n",
            "6604 [D loss: 1.103883] [G loss: 1.170607]\n",
            "6605 [D loss: 1.075592] [G loss: 1.180227]\n",
            "6606 [D loss: 1.092675] [G loss: 1.172686]\n",
            "6607 [D loss: 1.092065] [G loss: 1.173902]\n",
            "6608 [D loss: 1.048872] [G loss: 1.206271]\n",
            "6609 [D loss: 1.083094] [G loss: 1.171338]\n",
            "6610 [D loss: 1.099023] [G loss: 1.173557]\n",
            "6611 [D loss: 1.075981] [G loss: 1.193548]\n",
            "6612 [D loss: 1.098455] [G loss: 1.169515]\n",
            "6613 [D loss: 1.090618] [G loss: 1.185349]\n",
            "6614 [D loss: 1.107086] [G loss: 1.166079]\n",
            "6615 [D loss: 1.076433] [G loss: 1.187567]\n",
            "6616 [D loss: 1.076683] [G loss: 1.193464]\n",
            "6617 [D loss: 1.099340] [G loss: 1.186510]\n",
            "6618 [D loss: 1.081350] [G loss: 1.183290]\n",
            "6619 [D loss: 1.059592] [G loss: 1.184148]\n",
            "6620 [D loss: 1.089107] [G loss: 1.192798]\n",
            "6621 [D loss: 1.081457] [G loss: 1.193154]\n",
            "6622 [D loss: 1.100726] [G loss: 1.190526]\n",
            "6623 [D loss: 1.091980] [G loss: 1.193973]\n",
            "6624 [D loss: 1.075547] [G loss: 1.208142]\n",
            "6625 [D loss: 1.089278] [G loss: 1.185857]\n",
            "6626 [D loss: 1.084124] [G loss: 1.176276]\n",
            "6627 [D loss: 1.091700] [G loss: 1.172825]\n",
            "6628 [D loss: 1.072904] [G loss: 1.197045]\n",
            "6629 [D loss: 1.079051] [G loss: 1.191443]\n",
            "6630 [D loss: 1.082643] [G loss: 1.183582]\n",
            "6631 [D loss: 1.094967] [G loss: 1.169692]\n",
            "6632 [D loss: 1.100197] [G loss: 1.166039]\n",
            "6633 [D loss: 1.075565] [G loss: 1.185809]\n",
            "6634 [D loss: 1.084456] [G loss: 1.185080]\n",
            "6635 [D loss: 1.084986] [G loss: 1.177210]\n",
            "6636 [D loss: 1.074853] [G loss: 1.206703]\n",
            "6637 [D loss: 1.106179] [G loss: 1.180456]\n",
            "6638 [D loss: 1.088127] [G loss: 1.183088]\n",
            "6639 [D loss: 1.082942] [G loss: 1.210172]\n",
            "6640 [D loss: 1.085440] [G loss: 1.179257]\n",
            "6641 [D loss: 1.079014] [G loss: 1.202371]\n",
            "6642 [D loss: 1.090142] [G loss: 1.176234]\n",
            "6643 [D loss: 1.092701] [G loss: 1.172492]\n",
            "6644 [D loss: 1.096963] [G loss: 1.175572]\n",
            "6645 [D loss: 1.080979] [G loss: 1.174322]\n",
            "6646 [D loss: 1.084887] [G loss: 1.182287]\n",
            "6647 [D loss: 1.070326] [G loss: 1.182513]\n",
            "6648 [D loss: 1.090406] [G loss: 1.186267]\n",
            "6649 [D loss: 1.117531] [G loss: 1.152265]\n",
            "6650 [D loss: 1.069874] [G loss: 1.173306]\n",
            "[[0.5063706  0.4830804  0.50567675 0.506053  ]\n",
            " [0.5231116  0.4372583  0.50984895 0.55408263]\n",
            " [0.5049484  0.49327645 0.503329   0.5018227 ]\n",
            " [0.5836403  0.2866831  0.5812286  0.6151266 ]\n",
            " [0.5521262  0.4457567  0.48914272 0.5085731 ]]\n",
            "6651 [D loss: 1.090411] [G loss: 1.173656]\n",
            "6652 [D loss: 1.076828] [G loss: 1.186162]\n",
            "6653 [D loss: 1.091083] [G loss: 1.183716]\n",
            "6654 [D loss: 1.094069] [G loss: 1.177325]\n",
            "6655 [D loss: 1.078537] [G loss: 1.189039]\n",
            "6656 [D loss: 1.087761] [G loss: 1.178382]\n",
            "6657 [D loss: 1.096765] [G loss: 1.177454]\n",
            "6658 [D loss: 1.077480] [G loss: 1.197140]\n",
            "6659 [D loss: 1.098999] [G loss: 1.186287]\n",
            "6660 [D loss: 1.080472] [G loss: 1.190308]\n",
            "6661 [D loss: 1.081078] [G loss: 1.184832]\n",
            "6662 [D loss: 1.095245] [G loss: 1.175376]\n",
            "6663 [D loss: 1.104050] [G loss: 1.165151]\n",
            "6664 [D loss: 1.092233] [G loss: 1.174666]\n",
            "6665 [D loss: 1.080055] [G loss: 1.177220]\n",
            "6666 [D loss: 1.088925] [G loss: 1.173517]\n",
            "6667 [D loss: 1.077499] [G loss: 1.177335]\n",
            "6668 [D loss: 1.085667] [G loss: 1.181387]\n",
            "6669 [D loss: 1.096153] [G loss: 1.187532]\n",
            "6670 [D loss: 1.076721] [G loss: 1.176716]\n",
            "6671 [D loss: 1.098669] [G loss: 1.165948]\n",
            "6672 [D loss: 1.099719] [G loss: 1.174589]\n",
            "6673 [D loss: 1.106396] [G loss: 1.168465]\n",
            "6674 [D loss: 1.088146] [G loss: 1.175712]\n",
            "6675 [D loss: 1.094426] [G loss: 1.186491]\n",
            "6676 [D loss: 1.085333] [G loss: 1.172130]\n",
            "6677 [D loss: 1.095497] [G loss: 1.190972]\n",
            "6678 [D loss: 1.073009] [G loss: 1.179862]\n",
            "6679 [D loss: 1.071983] [G loss: 1.189986]\n",
            "6680 [D loss: 1.097135] [G loss: 1.176182]\n",
            "6681 [D loss: 1.100889] [G loss: 1.180391]\n",
            "6682 [D loss: 1.097245] [G loss: 1.176987]\n",
            "6683 [D loss: 1.093654] [G loss: 1.166414]\n",
            "6684 [D loss: 1.087544] [G loss: 1.179331]\n",
            "6685 [D loss: 1.103560] [G loss: 1.173795]\n",
            "6686 [D loss: 1.112135] [G loss: 1.159700]\n",
            "6687 [D loss: 1.077567] [G loss: 1.188944]\n",
            "6688 [D loss: 1.083212] [G loss: 1.191364]\n",
            "6689 [D loss: 1.087927] [G loss: 1.189460]\n",
            "6690 [D loss: 1.102298] [G loss: 1.161913]\n",
            "6691 [D loss: 1.106912] [G loss: 1.182206]\n",
            "6692 [D loss: 1.093168] [G loss: 1.184028]\n",
            "6693 [D loss: 1.081201] [G loss: 1.168014]\n",
            "6694 [D loss: 1.095410] [G loss: 1.170192]\n",
            "6695 [D loss: 1.084765] [G loss: 1.192328]\n",
            "6696 [D loss: 1.085312] [G loss: 1.196139]\n",
            "6697 [D loss: 1.102643] [G loss: 1.160101]\n",
            "6698 [D loss: 1.096239] [G loss: 1.190367]\n",
            "6699 [D loss: 1.089288] [G loss: 1.182992]\n",
            "6700 [D loss: 1.087757] [G loss: 1.189896]\n",
            "[[0.63545275 0.24999201 0.480961   0.60229826]\n",
            " [0.57771826 0.43174976 0.55767286 0.435672  ]\n",
            " [0.508084   0.47130203 0.5084219  0.5108973 ]\n",
            " [0.50427365 0.49693602 0.50222105 0.50044316]\n",
            " [0.6688883  0.31186727 0.5344213  0.44670072]]\n",
            "6701 [D loss: 1.084419] [G loss: 1.173339]\n",
            "6702 [D loss: 1.083052] [G loss: 1.184899]\n",
            "6703 [D loss: 1.084166] [G loss: 1.187367]\n",
            "6704 [D loss: 1.087961] [G loss: 1.184560]\n",
            "6705 [D loss: 1.086228] [G loss: 1.174237]\n",
            "6706 [D loss: 1.077223] [G loss: 1.185832]\n",
            "6707 [D loss: 1.060066] [G loss: 1.199202]\n",
            "6708 [D loss: 1.084193] [G loss: 1.193535]\n",
            "6709 [D loss: 1.099654] [G loss: 1.188386]\n",
            "6710 [D loss: 1.095996] [G loss: 1.176485]\n",
            "6711 [D loss: 1.091381] [G loss: 1.191459]\n",
            "6712 [D loss: 1.096618] [G loss: 1.184926]\n",
            "6713 [D loss: 1.106848] [G loss: 1.173935]\n",
            "6714 [D loss: 1.085314] [G loss: 1.196559]\n",
            "6715 [D loss: 1.092133] [G loss: 1.182462]\n",
            "6716 [D loss: 1.093420] [G loss: 1.185970]\n",
            "6717 [D loss: 1.086212] [G loss: 1.175627]\n",
            "6718 [D loss: 1.076470] [G loss: 1.181173]\n",
            "6719 [D loss: 1.099974] [G loss: 1.172110]\n",
            "6720 [D loss: 1.089340] [G loss: 1.171932]\n",
            "6721 [D loss: 1.099845] [G loss: 1.158823]\n",
            "6722 [D loss: 1.094488] [G loss: 1.182624]\n",
            "6723 [D loss: 1.099626] [G loss: 1.170678]\n",
            "6724 [D loss: 1.086141] [G loss: 1.171579]\n",
            "6725 [D loss: 1.074928] [G loss: 1.198534]\n",
            "6726 [D loss: 1.090654] [G loss: 1.182581]\n",
            "6727 [D loss: 1.080977] [G loss: 1.183583]\n",
            "6728 [D loss: 1.082180] [G loss: 1.194431]\n",
            "6729 [D loss: 1.095699] [G loss: 1.184879]\n",
            "6730 [D loss: 1.082137] [G loss: 1.189101]\n",
            "6731 [D loss: 1.073881] [G loss: 1.195088]\n",
            "6732 [D loss: 1.085187] [G loss: 1.194659]\n",
            "6733 [D loss: 1.075612] [G loss: 1.181942]\n",
            "6734 [D loss: 1.082189] [G loss: 1.183751]\n",
            "6735 [D loss: 1.101899] [G loss: 1.161777]\n",
            "6736 [D loss: 1.100227] [G loss: 1.161223]\n",
            "6737 [D loss: 1.102005] [G loss: 1.164414]\n",
            "6738 [D loss: 1.095258] [G loss: 1.164671]\n",
            "6739 [D loss: 1.093785] [G loss: 1.182060]\n",
            "6740 [D loss: 1.102458] [G loss: 1.191944]\n",
            "6741 [D loss: 1.076092] [G loss: 1.183439]\n",
            "6742 [D loss: 1.085876] [G loss: 1.171730]\n",
            "6743 [D loss: 1.087757] [G loss: 1.202423]\n",
            "6744 [D loss: 1.073352] [G loss: 1.192071]\n",
            "6745 [D loss: 1.079068] [G loss: 1.192035]\n",
            "6746 [D loss: 1.090990] [G loss: 1.172999]\n",
            "6747 [D loss: 1.096118] [G loss: 1.182484]\n",
            "6748 [D loss: 1.083246] [G loss: 1.170727]\n",
            "6749 [D loss: 1.084262] [G loss: 1.173326]\n",
            "6750 [D loss: 1.065639] [G loss: 1.202570]\n",
            "[[0.9123694  0.26030758 0.80784315 0.12423395]\n",
            " [0.7980912  0.21870607 0.58591413 0.45911226]\n",
            " [0.7005621  0.3663715  0.5517392  0.40589666]\n",
            " [0.6790131  0.33611736 0.628856   0.34327996]\n",
            " [0.6053392  0.36278972 0.557675   0.5429899 ]]\n",
            "6751 [D loss: 1.075923] [G loss: 1.183664]\n",
            "6752 [D loss: 1.092457] [G loss: 1.180063]\n",
            "6753 [D loss: 1.083897] [G loss: 1.205493]\n",
            "6754 [D loss: 1.094873] [G loss: 1.178039]\n",
            "6755 [D loss: 1.084657] [G loss: 1.175833]\n",
            "6756 [D loss: 1.096945] [G loss: 1.174542]\n",
            "6757 [D loss: 1.100833] [G loss: 1.174081]\n",
            "6758 [D loss: 1.100726] [G loss: 1.175167]\n",
            "6759 [D loss: 1.092072] [G loss: 1.183855]\n",
            "6760 [D loss: 1.090915] [G loss: 1.190545]\n",
            "6761 [D loss: 1.079149] [G loss: 1.197969]\n",
            "6762 [D loss: 1.082037] [G loss: 1.182121]\n",
            "6763 [D loss: 1.066095] [G loss: 1.200401]\n",
            "6764 [D loss: 1.095879] [G loss: 1.172181]\n",
            "6765 [D loss: 1.085211] [G loss: 1.183212]\n",
            "6766 [D loss: 1.082228] [G loss: 1.181826]\n",
            "6767 [D loss: 1.086481] [G loss: 1.185513]\n",
            "6768 [D loss: 1.092321] [G loss: 1.186935]\n",
            "6769 [D loss: 1.101211] [G loss: 1.174885]\n",
            "6770 [D loss: 1.102555] [G loss: 1.176551]\n",
            "6771 [D loss: 1.074961] [G loss: 1.186860]\n",
            "6772 [D loss: 1.084146] [G loss: 1.176264]\n",
            "6773 [D loss: 1.088642] [G loss: 1.177180]\n",
            "6774 [D loss: 1.087853] [G loss: 1.156315]\n",
            "6775 [D loss: 1.077070] [G loss: 1.196138]\n",
            "6776 [D loss: 1.090284] [G loss: 1.194139]\n",
            "6777 [D loss: 1.087143] [G loss: 1.189943]\n",
            "6778 [D loss: 1.091381] [G loss: 1.170966]\n",
            "6779 [D loss: 1.105235] [G loss: 1.162165]\n",
            "6780 [D loss: 1.096162] [G loss: 1.172678]\n",
            "6781 [D loss: 1.098279] [G loss: 1.178192]\n",
            "6782 [D loss: 1.101022] [G loss: 1.172026]\n",
            "6783 [D loss: 1.098369] [G loss: 1.178179]\n",
            "6784 [D loss: 1.089623] [G loss: 1.181064]\n",
            "6785 [D loss: 1.092289] [G loss: 1.184451]\n",
            "6786 [D loss: 1.084912] [G loss: 1.184913]\n",
            "6787 [D loss: 1.083754] [G loss: 1.170585]\n",
            "6788 [D loss: 1.072588] [G loss: 1.186355]\n",
            "6789 [D loss: 1.081679] [G loss: 1.200516]\n",
            "6790 [D loss: 1.084643] [G loss: 1.167807]\n",
            "6791 [D loss: 1.079126] [G loss: 1.187953]\n",
            "6792 [D loss: 1.081197] [G loss: 1.175843]\n",
            "6793 [D loss: 1.097705] [G loss: 1.166220]\n",
            "6794 [D loss: 1.100652] [G loss: 1.187679]\n",
            "6795 [D loss: 1.069172] [G loss: 1.187582]\n",
            "6796 [D loss: 1.106588] [G loss: 1.186032]\n",
            "6797 [D loss: 1.076410] [G loss: 1.178307]\n",
            "6798 [D loss: 1.087157] [G loss: 1.196066]\n",
            "6799 [D loss: 1.084248] [G loss: 1.174166]\n",
            "6800 [D loss: 1.076501] [G loss: 1.182787]\n",
            "[[0.5632066  0.33874753 0.5039768  0.60109603]\n",
            " [0.51681787 0.4853773  0.50055987 0.5062775 ]\n",
            " [0.5043545  0.4968822  0.5022483  0.5004133 ]\n",
            " [0.68704    0.39931092 0.6075713  0.3842581 ]\n",
            " [0.8125023  0.29365155 0.64027876 0.3067209 ]]\n",
            "6801 [D loss: 1.066608] [G loss: 1.192837]\n",
            "6802 [D loss: 1.097883] [G loss: 1.168779]\n",
            "6803 [D loss: 1.095966] [G loss: 1.167249]\n",
            "6804 [D loss: 1.086812] [G loss: 1.174257]\n",
            "6805 [D loss: 1.075461] [G loss: 1.175883]\n",
            "6806 [D loss: 1.090276] [G loss: 1.172897]\n",
            "6807 [D loss: 1.084434] [G loss: 1.180565]\n",
            "6808 [D loss: 1.097805] [G loss: 1.176482]\n",
            "6809 [D loss: 1.086521] [G loss: 1.175202]\n",
            "6810 [D loss: 1.089247] [G loss: 1.182815]\n",
            "6811 [D loss: 1.098762] [G loss: 1.182823]\n",
            "6812 [D loss: 1.095969] [G loss: 1.159089]\n",
            "6813 [D loss: 1.094901] [G loss: 1.181654]\n",
            "6814 [D loss: 1.087596] [G loss: 1.174544]\n",
            "6815 [D loss: 1.113274] [G loss: 1.162369]\n",
            "6816 [D loss: 1.107173] [G loss: 1.172604]\n",
            "6817 [D loss: 1.105063] [G loss: 1.179615]\n",
            "6818 [D loss: 1.096661] [G loss: 1.191791]\n",
            "6819 [D loss: 1.077533] [G loss: 1.180927]\n",
            "6820 [D loss: 1.089634] [G loss: 1.167284]\n",
            "6821 [D loss: 1.091751] [G loss: 1.183561]\n",
            "6822 [D loss: 1.084802] [G loss: 1.169568]\n",
            "6823 [D loss: 1.097619] [G loss: 1.158484]\n",
            "6824 [D loss: 1.077462] [G loss: 1.178361]\n",
            "6825 [D loss: 1.104126] [G loss: 1.176336]\n",
            "6826 [D loss: 1.084472] [G loss: 1.174102]\n",
            "6827 [D loss: 1.090427] [G loss: 1.180599]\n",
            "6828 [D loss: 1.104002] [G loss: 1.166255]\n",
            "6829 [D loss: 1.092384] [G loss: 1.162231]\n",
            "6830 [D loss: 1.093350] [G loss: 1.165540]\n",
            "6831 [D loss: 1.095927] [G loss: 1.160470]\n",
            "6832 [D loss: 1.092881] [G loss: 1.188612]\n",
            "6833 [D loss: 1.097811] [G loss: 1.181072]\n",
            "6834 [D loss: 1.099057] [G loss: 1.156266]\n",
            "6835 [D loss: 1.082874] [G loss: 1.176570]\n",
            "6836 [D loss: 1.077383] [G loss: 1.175445]\n",
            "6837 [D loss: 1.069002] [G loss: 1.170723]\n",
            "6838 [D loss: 1.080948] [G loss: 1.178984]\n",
            "6839 [D loss: 1.094011] [G loss: 1.176510]\n",
            "6840 [D loss: 1.089673] [G loss: 1.160814]\n",
            "6841 [D loss: 1.078751] [G loss: 1.161414]\n",
            "6842 [D loss: 1.089621] [G loss: 1.183039]\n",
            "6843 [D loss: 1.078441] [G loss: 1.178645]\n",
            "6844 [D loss: 1.092918] [G loss: 1.154906]\n",
            "6845 [D loss: 1.110129] [G loss: 1.172471]\n",
            "6846 [D loss: 1.090802] [G loss: 1.167425]\n",
            "6847 [D loss: 1.087193] [G loss: 1.191420]\n",
            "6848 [D loss: 1.089008] [G loss: 1.179571]\n",
            "6849 [D loss: 1.084201] [G loss: 1.163980]\n",
            "6850 [D loss: 1.098940] [G loss: 1.160462]\n",
            "[[0.8103335  0.19990969 0.5808203  0.4651399 ]\n",
            " [0.8082931  0.29902682 0.69013655 0.28259072]\n",
            " [0.6899187  0.3188377  0.5563762  0.49993157]\n",
            " [0.8024654  0.27628943 0.58062375 0.36739048]\n",
            " [0.8027767  0.35509607 0.7217859  0.2763556 ]]\n",
            "6851 [D loss: 1.072506] [G loss: 1.180635]\n",
            "6852 [D loss: 1.089078] [G loss: 1.176008]\n",
            "6853 [D loss: 1.089298] [G loss: 1.168134]\n",
            "6854 [D loss: 1.085143] [G loss: 1.175940]\n",
            "6855 [D loss: 1.090166] [G loss: 1.174550]\n",
            "6856 [D loss: 1.118225] [G loss: 1.139353]\n",
            "6857 [D loss: 1.088114] [G loss: 1.176906]\n",
            "6858 [D loss: 1.098045] [G loss: 1.180005]\n",
            "6859 [D loss: 1.091589] [G loss: 1.164618]\n",
            "6860 [D loss: 1.099619] [G loss: 1.174500]\n",
            "6861 [D loss: 1.078605] [G loss: 1.177673]\n",
            "6862 [D loss: 1.095713] [G loss: 1.179258]\n",
            "6863 [D loss: 1.102112] [G loss: 1.175157]\n",
            "6864 [D loss: 1.100010] [G loss: 1.181431]\n",
            "6865 [D loss: 1.077684] [G loss: 1.189871]\n",
            "6866 [D loss: 1.085003] [G loss: 1.184627]\n",
            "6867 [D loss: 1.090555] [G loss: 1.182645]\n",
            "6868 [D loss: 1.099935] [G loss: 1.156401]\n",
            "6869 [D loss: 1.087545] [G loss: 1.162938]\n",
            "6870 [D loss: 1.075605] [G loss: 1.186816]\n",
            "6871 [D loss: 1.090885] [G loss: 1.186167]\n",
            "6872 [D loss: 1.091653] [G loss: 1.160674]\n",
            "6873 [D loss: 1.096766] [G loss: 1.183201]\n",
            "6874 [D loss: 1.082852] [G loss: 1.188285]\n",
            "6875 [D loss: 1.088894] [G loss: 1.174848]\n",
            "6876 [D loss: 1.089826] [G loss: 1.177975]\n",
            "6877 [D loss: 1.094905] [G loss: 1.165979]\n",
            "6878 [D loss: 1.088968] [G loss: 1.181815]\n",
            "6879 [D loss: 1.114967] [G loss: 1.163330]\n",
            "6880 [D loss: 1.079168] [G loss: 1.173203]\n",
            "6881 [D loss: 1.081664] [G loss: 1.197650]\n",
            "6882 [D loss: 1.081168] [G loss: 1.195296]\n",
            "6883 [D loss: 1.082452] [G loss: 1.173477]\n",
            "6884 [D loss: 1.078868] [G loss: 1.178485]\n",
            "6885 [D loss: 1.088120] [G loss: 1.181781]\n",
            "6886 [D loss: 1.083872] [G loss: 1.192426]\n",
            "6887 [D loss: 1.108278] [G loss: 1.146381]\n",
            "6888 [D loss: 1.081070] [G loss: 1.173086]\n",
            "6889 [D loss: 1.086294] [G loss: 1.166676]\n",
            "6890 [D loss: 1.085417] [G loss: 1.177376]\n",
            "6891 [D loss: 1.096202] [G loss: 1.157733]\n",
            "6892 [D loss: 1.084633] [G loss: 1.192604]\n",
            "6893 [D loss: 1.086547] [G loss: 1.172771]\n",
            "6894 [D loss: 1.091165] [G loss: 1.191879]\n",
            "6895 [D loss: 1.075347] [G loss: 1.197908]\n",
            "6896 [D loss: 1.073753] [G loss: 1.185380]\n",
            "6897 [D loss: 1.089686] [G loss: 1.165553]\n",
            "6898 [D loss: 1.080457] [G loss: 1.173113]\n",
            "6899 [D loss: 1.097363] [G loss: 1.179683]\n",
            "6900 [D loss: 1.086002] [G loss: 1.163935]\n",
            "[[0.6048406  0.44798586 0.57148343 0.39912212]\n",
            " [0.531047   0.48602957 0.4926728  0.4903495 ]\n",
            " [0.7429105  0.27318758 0.64193046 0.33859193]\n",
            " [0.54149556 0.45706    0.51353353 0.5086805 ]\n",
            " [0.50443536 0.49682888 0.5022755  0.5003829 ]]\n",
            "6901 [D loss: 1.078911] [G loss: 1.187809]\n",
            "6902 [D loss: 1.091923] [G loss: 1.173372]\n",
            "6903 [D loss: 1.084096] [G loss: 1.180260]\n",
            "6904 [D loss: 1.083910] [G loss: 1.177310]\n",
            "6905 [D loss: 1.092063] [G loss: 1.183042]\n",
            "6906 [D loss: 1.094128] [G loss: 1.177207]\n",
            "6907 [D loss: 1.092587] [G loss: 1.154949]\n",
            "6908 [D loss: 1.074777] [G loss: 1.172627]\n",
            "6909 [D loss: 1.087734] [G loss: 1.161194]\n",
            "6910 [D loss: 1.105310] [G loss: 1.167096]\n",
            "6911 [D loss: 1.081817] [G loss: 1.176868]\n",
            "6912 [D loss: 1.093131] [G loss: 1.181529]\n",
            "6913 [D loss: 1.076515] [G loss: 1.176637]\n",
            "6914 [D loss: 1.071586] [G loss: 1.203119]\n",
            "6915 [D loss: 1.092989] [G loss: 1.172952]\n",
            "6916 [D loss: 1.080160] [G loss: 1.181371]\n",
            "6917 [D loss: 1.077889] [G loss: 1.177878]\n",
            "6918 [D loss: 1.087176] [G loss: 1.188916]\n",
            "6919 [D loss: 1.107297] [G loss: 1.154937]\n",
            "6920 [D loss: 1.078483] [G loss: 1.163962]\n",
            "6921 [D loss: 1.082130] [G loss: 1.169323]\n",
            "6922 [D loss: 1.104382] [G loss: 1.170066]\n",
            "6923 [D loss: 1.080707] [G loss: 1.174543]\n",
            "6924 [D loss: 1.098478] [G loss: 1.166081]\n",
            "6925 [D loss: 1.095879] [G loss: 1.152333]\n",
            "6926 [D loss: 1.105234] [G loss: 1.176112]\n",
            "6927 [D loss: 1.082819] [G loss: 1.202940]\n",
            "6928 [D loss: 1.084605] [G loss: 1.172307]\n",
            "6929 [D loss: 1.066778] [G loss: 1.195761]\n",
            "6930 [D loss: 1.093391] [G loss: 1.172725]\n",
            "6931 [D loss: 1.102496] [G loss: 1.165109]\n",
            "6932 [D loss: 1.080089] [G loss: 1.194921]\n",
            "6933 [D loss: 1.109390] [G loss: 1.156090]\n",
            "6934 [D loss: 1.095769] [G loss: 1.175767]\n",
            "6935 [D loss: 1.087337] [G loss: 1.172368]\n",
            "6936 [D loss: 1.097584] [G loss: 1.165730]\n",
            "6937 [D loss: 1.079738] [G loss: 1.165375]\n",
            "6938 [D loss: 1.096263] [G loss: 1.157298]\n",
            "6939 [D loss: 1.114743] [G loss: 1.157084]\n",
            "6940 [D loss: 1.103358] [G loss: 1.160977]\n",
            "6941 [D loss: 1.107703] [G loss: 1.154966]\n",
            "6942 [D loss: 1.097171] [G loss: 1.177589]\n",
            "6943 [D loss: 1.103488] [G loss: 1.165346]\n",
            "6944 [D loss: 1.090693] [G loss: 1.182538]\n",
            "6945 [D loss: 1.110713] [G loss: 1.174634]\n",
            "6946 [D loss: 1.106371] [G loss: 1.176677]\n",
            "6947 [D loss: 1.090147] [G loss: 1.174543]\n",
            "6948 [D loss: 1.079550] [G loss: 1.156744]\n",
            "6949 [D loss: 1.106175] [G loss: 1.158613]\n",
            "6950 [D loss: 1.098149] [G loss: 1.163489]\n",
            "[[0.5219605  0.45219764 0.50085604 0.5261285 ]\n",
            " [0.50447583 0.49680147 0.5022892  0.5003685 ]\n",
            " [0.56832397 0.43302676 0.5299596  0.48212796]\n",
            " [0.55562377 0.4693353  0.4924115  0.4902008 ]\n",
            " [0.5187706  0.48852172 0.5089758  0.49213693]]\n",
            "6951 [D loss: 1.079613] [G loss: 1.174266]\n",
            "6952 [D loss: 1.108065] [G loss: 1.178083]\n",
            "6953 [D loss: 1.113976] [G loss: 1.158108]\n",
            "6954 [D loss: 1.092918] [G loss: 1.177020]\n",
            "6955 [D loss: 1.100866] [G loss: 1.162937]\n",
            "6956 [D loss: 1.089739] [G loss: 1.183660]\n",
            "6957 [D loss: 1.096901] [G loss: 1.168392]\n",
            "6958 [D loss: 1.090802] [G loss: 1.190493]\n",
            "6959 [D loss: 1.090716] [G loss: 1.181114]\n",
            "6960 [D loss: 1.097755] [G loss: 1.175453]\n",
            "6961 [D loss: 1.088902] [G loss: 1.171848]\n",
            "6962 [D loss: 1.082322] [G loss: 1.170057]\n",
            "6963 [D loss: 1.112628] [G loss: 1.149025]\n",
            "6964 [D loss: 1.103884] [G loss: 1.173934]\n",
            "6965 [D loss: 1.088872] [G loss: 1.170671]\n",
            "6966 [D loss: 1.087974] [G loss: 1.172641]\n",
            "6967 [D loss: 1.100890] [G loss: 1.169651]\n",
            "6968 [D loss: 1.085260] [G loss: 1.188922]\n",
            "6969 [D loss: 1.086666] [G loss: 1.188228]\n",
            "6970 [D loss: 1.102069] [G loss: 1.167271]\n",
            "6971 [D loss: 1.099964] [G loss: 1.172588]\n",
            "6972 [D loss: 1.098434] [G loss: 1.156198]\n",
            "6973 [D loss: 1.080439] [G loss: 1.166175]\n",
            "6974 [D loss: 1.101580] [G loss: 1.154134]\n",
            "6975 [D loss: 1.076906] [G loss: 1.173390]\n",
            "6976 [D loss: 1.098388] [G loss: 1.153558]\n",
            "6977 [D loss: 1.086298] [G loss: 1.166363]\n",
            "6978 [D loss: 1.093482] [G loss: 1.175482]\n",
            "6979 [D loss: 1.080396] [G loss: 1.173506]\n",
            "6980 [D loss: 1.095766] [G loss: 1.168572]\n",
            "6981 [D loss: 1.106459] [G loss: 1.157292]\n",
            "6982 [D loss: 1.066043] [G loss: 1.188460]\n",
            "6983 [D loss: 1.060006] [G loss: 1.190995]\n",
            "6984 [D loss: 1.084489] [G loss: 1.177678]\n",
            "6985 [D loss: 1.095273] [G loss: 1.185089]\n",
            "6986 [D loss: 1.061608] [G loss: 1.173379]\n",
            "6987 [D loss: 1.088806] [G loss: 1.180969]\n",
            "6988 [D loss: 1.096748] [G loss: 1.173997]\n",
            "6989 [D loss: 1.088054] [G loss: 1.173339]\n",
            "6990 [D loss: 1.082494] [G loss: 1.152788]\n",
            "6991 [D loss: 1.084990] [G loss: 1.169240]\n",
            "6992 [D loss: 1.099746] [G loss: 1.155085]\n",
            "6993 [D loss: 1.107013] [G loss: 1.160901]\n",
            "6994 [D loss: 1.075434] [G loss: 1.177136]\n",
            "6995 [D loss: 1.102097] [G loss: 1.163258]\n",
            "6996 [D loss: 1.106776] [G loss: 1.159718]\n",
            "6997 [D loss: 1.101090] [G loss: 1.177911]\n",
            "6998 [D loss: 1.103752] [G loss: 1.162740]\n",
            "6999 [D loss: 1.088564] [G loss: 1.174185]\n",
            "7000 [D loss: 1.082082] [G loss: 1.185136]\n",
            "[[0.77128994 0.342456   0.6929576  0.26441368]\n",
            " [0.50451624 0.49677488 0.502303   0.500353  ]\n",
            " [0.5673885  0.45416337 0.5363594  0.4580934 ]\n",
            " [0.8789144  0.25165576 0.73711777 0.2007174 ]\n",
            " [0.5185995  0.48376796 0.5184098  0.5093949 ]]\n",
            "7001 [D loss: 1.106368] [G loss: 1.169628]\n",
            "7002 [D loss: 1.088104] [G loss: 1.179475]\n",
            "7003 [D loss: 1.096218] [G loss: 1.163067]\n",
            "7004 [D loss: 1.083937] [G loss: 1.175421]\n",
            "7005 [D loss: 1.079261] [G loss: 1.198228]\n",
            "7006 [D loss: 1.088119] [G loss: 1.175342]\n",
            "7007 [D loss: 1.091387] [G loss: 1.160726]\n",
            "7008 [D loss: 1.099322] [G loss: 1.158690]\n",
            "7009 [D loss: 1.085515] [G loss: 1.191917]\n",
            "7010 [D loss: 1.091744] [G loss: 1.171300]\n",
            "7011 [D loss: 1.089902] [G loss: 1.194566]\n",
            "7012 [D loss: 1.105314] [G loss: 1.173031]\n",
            "7013 [D loss: 1.088373] [G loss: 1.163016]\n",
            "7014 [D loss: 1.074301] [G loss: 1.176858]\n",
            "7015 [D loss: 1.093687] [G loss: 1.168315]\n",
            "7016 [D loss: 1.096854] [G loss: 1.176065]\n",
            "7017 [D loss: 1.086685] [G loss: 1.191283]\n",
            "7018 [D loss: 1.090344] [G loss: 1.169845]\n",
            "7019 [D loss: 1.091707] [G loss: 1.169924]\n",
            "7020 [D loss: 1.084142] [G loss: 1.176889]\n",
            "7021 [D loss: 1.096700] [G loss: 1.164566]\n",
            "7022 [D loss: 1.106584] [G loss: 1.171194]\n",
            "7023 [D loss: 1.067789] [G loss: 1.159792]\n",
            "7024 [D loss: 1.071980] [G loss: 1.213797]\n",
            "7025 [D loss: 1.076757] [G loss: 1.176469]\n",
            "7026 [D loss: 1.071811] [G loss: 1.189117]\n",
            "7027 [D loss: 1.086816] [G loss: 1.177640]\n",
            "7028 [D loss: 1.088039] [G loss: 1.156486]\n",
            "7029 [D loss: 1.081098] [G loss: 1.178980]\n",
            "7030 [D loss: 1.089884] [G loss: 1.170788]\n",
            "7031 [D loss: 1.093371] [G loss: 1.168806]\n",
            "7032 [D loss: 1.103876] [G loss: 1.165477]\n",
            "7033 [D loss: 1.075903] [G loss: 1.184451]\n",
            "7034 [D loss: 1.090998] [G loss: 1.174794]\n",
            "7035 [D loss: 1.104904] [G loss: 1.164232]\n",
            "7036 [D loss: 1.090336] [G loss: 1.182367]\n",
            "7037 [D loss: 1.086096] [G loss: 1.191834]\n",
            "7038 [D loss: 1.087313] [G loss: 1.153805]\n",
            "7039 [D loss: 1.115738] [G loss: 1.136524]\n",
            "7040 [D loss: 1.085326] [G loss: 1.174408]\n",
            "7041 [D loss: 1.075311] [G loss: 1.181724]\n",
            "7042 [D loss: 1.096793] [G loss: 1.172925]\n",
            "7043 [D loss: 1.090685] [G loss: 1.173075]\n",
            "7044 [D loss: 1.101793] [G loss: 1.180480]\n",
            "7045 [D loss: 1.084024] [G loss: 1.180689]\n",
            "7046 [D loss: 1.091297] [G loss: 1.179558]\n",
            "7047 [D loss: 1.094804] [G loss: 1.164916]\n",
            "7048 [D loss: 1.093816] [G loss: 1.167179]\n",
            "7049 [D loss: 1.099775] [G loss: 1.163978]\n",
            "7050 [D loss: 1.103138] [G loss: 1.160871]\n",
            "[[0.7299184  0.379172   0.6739303  0.27588913]\n",
            " [0.5177789  0.50437915 0.50801694 0.48760498]\n",
            " [0.5107119  0.48015073 0.50992197 0.5227183 ]\n",
            " [0.7453483  0.27939177 0.59224445 0.4772858 ]\n",
            " [0.50935686 0.4930511  0.50183    0.5029798 ]]\n",
            "7051 [D loss: 1.086427] [G loss: 1.158354]\n",
            "7052 [D loss: 1.101620] [G loss: 1.166044]\n",
            "7053 [D loss: 1.083200] [G loss: 1.193660]\n",
            "7054 [D loss: 1.083671] [G loss: 1.173436]\n",
            "7055 [D loss: 1.090202] [G loss: 1.179605]\n",
            "7056 [D loss: 1.093816] [G loss: 1.174994]\n",
            "7057 [D loss: 1.097216] [G loss: 1.177942]\n",
            "7058 [D loss: 1.096150] [G loss: 1.159765]\n",
            "7059 [D loss: 1.096809] [G loss: 1.172534]\n",
            "7060 [D loss: 1.099324] [G loss: 1.162745]\n",
            "7061 [D loss: 1.080568] [G loss: 1.181128]\n",
            "7062 [D loss: 1.072387] [G loss: 1.185088]\n",
            "7063 [D loss: 1.096678] [G loss: 1.173315]\n",
            "7064 [D loss: 1.102343] [G loss: 1.173162]\n",
            "7065 [D loss: 1.090163] [G loss: 1.167266]\n",
            "7066 [D loss: 1.101463] [G loss: 1.172239]\n",
            "7067 [D loss: 1.082434] [G loss: 1.168316]\n",
            "7068 [D loss: 1.105214] [G loss: 1.162164]\n",
            "7069 [D loss: 1.079739] [G loss: 1.173390]\n",
            "7070 [D loss: 1.091269] [G loss: 1.155627]\n",
            "7071 [D loss: 1.076291] [G loss: 1.179055]\n",
            "7072 [D loss: 1.088161] [G loss: 1.166442]\n",
            "7073 [D loss: 1.108845] [G loss: 1.163603]\n",
            "7074 [D loss: 1.095886] [G loss: 1.182205]\n",
            "7075 [D loss: 1.107296] [G loss: 1.141839]\n",
            "7076 [D loss: 1.097030] [G loss: 1.163846]\n",
            "7077 [D loss: 1.082612] [G loss: 1.179282]\n",
            "7078 [D loss: 1.103381] [G loss: 1.160401]\n",
            "7079 [D loss: 1.079334] [G loss: 1.191871]\n",
            "7080 [D loss: 1.108000] [G loss: 1.169519]\n",
            "7081 [D loss: 1.091090] [G loss: 1.177340]\n",
            "7082 [D loss: 1.099616] [G loss: 1.165977]\n",
            "7083 [D loss: 1.075407] [G loss: 1.181018]\n",
            "7084 [D loss: 1.082484] [G loss: 1.181006]\n",
            "7085 [D loss: 1.095406] [G loss: 1.174342]\n",
            "7086 [D loss: 1.087025] [G loss: 1.178204]\n",
            "7087 [D loss: 1.098006] [G loss: 1.160666]\n",
            "7088 [D loss: 1.099999] [G loss: 1.170618]\n",
            "7089 [D loss: 1.098674] [G loss: 1.173652]\n",
            "7090 [D loss: 1.093232] [G loss: 1.164858]\n",
            "7091 [D loss: 1.094557] [G loss: 1.164175]\n",
            "7092 [D loss: 1.090037] [G loss: 1.164478]\n",
            "7093 [D loss: 1.075022] [G loss: 1.165535]\n",
            "7094 [D loss: 1.096428] [G loss: 1.191968]\n",
            "7095 [D loss: 1.098305] [G loss: 1.170422]\n",
            "7096 [D loss: 1.101984] [G loss: 1.167469]\n",
            "7097 [D loss: 1.098304] [G loss: 1.165011]\n",
            "7098 [D loss: 1.091724] [G loss: 1.169336]\n",
            "7099 [D loss: 1.097126] [G loss: 1.163939]\n",
            "7100 [D loss: 1.085662] [G loss: 1.162741]\n",
            "[[0.7365629  0.30145338 0.5590674  0.48021197]\n",
            " [0.60750544 0.4211399  0.5353549  0.49037147]\n",
            " [0.9468295  0.12706408 0.7689171  0.20249672]\n",
            " [0.5789145  0.4127337  0.5231209  0.524228  ]\n",
            " [0.5423507  0.45668855 0.5176202  0.49125928]]\n",
            "7101 [D loss: 1.076218] [G loss: 1.191610]\n",
            "7102 [D loss: 1.084998] [G loss: 1.175706]\n",
            "7103 [D loss: 1.086788] [G loss: 1.171508]\n",
            "7104 [D loss: 1.097547] [G loss: 1.150404]\n",
            "7105 [D loss: 1.089188] [G loss: 1.186636]\n",
            "7106 [D loss: 1.098988] [G loss: 1.167778]\n",
            "7107 [D loss: 1.098205] [G loss: 1.162449]\n",
            "7108 [D loss: 1.097091] [G loss: 1.155606]\n",
            "7109 [D loss: 1.087637] [G loss: 1.166853]\n",
            "7110 [D loss: 1.088808] [G loss: 1.165817]\n",
            "7111 [D loss: 1.099207] [G loss: 1.167568]\n",
            "7112 [D loss: 1.083035] [G loss: 1.167397]\n",
            "7113 [D loss: 1.095772] [G loss: 1.172363]\n",
            "7114 [D loss: 1.102671] [G loss: 1.158726]\n",
            "7115 [D loss: 1.098033] [G loss: 1.171320]\n",
            "7116 [D loss: 1.081577] [G loss: 1.172000]\n",
            "7117 [D loss: 1.092214] [G loss: 1.169909]\n",
            "7118 [D loss: 1.108946] [G loss: 1.156153]\n",
            "7119 [D loss: 1.092677] [G loss: 1.157134]\n",
            "7120 [D loss: 1.083753] [G loss: 1.165917]\n",
            "7121 [D loss: 1.067832] [G loss: 1.175473]\n",
            "7122 [D loss: 1.098500] [G loss: 1.165100]\n",
            "7123 [D loss: 1.075301] [G loss: 1.163393]\n",
            "7124 [D loss: 1.074490] [G loss: 1.183227]\n",
            "7125 [D loss: 1.086092] [G loss: 1.172514]\n",
            "7126 [D loss: 1.097017] [G loss: 1.167633]\n",
            "7127 [D loss: 1.088039] [G loss: 1.181123]\n",
            "7128 [D loss: 1.099295] [G loss: 1.170299]\n",
            "7129 [D loss: 1.080271] [G loss: 1.173915]\n",
            "7130 [D loss: 1.102706] [G loss: 1.151555]\n",
            "7131 [D loss: 1.107692] [G loss: 1.179304]\n",
            "7132 [D loss: 1.091992] [G loss: 1.149008]\n",
            "7133 [D loss: 1.088598] [G loss: 1.165295]\n",
            "7134 [D loss: 1.078599] [G loss: 1.170722]\n",
            "7135 [D loss: 1.092604] [G loss: 1.178089]\n",
            "7136 [D loss: 1.096447] [G loss: 1.164407]\n",
            "7137 [D loss: 1.101117] [G loss: 1.159017]\n",
            "7138 [D loss: 1.095014] [G loss: 1.168512]\n",
            "7139 [D loss: 1.092834] [G loss: 1.168350]\n",
            "7140 [D loss: 1.102770] [G loss: 1.160280]\n",
            "7141 [D loss: 1.095325] [G loss: 1.166309]\n",
            "7142 [D loss: 1.093780] [G loss: 1.167763]\n",
            "7143 [D loss: 1.096832] [G loss: 1.159116]\n",
            "7144 [D loss: 1.092155] [G loss: 1.173094]\n",
            "7145 [D loss: 1.082861] [G loss: 1.156405]\n",
            "7146 [D loss: 1.082926] [G loss: 1.166569]\n",
            "7147 [D loss: 1.093657] [G loss: 1.160211]\n",
            "7148 [D loss: 1.104104] [G loss: 1.164392]\n",
            "7149 [D loss: 1.096936] [G loss: 1.163981]\n",
            "7150 [D loss: 1.086380] [G loss: 1.166240]\n",
            "[[0.74963224 0.40486014 0.70180476 0.30945837]\n",
            " [0.54301286 0.41524154 0.5019106  0.5636831 ]\n",
            " [0.50463766 0.4966944  0.5023439  0.5003076 ]\n",
            " [0.93307567 0.16681902 0.7996239  0.17094669]\n",
            " [0.50463766 0.4966944  0.5023439  0.5003076 ]]\n",
            "7151 [D loss: 1.104013] [G loss: 1.151884]\n",
            "7152 [D loss: 1.074989] [G loss: 1.156818]\n",
            "7153 [D loss: 1.099284] [G loss: 1.176576]\n",
            "7154 [D loss: 1.109308] [G loss: 1.154426]\n",
            "7155 [D loss: 1.083484] [G loss: 1.169257]\n",
            "7156 [D loss: 1.096796] [G loss: 1.148465]\n",
            "7157 [D loss: 1.091379] [G loss: 1.168666]\n",
            "7158 [D loss: 1.089091] [G loss: 1.166118]\n",
            "7159 [D loss: 1.097825] [G loss: 1.147964]\n",
            "7160 [D loss: 1.100272] [G loss: 1.139254]\n",
            "7161 [D loss: 1.096600] [G loss: 1.179976]\n",
            "7162 [D loss: 1.095605] [G loss: 1.165264]\n",
            "7163 [D loss: 1.081704] [G loss: 1.181218]\n",
            "7164 [D loss: 1.080177] [G loss: 1.163589]\n",
            "7165 [D loss: 1.092759] [G loss: 1.177986]\n",
            "7166 [D loss: 1.094865] [G loss: 1.160872]\n",
            "7167 [D loss: 1.080653] [G loss: 1.187017]\n",
            "7168 [D loss: 1.089195] [G loss: 1.177624]\n",
            "7169 [D loss: 1.105005] [G loss: 1.150392]\n",
            "7170 [D loss: 1.084770] [G loss: 1.168106]\n",
            "7171 [D loss: 1.090448] [G loss: 1.193818]\n",
            "7172 [D loss: 1.085790] [G loss: 1.176271]\n",
            "7173 [D loss: 1.094048] [G loss: 1.163288]\n",
            "7174 [D loss: 1.091879] [G loss: 1.164159]\n",
            "7175 [D loss: 1.101566] [G loss: 1.177895]\n",
            "7176 [D loss: 1.102373] [G loss: 1.165624]\n",
            "7177 [D loss: 1.087408] [G loss: 1.165150]\n",
            "7178 [D loss: 1.090291] [G loss: 1.148194]\n",
            "7179 [D loss: 1.092971] [G loss: 1.174603]\n",
            "7180 [D loss: 1.092576] [G loss: 1.170616]\n",
            "7181 [D loss: 1.096414] [G loss: 1.168068]\n",
            "7182 [D loss: 1.097674] [G loss: 1.168188]\n",
            "7183 [D loss: 1.087592] [G loss: 1.173804]\n",
            "7184 [D loss: 1.103132] [G loss: 1.172216]\n",
            "7185 [D loss: 1.105111] [G loss: 1.149166]\n",
            "7186 [D loss: 1.115693] [G loss: 1.154897]\n",
            "7187 [D loss: 1.094493] [G loss: 1.154027]\n",
            "7188 [D loss: 1.092143] [G loss: 1.170752]\n",
            "7189 [D loss: 1.085022] [G loss: 1.168201]\n",
            "7190 [D loss: 1.100619] [G loss: 1.162295]\n",
            "7191 [D loss: 1.083931] [G loss: 1.189715]\n",
            "7192 [D loss: 1.096227] [G loss: 1.161175]\n",
            "7193 [D loss: 1.106945] [G loss: 1.149434]\n",
            "7194 [D loss: 1.111424] [G loss: 1.141198]\n",
            "7195 [D loss: 1.103456] [G loss: 1.166732]\n",
            "7196 [D loss: 1.090289] [G loss: 1.171247]\n",
            "7197 [D loss: 1.088321] [G loss: 1.164556]\n",
            "7198 [D loss: 1.101738] [G loss: 1.177336]\n",
            "7199 [D loss: 1.105527] [G loss: 1.165933]\n",
            "7200 [D loss: 1.077526] [G loss: 1.180278]\n",
            "[[0.67188704 0.41594157 0.62467974 0.40992352]\n",
            " [0.7740371  0.34477785 0.65846246 0.42486286]\n",
            " [0.50467813 0.4966672  0.50235754 0.5002928 ]\n",
            " [0.68054277 0.34622225 0.5557196  0.45773825]\n",
            " [0.50994164 0.49512675 0.50357705 0.4966461 ]]\n",
            "7201 [D loss: 1.094375] [G loss: 1.173455]\n",
            "7202 [D loss: 1.107211] [G loss: 1.154690]\n",
            "7203 [D loss: 1.101011] [G loss: 1.156569]\n",
            "7204 [D loss: 1.088455] [G loss: 1.142578]\n",
            "7205 [D loss: 1.090809] [G loss: 1.150713]\n",
            "7206 [D loss: 1.094477] [G loss: 1.178094]\n",
            "7207 [D loss: 1.087499] [G loss: 1.165896]\n",
            "7208 [D loss: 1.107746] [G loss: 1.161048]\n",
            "7209 [D loss: 1.094390] [G loss: 1.167917]\n",
            "7210 [D loss: 1.083520] [G loss: 1.185671]\n",
            "7211 [D loss: 1.083750] [G loss: 1.179089]\n",
            "7212 [D loss: 1.093878] [G loss: 1.171616]\n",
            "7213 [D loss: 1.092287] [G loss: 1.168935]\n",
            "7214 [D loss: 1.092677] [G loss: 1.172724]\n",
            "7215 [D loss: 1.108274] [G loss: 1.157036]\n",
            "7216 [D loss: 1.096729] [G loss: 1.155617]\n",
            "7217 [D loss: 1.073567] [G loss: 1.188475]\n",
            "7218 [D loss: 1.093962] [G loss: 1.165487]\n",
            "7219 [D loss: 1.089950] [G loss: 1.160731]\n",
            "7220 [D loss: 1.085789] [G loss: 1.160031]\n",
            "7221 [D loss: 1.085067] [G loss: 1.159649]\n",
            "7222 [D loss: 1.108411] [G loss: 1.155032]\n",
            "7223 [D loss: 1.104656] [G loss: 1.157551]\n",
            "7224 [D loss: 1.085646] [G loss: 1.173778]\n",
            "7225 [D loss: 1.105947] [G loss: 1.158051]\n",
            "7226 [D loss: 1.091326] [G loss: 1.167058]\n",
            "7227 [D loss: 1.099411] [G loss: 1.165327]\n",
            "7228 [D loss: 1.112967] [G loss: 1.136754]\n",
            "7229 [D loss: 1.101196] [G loss: 1.157292]\n",
            "7230 [D loss: 1.115646] [G loss: 1.141918]\n",
            "7231 [D loss: 1.118780] [G loss: 1.148324]\n",
            "7232 [D loss: 1.079246] [G loss: 1.174773]\n",
            "7233 [D loss: 1.084504] [G loss: 1.181209]\n",
            "7234 [D loss: 1.095203] [G loss: 1.154664]\n",
            "7235 [D loss: 1.098010] [G loss: 1.163046]\n",
            "7236 [D loss: 1.105467] [G loss: 1.167368]\n",
            "7237 [D loss: 1.092788] [G loss: 1.183387]\n",
            "7238 [D loss: 1.093365] [G loss: 1.165127]\n",
            "7239 [D loss: 1.112073] [G loss: 1.158770]\n",
            "7240 [D loss: 1.101405] [G loss: 1.166168]\n",
            "7241 [D loss: 1.088855] [G loss: 1.176874]\n",
            "7242 [D loss: 1.076135] [G loss: 1.183440]\n",
            "7243 [D loss: 1.087965] [G loss: 1.180745]\n",
            "7244 [D loss: 1.098827] [G loss: 1.148313]\n",
            "7245 [D loss: 1.101839] [G loss: 1.157481]\n",
            "7246 [D loss: 1.094440] [G loss: 1.163149]\n",
            "7247 [D loss: 1.093313] [G loss: 1.152528]\n",
            "7248 [D loss: 1.100749] [G loss: 1.171777]\n",
            "7249 [D loss: 1.092816] [G loss: 1.153136]\n",
            "7250 [D loss: 1.108484] [G loss: 1.152333]\n",
            "[[0.5459489  0.44242844 0.4984097  0.5357675 ]\n",
            " [0.508276   0.49515828 0.5044523  0.49762017]\n",
            " [0.50471866 0.49664062 0.5023713  0.5002773 ]\n",
            " [0.52803624 0.46720928 0.5180474  0.52035326]\n",
            " [0.8773158  0.2926272  0.78919655 0.14488718]]\n",
            "7251 [D loss: 1.095591] [G loss: 1.157385]\n",
            "7252 [D loss: 1.108360] [G loss: 1.164497]\n",
            "7253 [D loss: 1.094789] [G loss: 1.160623]\n",
            "7254 [D loss: 1.085098] [G loss: 1.182286]\n",
            "7255 [D loss: 1.095947] [G loss: 1.167103]\n",
            "7256 [D loss: 1.102011] [G loss: 1.158268]\n",
            "7257 [D loss: 1.086138] [G loss: 1.168079]\n",
            "7258 [D loss: 1.088162] [G loss: 1.174341]\n",
            "7259 [D loss: 1.106111] [G loss: 1.165103]\n",
            "7260 [D loss: 1.089240] [G loss: 1.178119]\n",
            "7261 [D loss: 1.096409] [G loss: 1.179907]\n",
            "7262 [D loss: 1.092672] [G loss: 1.183959]\n",
            "7263 [D loss: 1.098064] [G loss: 1.155783]\n",
            "7264 [D loss: 1.094931] [G loss: 1.167866]\n",
            "7265 [D loss: 1.091219] [G loss: 1.164192]\n",
            "7266 [D loss: 1.094871] [G loss: 1.175975]\n",
            "7267 [D loss: 1.083970] [G loss: 1.173322]\n",
            "7268 [D loss: 1.093591] [G loss: 1.172849]\n",
            "7269 [D loss: 1.094994] [G loss: 1.163132]\n",
            "7270 [D loss: 1.098194] [G loss: 1.150096]\n",
            "7271 [D loss: 1.090426] [G loss: 1.172857]\n",
            "7272 [D loss: 1.085838] [G loss: 1.158801]\n",
            "7273 [D loss: 1.086923] [G loss: 1.164204]\n",
            "7274 [D loss: 1.100170] [G loss: 1.154403]\n",
            "7275 [D loss: 1.110072] [G loss: 1.150370]\n",
            "7276 [D loss: 1.085709] [G loss: 1.169674]\n",
            "7277 [D loss: 1.101222] [G loss: 1.163466]\n",
            "7278 [D loss: 1.097155] [G loss: 1.159883]\n",
            "7279 [D loss: 1.084037] [G loss: 1.181830]\n",
            "7280 [D loss: 1.097355] [G loss: 1.160009]\n",
            "7281 [D loss: 1.105389] [G loss: 1.169943]\n",
            "7282 [D loss: 1.122159] [G loss: 1.144267]\n",
            "7283 [D loss: 1.100822] [G loss: 1.172879]\n",
            "7284 [D loss: 1.104476] [G loss: 1.149557]\n",
            "7285 [D loss: 1.106894] [G loss: 1.156448]\n",
            "7286 [D loss: 1.098763] [G loss: 1.174196]\n",
            "7287 [D loss: 1.101949] [G loss: 1.148627]\n",
            "7288 [D loss: 1.093745] [G loss: 1.169575]\n",
            "7289 [D loss: 1.082648] [G loss: 1.167962]\n",
            "7290 [D loss: 1.107158] [G loss: 1.164441]\n",
            "7291 [D loss: 1.102985] [G loss: 1.153567]\n",
            "7292 [D loss: 1.083405] [G loss: 1.174824]\n",
            "7293 [D loss: 1.090777] [G loss: 1.155808]\n",
            "7294 [D loss: 1.079730] [G loss: 1.154080]\n",
            "7295 [D loss: 1.087712] [G loss: 1.169582]\n",
            "7296 [D loss: 1.078629] [G loss: 1.176603]\n",
            "7297 [D loss: 1.095094] [G loss: 1.166910]\n",
            "7298 [D loss: 1.095227] [G loss: 1.158977]\n",
            "7299 [D loss: 1.116874] [G loss: 1.162292]\n",
            "7300 [D loss: 1.094120] [G loss: 1.147890]\n",
            "[[0.55097896 0.42977193 0.52846557 0.5475579 ]\n",
            " [0.51442647 0.49148068 0.5054817  0.49562225]\n",
            " [0.5742648  0.3773012  0.52655846 0.55871916]\n",
            " [0.7376425  0.22526069 0.5334173  0.5376872 ]\n",
            " [0.90783435 0.20697665 0.7859017  0.16529107]]\n",
            "7301 [D loss: 1.090849] [G loss: 1.170542]\n",
            "7302 [D loss: 1.105904] [G loss: 1.177972]\n",
            "7303 [D loss: 1.100717] [G loss: 1.168677]\n",
            "7304 [D loss: 1.081337] [G loss: 1.178404]\n",
            "7305 [D loss: 1.102141] [G loss: 1.159859]\n",
            "7306 [D loss: 1.116506] [G loss: 1.137626]\n",
            "7307 [D loss: 1.096762] [G loss: 1.165018]\n",
            "7308 [D loss: 1.094875] [G loss: 1.169954]\n",
            "7309 [D loss: 1.091395] [G loss: 1.160625]\n",
            "7310 [D loss: 1.080700] [G loss: 1.167604]\n",
            "7311 [D loss: 1.086772] [G loss: 1.175412]\n",
            "7312 [D loss: 1.091275] [G loss: 1.169444]\n",
            "7313 [D loss: 1.093206] [G loss: 1.162192]\n",
            "7314 [D loss: 1.088606] [G loss: 1.180938]\n",
            "7315 [D loss: 1.100232] [G loss: 1.157598]\n",
            "7316 [D loss: 1.091570] [G loss: 1.167490]\n",
            "7317 [D loss: 1.111610] [G loss: 1.158346]\n",
            "7318 [D loss: 1.096836] [G loss: 1.162413]\n",
            "7319 [D loss: 1.088258] [G loss: 1.173977]\n",
            "7320 [D loss: 1.105052] [G loss: 1.157656]\n",
            "7321 [D loss: 1.110058] [G loss: 1.142719]\n",
            "7322 [D loss: 1.097979] [G loss: 1.174963]\n",
            "7323 [D loss: 1.077071] [G loss: 1.185731]\n",
            "7324 [D loss: 1.100532] [G loss: 1.169678]\n",
            "7325 [D loss: 1.087232] [G loss: 1.166909]\n",
            "7326 [D loss: 1.100001] [G loss: 1.156641]\n",
            "7327 [D loss: 1.099681] [G loss: 1.161596]\n",
            "7328 [D loss: 1.097715] [G loss: 1.147052]\n",
            "7329 [D loss: 1.093462] [G loss: 1.162588]\n",
            "7330 [D loss: 1.113055] [G loss: 1.142530]\n",
            "7331 [D loss: 1.089030] [G loss: 1.177453]\n",
            "7332 [D loss: 1.104153] [G loss: 1.145130]\n",
            "7333 [D loss: 1.081376] [G loss: 1.186853]\n",
            "7334 [D loss: 1.113660] [G loss: 1.143632]\n",
            "7335 [D loss: 1.089920] [G loss: 1.142937]\n",
            "7336 [D loss: 1.113971] [G loss: 1.150962]\n",
            "7337 [D loss: 1.103703] [G loss: 1.164269]\n",
            "7338 [D loss: 1.106886] [G loss: 1.152732]\n",
            "7339 [D loss: 1.091095] [G loss: 1.162390]\n",
            "7340 [D loss: 1.085364] [G loss: 1.173162]\n",
            "7341 [D loss: 1.104601] [G loss: 1.157235]\n",
            "7342 [D loss: 1.083049] [G loss: 1.171890]\n",
            "7343 [D loss: 1.083407] [G loss: 1.148725]\n",
            "7344 [D loss: 1.094991] [G loss: 1.160108]\n",
            "7345 [D loss: 1.093495] [G loss: 1.167263]\n",
            "7346 [D loss: 1.100566] [G loss: 1.171645]\n",
            "7347 [D loss: 1.089984] [G loss: 1.178178]\n",
            "7348 [D loss: 1.086684] [G loss: 1.164823]\n",
            "7349 [D loss: 1.095648] [G loss: 1.152886]\n",
            "7350 [D loss: 1.107327] [G loss: 1.160615]\n",
            "[[0.76860434 0.26684514 0.63001007 0.3775118 ]\n",
            " [0.57469237 0.39367065 0.61541814 0.5450529 ]\n",
            " [0.7199041  0.3867704  0.60032344 0.32564408]\n",
            " [0.70244795 0.2680369  0.5445599  0.6221236 ]\n",
            " [0.5915811  0.3634871  0.569041   0.5876768 ]]\n",
            "7351 [D loss: 1.090771] [G loss: 1.158830]\n",
            "7352 [D loss: 1.101448] [G loss: 1.151794]\n",
            "7353 [D loss: 1.094604] [G loss: 1.162351]\n",
            "7354 [D loss: 1.093112] [G loss: 1.143557]\n",
            "7355 [D loss: 1.088042] [G loss: 1.180172]\n",
            "7356 [D loss: 1.103184] [G loss: 1.153137]\n",
            "7357 [D loss: 1.083120] [G loss: 1.163630]\n",
            "7358 [D loss: 1.108082] [G loss: 1.146874]\n",
            "7359 [D loss: 1.091709] [G loss: 1.179187]\n",
            "7360 [D loss: 1.085721] [G loss: 1.150853]\n",
            "7361 [D loss: 1.101158] [G loss: 1.165939]\n",
            "7362 [D loss: 1.086072] [G loss: 1.188169]\n",
            "7363 [D loss: 1.099844] [G loss: 1.175026]\n",
            "7364 [D loss: 1.089515] [G loss: 1.142034]\n",
            "7365 [D loss: 1.095326] [G loss: 1.168630]\n",
            "7366 [D loss: 1.097907] [G loss: 1.151620]\n",
            "7367 [D loss: 1.087518] [G loss: 1.172765]\n",
            "7368 [D loss: 1.089658] [G loss: 1.148399]\n",
            "7369 [D loss: 1.087032] [G loss: 1.157474]\n",
            "7370 [D loss: 1.111953] [G loss: 1.143861]\n",
            "7371 [D loss: 1.099053] [G loss: 1.165960]\n",
            "7372 [D loss: 1.099037] [G loss: 1.143486]\n",
            "7373 [D loss: 1.092528] [G loss: 1.161378]\n",
            "7374 [D loss: 1.094941] [G loss: 1.139641]\n",
            "7375 [D loss: 1.099522] [G loss: 1.168406]\n",
            "7376 [D loss: 1.088965] [G loss: 1.151957]\n",
            "7377 [D loss: 1.098378] [G loss: 1.146808]\n",
            "7378 [D loss: 1.097990] [G loss: 1.173848]\n",
            "7379 [D loss: 1.092052] [G loss: 1.185095]\n",
            "7380 [D loss: 1.095770] [G loss: 1.149803]\n",
            "7381 [D loss: 1.093928] [G loss: 1.154841]\n",
            "7382 [D loss: 1.083196] [G loss: 1.170075]\n",
            "7383 [D loss: 1.088450] [G loss: 1.135209]\n",
            "7384 [D loss: 1.092953] [G loss: 1.174611]\n",
            "7385 [D loss: 1.089356] [G loss: 1.167267]\n",
            "7386 [D loss: 1.108654] [G loss: 1.157897]\n",
            "7387 [D loss: 1.084702] [G loss: 1.171076]\n",
            "7388 [D loss: 1.101414] [G loss: 1.160971]\n",
            "7389 [D loss: 1.111035] [G loss: 1.162977]\n",
            "7390 [D loss: 1.092276] [G loss: 1.149565]\n",
            "7391 [D loss: 1.084396] [G loss: 1.166962]\n",
            "7392 [D loss: 1.100956] [G loss: 1.162640]\n",
            "7393 [D loss: 1.095097] [G loss: 1.164390]\n",
            "7394 [D loss: 1.099490] [G loss: 1.158438]\n",
            "7395 [D loss: 1.098204] [G loss: 1.166706]\n",
            "7396 [D loss: 1.108358] [G loss: 1.137048]\n",
            "7397 [D loss: 1.090877] [G loss: 1.178532]\n",
            "7398 [D loss: 1.092958] [G loss: 1.164605]\n",
            "7399 [D loss: 1.106889] [G loss: 1.141817]\n",
            "7400 [D loss: 1.100326] [G loss: 1.160766]\n",
            "[[0.7597947  0.37899998 0.68800724 0.23964335]\n",
            " [0.6719881  0.37102205 0.58788836 0.48033386]\n",
            " [0.77674866 0.24751271 0.6147705  0.41957116]\n",
            " [0.519783   0.4835574  0.51607066 0.50706244]\n",
            " [0.5053702  0.49699032 0.50296587 0.49935865]]\n",
            "7401 [D loss: 1.096053] [G loss: 1.161968]\n",
            "7402 [D loss: 1.100498] [G loss: 1.155100]\n",
            "7403 [D loss: 1.101449] [G loss: 1.162974]\n",
            "7404 [D loss: 1.095998] [G loss: 1.166488]\n",
            "7405 [D loss: 1.095077] [G loss: 1.156802]\n",
            "7406 [D loss: 1.101256] [G loss: 1.149526]\n",
            "7407 [D loss: 1.101175] [G loss: 1.163457]\n",
            "7408 [D loss: 1.103251] [G loss: 1.155418]\n",
            "7409 [D loss: 1.094787] [G loss: 1.157250]\n",
            "7410 [D loss: 1.089067] [G loss: 1.184683]\n",
            "7411 [D loss: 1.098266] [G loss: 1.164731]\n",
            "7412 [D loss: 1.092860] [G loss: 1.147514]\n",
            "7413 [D loss: 1.101453] [G loss: 1.153024]\n",
            "7414 [D loss: 1.066599] [G loss: 1.185145]\n",
            "7415 [D loss: 1.081339] [G loss: 1.175013]\n",
            "7416 [D loss: 1.100096] [G loss: 1.163979]\n",
            "7417 [D loss: 1.104149] [G loss: 1.154326]\n",
            "7418 [D loss: 1.092407] [G loss: 1.169281]\n",
            "7419 [D loss: 1.072055] [G loss: 1.193260]\n",
            "7420 [D loss: 1.087649] [G loss: 1.169320]\n",
            "7421 [D loss: 1.099119] [G loss: 1.147061]\n",
            "7422 [D loss: 1.117459] [G loss: 1.162050]\n",
            "7423 [D loss: 1.097605] [G loss: 1.156276]\n",
            "7424 [D loss: 1.079399] [G loss: 1.158888]\n",
            "7425 [D loss: 1.103878] [G loss: 1.162339]\n",
            "7426 [D loss: 1.095746] [G loss: 1.154827]\n",
            "7427 [D loss: 1.101493] [G loss: 1.154043]\n",
            "7428 [D loss: 1.105071] [G loss: 1.143082]\n",
            "7429 [D loss: 1.098682] [G loss: 1.162211]\n",
            "7430 [D loss: 1.120822] [G loss: 1.142822]\n",
            "7431 [D loss: 1.096185] [G loss: 1.167947]\n",
            "7432 [D loss: 1.090851] [G loss: 1.151313]\n",
            "7433 [D loss: 1.108606] [G loss: 1.155833]\n",
            "7434 [D loss: 1.098321] [G loss: 1.145111]\n",
            "7435 [D loss: 1.086689] [G loss: 1.177998]\n",
            "7436 [D loss: 1.109015] [G loss: 1.166929]\n",
            "7437 [D loss: 1.090895] [G loss: 1.167425]\n",
            "7438 [D loss: 1.096019] [G loss: 1.182866]\n",
            "7439 [D loss: 1.090975] [G loss: 1.164101]\n",
            "7440 [D loss: 1.097736] [G loss: 1.147459]\n",
            "7441 [D loss: 1.081676] [G loss: 1.190256]\n",
            "7442 [D loss: 1.107901] [G loss: 1.153903]\n",
            "7443 [D loss: 1.105393] [G loss: 1.141055]\n",
            "7444 [D loss: 1.089428] [G loss: 1.158469]\n",
            "7445 [D loss: 1.107756] [G loss: 1.150454]\n",
            "7446 [D loss: 1.080265] [G loss: 1.177632]\n",
            "7447 [D loss: 1.099178] [G loss: 1.173626]\n",
            "7448 [D loss: 1.102016] [G loss: 1.156877]\n",
            "7449 [D loss: 1.107106] [G loss: 1.151836]\n",
            "7450 [D loss: 1.101834] [G loss: 1.145069]\n",
            "[[0.5136307  0.46092266 0.53464603 0.5291792 ]\n",
            " [0.68494207 0.4188462  0.63379043 0.33568513]\n",
            " [0.58453727 0.39863628 0.55026823 0.46363914]\n",
            " [0.51239914 0.48280293 0.50674033 0.49957633]\n",
            " [0.50488067 0.49653387 0.5024265  0.5002154 ]]\n",
            "7451 [D loss: 1.094431] [G loss: 1.165371]\n",
            "7452 [D loss: 1.102991] [G loss: 1.169933]\n",
            "7453 [D loss: 1.112634] [G loss: 1.126369]\n",
            "7454 [D loss: 1.077130] [G loss: 1.181663]\n",
            "7455 [D loss: 1.085848] [G loss: 1.171784]\n",
            "7456 [D loss: 1.091176] [G loss: 1.156399]\n",
            "7457 [D loss: 1.094639] [G loss: 1.157285]\n",
            "7458 [D loss: 1.104624] [G loss: 1.151866]\n",
            "7459 [D loss: 1.103816] [G loss: 1.162675]\n",
            "7460 [D loss: 1.102406] [G loss: 1.147095]\n",
            "7461 [D loss: 1.092383] [G loss: 1.155620]\n",
            "7462 [D loss: 1.084595] [G loss: 1.181977]\n",
            "7463 [D loss: 1.095790] [G loss: 1.147663]\n",
            "7464 [D loss: 1.088792] [G loss: 1.164537]\n",
            "7465 [D loss: 1.106296] [G loss: 1.144094]\n",
            "7466 [D loss: 1.109344] [G loss: 1.141980]\n",
            "7467 [D loss: 1.104230] [G loss: 1.161628]\n",
            "7468 [D loss: 1.094034] [G loss: 1.157450]\n",
            "7469 [D loss: 1.096403] [G loss: 1.172445]\n",
            "7470 [D loss: 1.105013] [G loss: 1.149969]\n",
            "7471 [D loss: 1.091385] [G loss: 1.170297]\n",
            "7472 [D loss: 1.087071] [G loss: 1.170537]\n",
            "7473 [D loss: 1.125904] [G loss: 1.128664]\n",
            "7474 [D loss: 1.115233] [G loss: 1.135806]\n",
            "7475 [D loss: 1.099113] [G loss: 1.174050]\n",
            "7476 [D loss: 1.089881] [G loss: 1.164571]\n",
            "7477 [D loss: 1.111595] [G loss: 1.137940]\n",
            "7478 [D loss: 1.086180] [G loss: 1.177477]\n",
            "7479 [D loss: 1.078035] [G loss: 1.179090]\n",
            "7480 [D loss: 1.103694] [G loss: 1.150154]\n",
            "7481 [D loss: 1.090003] [G loss: 1.170333]\n",
            "7482 [D loss: 1.111144] [G loss: 1.135690]\n",
            "7483 [D loss: 1.101146] [G loss: 1.156590]\n",
            "7484 [D loss: 1.093301] [G loss: 1.147523]\n",
            "7485 [D loss: 1.088229] [G loss: 1.173128]\n",
            "7486 [D loss: 1.107690] [G loss: 1.145296]\n",
            "7487 [D loss: 1.100521] [G loss: 1.161122]\n",
            "7488 [D loss: 1.101017] [G loss: 1.169325]\n",
            "7489 [D loss: 1.111759] [G loss: 1.137606]\n",
            "7490 [D loss: 1.093069] [G loss: 1.171506]\n",
            "7491 [D loss: 1.105830] [G loss: 1.154300]\n",
            "7492 [D loss: 1.114428] [G loss: 1.135072]\n",
            "7493 [D loss: 1.087946] [G loss: 1.167519]\n",
            "7494 [D loss: 1.082205] [G loss: 1.151869]\n",
            "7495 [D loss: 1.083944] [G loss: 1.150172]\n",
            "7496 [D loss: 1.102748] [G loss: 1.154829]\n",
            "7497 [D loss: 1.099501] [G loss: 1.166130]\n",
            "7498 [D loss: 1.103329] [G loss: 1.170938]\n",
            "7499 [D loss: 1.094794] [G loss: 1.162271]\n",
            "7500 [D loss: 1.117549] [G loss: 1.151523]\n",
            "[[0.54891413 0.40698275 0.57042396 0.5120377 ]\n",
            " [0.74856186 0.35953563 0.6953115  0.23728168]\n",
            " [0.7830845  0.33307844 0.7073143  0.2535654 ]\n",
            " [0.81155425 0.29559997 0.74919665 0.21328063]\n",
            " [0.5049209  0.49650714 0.50244045 0.50020003]]\n",
            "7501 [D loss: 1.095529] [G loss: 1.154680]\n",
            "7502 [D loss: 1.113804] [G loss: 1.137577]\n",
            "7503 [D loss: 1.082466] [G loss: 1.174577]\n",
            "7504 [D loss: 1.103102] [G loss: 1.172551]\n",
            "7505 [D loss: 1.101347] [G loss: 1.155508]\n",
            "7506 [D loss: 1.090177] [G loss: 1.160286]\n",
            "7507 [D loss: 1.102635] [G loss: 1.153355]\n",
            "7508 [D loss: 1.099889] [G loss: 1.157328]\n",
            "7509 [D loss: 1.096181] [G loss: 1.159250]\n",
            "7510 [D loss: 1.092314] [G loss: 1.167421]\n",
            "7511 [D loss: 1.100884] [G loss: 1.151077]\n",
            "7512 [D loss: 1.101755] [G loss: 1.166806]\n",
            "7513 [D loss: 1.083607] [G loss: 1.168839]\n",
            "7514 [D loss: 1.094652] [G loss: 1.163100]\n",
            "7515 [D loss: 1.099918] [G loss: 1.163330]\n",
            "7516 [D loss: 1.092922] [G loss: 1.172368]\n",
            "7517 [D loss: 1.099139] [G loss: 1.155761]\n",
            "7518 [D loss: 1.085024] [G loss: 1.173610]\n",
            "7519 [D loss: 1.108001] [G loss: 1.148009]\n",
            "7520 [D loss: 1.098124] [G loss: 1.156571]\n",
            "7521 [D loss: 1.094668] [G loss: 1.165622]\n",
            "7522 [D loss: 1.091983] [G loss: 1.164159]\n",
            "7523 [D loss: 1.116005] [G loss: 1.140861]\n",
            "7524 [D loss: 1.091598] [G loss: 1.161242]\n",
            "7525 [D loss: 1.095520] [G loss: 1.158724]\n",
            "7526 [D loss: 1.114920] [G loss: 1.159051]\n",
            "7527 [D loss: 1.094031] [G loss: 1.159147]\n",
            "7528 [D loss: 1.119007] [G loss: 1.132299]\n",
            "7529 [D loss: 1.087933] [G loss: 1.163353]\n",
            "7530 [D loss: 1.093342] [G loss: 1.151132]\n",
            "7531 [D loss: 1.080244] [G loss: 1.179613]\n",
            "7532 [D loss: 1.105140] [G loss: 1.141083]\n",
            "7533 [D loss: 1.093887] [G loss: 1.157569]\n",
            "7534 [D loss: 1.087896] [G loss: 1.165491]\n",
            "7535 [D loss: 1.100832] [G loss: 1.155384]\n",
            "7536 [D loss: 1.087934] [G loss: 1.178406]\n",
            "7537 [D loss: 1.099833] [G loss: 1.150137]\n",
            "7538 [D loss: 1.091403] [G loss: 1.152029]\n",
            "7539 [D loss: 1.094606] [G loss: 1.162539]\n",
            "7540 [D loss: 1.097673] [G loss: 1.156525]\n",
            "7541 [D loss: 1.097160] [G loss: 1.155704]\n",
            "7542 [D loss: 1.111423] [G loss: 1.145665]\n",
            "7543 [D loss: 1.095267] [G loss: 1.145356]\n",
            "7544 [D loss: 1.101030] [G loss: 1.159237]\n",
            "7545 [D loss: 1.108748] [G loss: 1.134628]\n",
            "7546 [D loss: 1.087742] [G loss: 1.144555]\n",
            "7547 [D loss: 1.093994] [G loss: 1.142061]\n",
            "7548 [D loss: 1.091074] [G loss: 1.172498]\n",
            "7549 [D loss: 1.090828] [G loss: 1.154331]\n",
            "7550 [D loss: 1.089245] [G loss: 1.177648]\n",
            "[[0.5049603  0.49648073 0.5024551  0.5001857 ]\n",
            " [0.9113155  0.29918805 0.8551868  0.141205  ]\n",
            " [0.6566491  0.3376852  0.66036063 0.38155308]\n",
            " [0.5321265  0.44122902 0.53177327 0.54605114]\n",
            " [0.53926086 0.4285677  0.51748794 0.5018152 ]]\n",
            "7551 [D loss: 1.111105] [G loss: 1.153141]\n",
            "7552 [D loss: 1.093211] [G loss: 1.156745]\n",
            "7553 [D loss: 1.106395] [G loss: 1.141128]\n",
            "7554 [D loss: 1.095895] [G loss: 1.164942]\n",
            "7555 [D loss: 1.112005] [G loss: 1.134535]\n",
            "7556 [D loss: 1.094743] [G loss: 1.179749]\n",
            "7557 [D loss: 1.101986] [G loss: 1.150336]\n",
            "7558 [D loss: 1.093105] [G loss: 1.162452]\n",
            "7559 [D loss: 1.087556] [G loss: 1.161977]\n",
            "7560 [D loss: 1.107853] [G loss: 1.159319]\n",
            "7561 [D loss: 1.096502] [G loss: 1.147833]\n",
            "7562 [D loss: 1.071887] [G loss: 1.169973]\n",
            "7563 [D loss: 1.094085] [G loss: 1.146854]\n",
            "7564 [D loss: 1.112705] [G loss: 1.154318]\n",
            "7565 [D loss: 1.117118] [G loss: 1.152077]\n",
            "7566 [D loss: 1.084650] [G loss: 1.162348]\n",
            "7567 [D loss: 1.095535] [G loss: 1.142415]\n",
            "7568 [D loss: 1.105066] [G loss: 1.146230]\n",
            "7569 [D loss: 1.112288] [G loss: 1.151886]\n",
            "7570 [D loss: 1.104275] [G loss: 1.156617]\n",
            "7571 [D loss: 1.072420] [G loss: 1.196407]\n",
            "7572 [D loss: 1.086308] [G loss: 1.158319]\n",
            "7573 [D loss: 1.091955] [G loss: 1.150989]\n",
            "7574 [D loss: 1.093539] [G loss: 1.144930]\n",
            "7575 [D loss: 1.111577] [G loss: 1.151435]\n",
            "7576 [D loss: 1.107177] [G loss: 1.147926]\n",
            "7577 [D loss: 1.094141] [G loss: 1.173176]\n",
            "7578 [D loss: 1.093837] [G loss: 1.160104]\n",
            "7579 [D loss: 1.103155] [G loss: 1.154979]\n",
            "7580 [D loss: 1.096945] [G loss: 1.158458]\n",
            "7581 [D loss: 1.089577] [G loss: 1.153953]\n",
            "7582 [D loss: 1.107126] [G loss: 1.155301]\n",
            "7583 [D loss: 1.096181] [G loss: 1.153671]\n",
            "7584 [D loss: 1.089779] [G loss: 1.152646]\n",
            "7585 [D loss: 1.085397] [G loss: 1.170133]\n",
            "7586 [D loss: 1.082986] [G loss: 1.174256]\n",
            "7587 [D loss: 1.093972] [G loss: 1.160771]\n",
            "7588 [D loss: 1.112697] [G loss: 1.148029]\n",
            "7589 [D loss: 1.087055] [G loss: 1.172739]\n",
            "7590 [D loss: 1.100302] [G loss: 1.144309]\n",
            "7591 [D loss: 1.098778] [G loss: 1.140008]\n",
            "7592 [D loss: 1.082181] [G loss: 1.162911]\n",
            "7593 [D loss: 1.102081] [G loss: 1.158451]\n",
            "7594 [D loss: 1.098551] [G loss: 1.158171]\n",
            "7595 [D loss: 1.118840] [G loss: 1.144433]\n",
            "7596 [D loss: 1.102807] [G loss: 1.166392]\n",
            "7597 [D loss: 1.103042] [G loss: 1.145980]\n",
            "7598 [D loss: 1.099409] [G loss: 1.156489]\n",
            "7599 [D loss: 1.084302] [G loss: 1.168087]\n",
            "7600 [D loss: 1.107292] [G loss: 1.130073]\n",
            "[[0.5841708  0.46706745 0.508758   0.4622043 ]\n",
            " [0.6411588  0.32021886 0.58261156 0.4674413 ]\n",
            " [0.5093964  0.46576318 0.507916   0.54534906]\n",
            " [0.6581256  0.41894752 0.502664   0.4188564 ]\n",
            " [0.50499904 0.49645394 0.50247055 0.5001722 ]]\n",
            "7601 [D loss: 1.112159] [G loss: 1.145616]\n",
            "7602 [D loss: 1.087353] [G loss: 1.176813]\n",
            "7603 [D loss: 1.089873] [G loss: 1.135301]\n",
            "7604 [D loss: 1.101304] [G loss: 1.163794]\n",
            "7605 [D loss: 1.114490] [G loss: 1.157549]\n",
            "7606 [D loss: 1.097216] [G loss: 1.153114]\n",
            "7607 [D loss: 1.106245] [G loss: 1.148466]\n",
            "7608 [D loss: 1.100671] [G loss: 1.146469]\n",
            "7609 [D loss: 1.105342] [G loss: 1.156242]\n",
            "7610 [D loss: 1.089216] [G loss: 1.173484]\n",
            "7611 [D loss: 1.078733] [G loss: 1.168986]\n",
            "7612 [D loss: 1.110483] [G loss: 1.132701]\n",
            "7613 [D loss: 1.112265] [G loss: 1.143430]\n",
            "7614 [D loss: 1.088234] [G loss: 1.163990]\n",
            "7615 [D loss: 1.117724] [G loss: 1.132629]\n",
            "7616 [D loss: 1.095464] [G loss: 1.159721]\n",
            "7617 [D loss: 1.094487] [G loss: 1.165581]\n",
            "7618 [D loss: 1.097730] [G loss: 1.176880]\n",
            "7619 [D loss: 1.106989] [G loss: 1.164053]\n",
            "7620 [D loss: 1.109383] [G loss: 1.154799]\n",
            "7621 [D loss: 1.113981] [G loss: 1.127688]\n",
            "7622 [D loss: 1.119966] [G loss: 1.141067]\n",
            "7623 [D loss: 1.094991] [G loss: 1.143253]\n",
            "7624 [D loss: 1.106850] [G loss: 1.152050]\n",
            "7625 [D loss: 1.089719] [G loss: 1.164892]\n",
            "7626 [D loss: 1.116240] [G loss: 1.151645]\n",
            "7627 [D loss: 1.109964] [G loss: 1.138397]\n",
            "7628 [D loss: 1.089879] [G loss: 1.160999]\n",
            "7629 [D loss: 1.108240] [G loss: 1.151148]\n",
            "7630 [D loss: 1.104994] [G loss: 1.143702]\n",
            "7631 [D loss: 1.093216] [G loss: 1.135725]\n",
            "7632 [D loss: 1.102935] [G loss: 1.141700]\n",
            "7633 [D loss: 1.102569] [G loss: 1.141540]\n",
            "7634 [D loss: 1.090264] [G loss: 1.152911]\n",
            "7635 [D loss: 1.073833] [G loss: 1.164376]\n",
            "7636 [D loss: 1.107346] [G loss: 1.161394]\n",
            "7637 [D loss: 1.087390] [G loss: 1.149862]\n",
            "7638 [D loss: 1.110982] [G loss: 1.146308]\n",
            "7639 [D loss: 1.107786] [G loss: 1.153067]\n",
            "7640 [D loss: 1.106210] [G loss: 1.132420]\n",
            "7641 [D loss: 1.111246] [G loss: 1.152721]\n",
            "7642 [D loss: 1.098777] [G loss: 1.153752]\n",
            "7643 [D loss: 1.093771] [G loss: 1.146081]\n",
            "7644 [D loss: 1.087223] [G loss: 1.176743]\n",
            "7645 [D loss: 1.099637] [G loss: 1.165970]\n",
            "7646 [D loss: 1.087160] [G loss: 1.144592]\n",
            "7647 [D loss: 1.103306] [G loss: 1.166809]\n",
            "7648 [D loss: 1.110048] [G loss: 1.150659]\n",
            "7649 [D loss: 1.106236] [G loss: 1.153519]\n",
            "7650 [D loss: 1.106563] [G loss: 1.156579]\n",
            "[[0.61027926 0.27791402 0.5406565  0.63711834]\n",
            " [0.68652415 0.4019913  0.625589   0.34226477]\n",
            " [0.76566267 0.37390214 0.697831   0.25372806]\n",
            " [0.57035536 0.44329408 0.48193187 0.49714288]\n",
            " [0.54120207 0.4440261  0.525195   0.4877996 ]]\n",
            "7651 [D loss: 1.094104] [G loss: 1.148495]\n",
            "7652 [D loss: 1.106177] [G loss: 1.142262]\n",
            "7653 [D loss: 1.095539] [G loss: 1.138368]\n",
            "7654 [D loss: 1.106444] [G loss: 1.151830]\n",
            "7655 [D loss: 1.087583] [G loss: 1.173692]\n",
            "7656 [D loss: 1.106782] [G loss: 1.160269]\n",
            "7657 [D loss: 1.109214] [G loss: 1.143223]\n",
            "7658 [D loss: 1.104131] [G loss: 1.146682]\n",
            "7659 [D loss: 1.090974] [G loss: 1.163736]\n",
            "7660 [D loss: 1.079518] [G loss: 1.168414]\n",
            "7661 [D loss: 1.097222] [G loss: 1.154655]\n",
            "7662 [D loss: 1.115665] [G loss: 1.138671]\n",
            "7663 [D loss: 1.111537] [G loss: 1.134776]\n",
            "7664 [D loss: 1.104944] [G loss: 1.137674]\n",
            "7665 [D loss: 1.112272] [G loss: 1.138549]\n",
            "7666 [D loss: 1.103969] [G loss: 1.163032]\n",
            "7667 [D loss: 1.091313] [G loss: 1.157855]\n",
            "7668 [D loss: 1.088275] [G loss: 1.163352]\n",
            "7669 [D loss: 1.098832] [G loss: 1.169225]\n",
            "7670 [D loss: 1.101224] [G loss: 1.135698]\n",
            "7671 [D loss: 1.100561] [G loss: 1.143776]\n",
            "7672 [D loss: 1.078958] [G loss: 1.168767]\n",
            "7673 [D loss: 1.090861] [G loss: 1.167435]\n",
            "7674 [D loss: 1.102945] [G loss: 1.130966]\n",
            "7675 [D loss: 1.109603] [G loss: 1.153972]\n",
            "7676 [D loss: 1.097703] [G loss: 1.155959]\n",
            "7677 [D loss: 1.085518] [G loss: 1.168750]\n",
            "7678 [D loss: 1.101151] [G loss: 1.144822]\n",
            "7679 [D loss: 1.102797] [G loss: 1.151415]\n",
            "7680 [D loss: 1.102410] [G loss: 1.154047]\n",
            "7681 [D loss: 1.083659] [G loss: 1.168412]\n",
            "7682 [D loss: 1.100437] [G loss: 1.148064]\n",
            "7683 [D loss: 1.081172] [G loss: 1.161526]\n",
            "7684 [D loss: 1.096477] [G loss: 1.140388]\n",
            "7685 [D loss: 1.081327] [G loss: 1.162843]\n",
            "7686 [D loss: 1.099470] [G loss: 1.154877]\n",
            "7687 [D loss: 1.107726] [G loss: 1.126366]\n",
            "7688 [D loss: 1.079413] [G loss: 1.182734]\n",
            "7689 [D loss: 1.108587] [G loss: 1.146793]\n",
            "7690 [D loss: 1.089749] [G loss: 1.150876]\n",
            "7691 [D loss: 1.101768] [G loss: 1.149072]\n",
            "7692 [D loss: 1.087328] [G loss: 1.147407]\n",
            "7693 [D loss: 1.080781] [G loss: 1.164054]\n",
            "7694 [D loss: 1.078328] [G loss: 1.172872]\n",
            "7695 [D loss: 1.091914] [G loss: 1.152304]\n",
            "7696 [D loss: 1.113148] [G loss: 1.137347]\n",
            "7697 [D loss: 1.101938] [G loss: 1.141500]\n",
            "7698 [D loss: 1.098080] [G loss: 1.144452]\n",
            "7699 [D loss: 1.098995] [G loss: 1.156448]\n",
            "7700 [D loss: 1.106073] [G loss: 1.150769]\n",
            "[[0.66578555 0.40226325 0.58889896 0.4350259 ]\n",
            " [0.8417823  0.32339606 0.75859493 0.16251251]\n",
            " [0.74913    0.33134475 0.65630674 0.27955642]\n",
            " [0.5825906  0.3393476  0.6043573  0.51314086]\n",
            " [0.5131982  0.49064037 0.5022947  0.50390106]]\n",
            "7701 [D loss: 1.098029] [G loss: 1.152375]\n",
            "7702 [D loss: 1.101636] [G loss: 1.133277]\n",
            "7703 [D loss: 1.117795] [G loss: 1.145998]\n",
            "7704 [D loss: 1.091249] [G loss: 1.143792]\n",
            "7705 [D loss: 1.089615] [G loss: 1.166631]\n",
            "7706 [D loss: 1.090262] [G loss: 1.156751]\n",
            "7707 [D loss: 1.098986] [G loss: 1.149371]\n",
            "7708 [D loss: 1.110448] [G loss: 1.146600]\n",
            "7709 [D loss: 1.101919] [G loss: 1.136894]\n",
            "7710 [D loss: 1.088233] [G loss: 1.181941]\n",
            "7711 [D loss: 1.086410] [G loss: 1.168135]\n",
            "7712 [D loss: 1.106287] [G loss: 1.139984]\n",
            "7713 [D loss: 1.091675] [G loss: 1.158543]\n",
            "7714 [D loss: 1.109525] [G loss: 1.143151]\n",
            "7715 [D loss: 1.101198] [G loss: 1.159761]\n",
            "7716 [D loss: 1.091931] [G loss: 1.142730]\n",
            "7717 [D loss: 1.115674] [G loss: 1.140826]\n",
            "7718 [D loss: 1.099212] [G loss: 1.161073]\n",
            "7719 [D loss: 1.112013] [G loss: 1.151690]\n",
            "7720 [D loss: 1.093050] [G loss: 1.176448]\n",
            "7721 [D loss: 1.114506] [G loss: 1.147423]\n",
            "7722 [D loss: 1.086566] [G loss: 1.175453]\n",
            "7723 [D loss: 1.099393] [G loss: 1.147370]\n",
            "7724 [D loss: 1.109698] [G loss: 1.133361]\n",
            "7725 [D loss: 1.113496] [G loss: 1.141767]\n",
            "7726 [D loss: 1.105571] [G loss: 1.137893]\n",
            "7727 [D loss: 1.097080] [G loss: 1.160116]\n",
            "7728 [D loss: 1.102052] [G loss: 1.157522]\n",
            "7729 [D loss: 1.100011] [G loss: 1.148345]\n",
            "7730 [D loss: 1.104164] [G loss: 1.149746]\n",
            "7731 [D loss: 1.103374] [G loss: 1.153190]\n",
            "7732 [D loss: 1.098178] [G loss: 1.145815]\n",
            "7733 [D loss: 1.106161] [G loss: 1.134895]\n",
            "7734 [D loss: 1.102459] [G loss: 1.132409]\n",
            "7735 [D loss: 1.110135] [G loss: 1.136760]\n",
            "7736 [D loss: 1.118723] [G loss: 1.154273]\n",
            "7737 [D loss: 1.096146] [G loss: 1.150002]\n",
            "7738 [D loss: 1.098950] [G loss: 1.158068]\n",
            "7739 [D loss: 1.100880] [G loss: 1.163717]\n",
            "7740 [D loss: 1.101918] [G loss: 1.148287]\n",
            "7741 [D loss: 1.086816] [G loss: 1.177118]\n",
            "7742 [D loss: 1.082609] [G loss: 1.150379]\n",
            "7743 [D loss: 1.089308] [G loss: 1.152450]\n",
            "7744 [D loss: 1.104978] [G loss: 1.151817]\n",
            "7745 [D loss: 1.106772] [G loss: 1.147825]\n",
            "7746 [D loss: 1.109948] [G loss: 1.156023]\n",
            "7747 [D loss: 1.085031] [G loss: 1.174135]\n",
            "7748 [D loss: 1.080654] [G loss: 1.177834]\n",
            "7749 [D loss: 1.102415] [G loss: 1.149337]\n",
            "7750 [D loss: 1.088807] [G loss: 1.135028]\n",
            "[[0.5658798  0.4097382  0.5423072  0.5650593 ]\n",
            " [0.89545316 0.15445724 0.70082426 0.2752886 ]\n",
            " [0.8307949  0.22168556 0.6775435  0.33519426]\n",
            " [0.592193   0.36565727 0.6124918  0.45827264]\n",
            " [0.88592964 0.17116064 0.65656304 0.389167  ]]\n",
            "7751 [D loss: 1.107830] [G loss: 1.147568]\n",
            "7752 [D loss: 1.129042] [G loss: 1.132992]\n",
            "7753 [D loss: 1.086217] [G loss: 1.149560]\n",
            "7754 [D loss: 1.114253] [G loss: 1.127053]\n",
            "7755 [D loss: 1.129904] [G loss: 1.115125]\n",
            "7756 [D loss: 1.105851] [G loss: 1.143241]\n",
            "7757 [D loss: 1.098467] [G loss: 1.162656]\n",
            "7758 [D loss: 1.106417] [G loss: 1.156190]\n",
            "7759 [D loss: 1.095533] [G loss: 1.146077]\n",
            "7760 [D loss: 1.104470] [G loss: 1.140970]\n",
            "7761 [D loss: 1.111013] [G loss: 1.135637]\n",
            "7762 [D loss: 1.103314] [G loss: 1.144412]\n",
            "7763 [D loss: 1.106948] [G loss: 1.152445]\n",
            "7764 [D loss: 1.098052] [G loss: 1.158741]\n",
            "7765 [D loss: 1.101696] [G loss: 1.150199]\n",
            "7766 [D loss: 1.103674] [G loss: 1.131163]\n",
            "7767 [D loss: 1.099750] [G loss: 1.148721]\n",
            "7768 [D loss: 1.088705] [G loss: 1.150418]\n",
            "7769 [D loss: 1.112499] [G loss: 1.135273]\n",
            "7770 [D loss: 1.113311] [G loss: 1.119568]\n",
            "7771 [D loss: 1.109337] [G loss: 1.154660]\n",
            "7772 [D loss: 1.105079] [G loss: 1.139832]\n",
            "7773 [D loss: 1.095888] [G loss: 1.171469]\n",
            "7774 [D loss: 1.112072] [G loss: 1.147524]\n",
            "7775 [D loss: 1.089197] [G loss: 1.159243]\n",
            "7776 [D loss: 1.101682] [G loss: 1.134047]\n",
            "7777 [D loss: 1.085594] [G loss: 1.157630]\n",
            "7778 [D loss: 1.094318] [G loss: 1.145651]\n",
            "7779 [D loss: 1.111469] [G loss: 1.128271]\n",
            "7780 [D loss: 1.085461] [G loss: 1.169527]\n",
            "7781 [D loss: 1.118680] [G loss: 1.145980]\n",
            "7782 [D loss: 1.115006] [G loss: 1.132712]\n",
            "7783 [D loss: 1.102464] [G loss: 1.125208]\n",
            "7784 [D loss: 1.093758] [G loss: 1.135906]\n",
            "7785 [D loss: 1.109201] [G loss: 1.142350]\n",
            "7786 [D loss: 1.096328] [G loss: 1.152319]\n",
            "7787 [D loss: 1.102632] [G loss: 1.145721]\n",
            "7788 [D loss: 1.099912] [G loss: 1.132456]\n",
            "7789 [D loss: 1.096676] [G loss: 1.152378]\n",
            "7790 [D loss: 1.103840] [G loss: 1.138231]\n",
            "7791 [D loss: 1.099524] [G loss: 1.127551]\n",
            "7792 [D loss: 1.099911] [G loss: 1.152662]\n",
            "7793 [D loss: 1.089318] [G loss: 1.160988]\n",
            "7794 [D loss: 1.094495] [G loss: 1.162000]\n",
            "7795 [D loss: 1.103783] [G loss: 1.133708]\n",
            "7796 [D loss: 1.106060] [G loss: 1.124209]\n",
            "7797 [D loss: 1.102750] [G loss: 1.137364]\n",
            "7798 [D loss: 1.085361] [G loss: 1.161989]\n",
            "7799 [D loss: 1.090896] [G loss: 1.155677]\n",
            "7800 [D loss: 1.111820] [G loss: 1.129178]\n",
            "[[0.5836345  0.40423283 0.52151316 0.51722413]\n",
            " [0.71312726 0.29999682 0.5752214  0.49973986]\n",
            " [0.5099358  0.48652673 0.51022834 0.5033934 ]\n",
            " [0.77255446 0.23716678 0.6008621  0.41293323]\n",
            " [0.65095276 0.22460842 0.4998938  0.6234913 ]]\n",
            "7801 [D loss: 1.092400] [G loss: 1.146245]\n",
            "7802 [D loss: 1.097527] [G loss: 1.165308]\n",
            "7803 [D loss: 1.108805] [G loss: 1.143221]\n",
            "7804 [D loss: 1.094497] [G loss: 1.166870]\n",
            "7805 [D loss: 1.076481] [G loss: 1.157342]\n",
            "7806 [D loss: 1.107524] [G loss: 1.145687]\n",
            "7807 [D loss: 1.097797] [G loss: 1.160285]\n",
            "7808 [D loss: 1.095005] [G loss: 1.144886]\n",
            "7809 [D loss: 1.088031] [G loss: 1.157404]\n",
            "7810 [D loss: 1.120045] [G loss: 1.139488]\n",
            "7811 [D loss: 1.088744] [G loss: 1.135051]\n",
            "7812 [D loss: 1.111444] [G loss: 1.148882]\n",
            "7813 [D loss: 1.098630] [G loss: 1.130197]\n",
            "7814 [D loss: 1.085565] [G loss: 1.161727]\n",
            "7815 [D loss: 1.096889] [G loss: 1.159251]\n",
            "7816 [D loss: 1.098724] [G loss: 1.139191]\n",
            "7817 [D loss: 1.105866] [G loss: 1.139996]\n",
            "7818 [D loss: 1.114713] [G loss: 1.139125]\n",
            "7819 [D loss: 1.110110] [G loss: 1.128503]\n",
            "7820 [D loss: 1.116319] [G loss: 1.128047]\n",
            "7821 [D loss: 1.107223] [G loss: 1.148342]\n",
            "7822 [D loss: 1.103536] [G loss: 1.141683]\n",
            "7823 [D loss: 1.092110] [G loss: 1.142563]\n",
            "7824 [D loss: 1.095849] [G loss: 1.164713]\n",
            "7825 [D loss: 1.094895] [G loss: 1.164108]\n",
            "7826 [D loss: 1.103409] [G loss: 1.139542]\n",
            "7827 [D loss: 1.106530] [G loss: 1.146060]\n",
            "7828 [D loss: 1.103420] [G loss: 1.140989]\n",
            "7829 [D loss: 1.107066] [G loss: 1.134534]\n",
            "7830 [D loss: 1.111958] [G loss: 1.127399]\n",
            "7831 [D loss: 1.087232] [G loss: 1.163199]\n",
            "7832 [D loss: 1.109008] [G loss: 1.139814]\n",
            "7833 [D loss: 1.107270] [G loss: 1.129354]\n",
            "7834 [D loss: 1.131970] [G loss: 1.122104]\n",
            "7835 [D loss: 1.102337] [G loss: 1.170817]\n",
            "7836 [D loss: 1.100100] [G loss: 1.146580]\n",
            "7837 [D loss: 1.104191] [G loss: 1.158683]\n",
            "7838 [D loss: 1.100598] [G loss: 1.119386]\n",
            "7839 [D loss: 1.097394] [G loss: 1.166103]\n",
            "7840 [D loss: 1.110750] [G loss: 1.152176]\n",
            "7841 [D loss: 1.088561] [G loss: 1.184134]\n",
            "7842 [D loss: 1.108069] [G loss: 1.148149]\n",
            "7843 [D loss: 1.112432] [G loss: 1.139696]\n",
            "7844 [D loss: 1.117285] [G loss: 1.134869]\n",
            "7845 [D loss: 1.093503] [G loss: 1.163844]\n",
            "7846 [D loss: 1.106568] [G loss: 1.143519]\n",
            "7847 [D loss: 1.081738] [G loss: 1.171411]\n",
            "7848 [D loss: 1.110271] [G loss: 1.151297]\n",
            "7849 [D loss: 1.089588] [G loss: 1.163282]\n",
            "7850 [D loss: 1.094325] [G loss: 1.147816]\n",
            "[[0.5070529  0.48820138 0.5050153  0.5028189 ]\n",
            " [0.65845203 0.37274095 0.56287247 0.48105675]\n",
            " [0.7096179  0.32613868 0.5914052  0.36999714]\n",
            " [0.7344142  0.37340093 0.67059994 0.29772812]\n",
            " [0.685767   0.40309033 0.6487831  0.31505167]]\n",
            "7851 [D loss: 1.114796] [G loss: 1.134286]\n",
            "7852 [D loss: 1.097540] [G loss: 1.142249]\n",
            "7853 [D loss: 1.099644] [G loss: 1.144640]\n",
            "7854 [D loss: 1.082476] [G loss: 1.163185]\n",
            "7855 [D loss: 1.102886] [G loss: 1.147928]\n",
            "7856 [D loss: 1.091088] [G loss: 1.126880]\n",
            "7857 [D loss: 1.097693] [G loss: 1.142043]\n",
            "7858 [D loss: 1.108090] [G loss: 1.138342]\n",
            "7859 [D loss: 1.102862] [G loss: 1.148183]\n",
            "7860 [D loss: 1.108969] [G loss: 1.146652]\n",
            "7861 [D loss: 1.095080] [G loss: 1.134222]\n",
            "7862 [D loss: 1.114277] [G loss: 1.131502]\n",
            "7863 [D loss: 1.119476] [G loss: 1.133003]\n",
            "7864 [D loss: 1.090262] [G loss: 1.164128]\n",
            "7865 [D loss: 1.100918] [G loss: 1.138718]\n",
            "7866 [D loss: 1.113822] [G loss: 1.134225]\n",
            "7867 [D loss: 1.114038] [G loss: 1.154944]\n",
            "7868 [D loss: 1.105720] [G loss: 1.139077]\n",
            "7869 [D loss: 1.089080] [G loss: 1.160988]\n",
            "7870 [D loss: 1.107555] [G loss: 1.143315]\n",
            "7871 [D loss: 1.106077] [G loss: 1.150433]\n",
            "7872 [D loss: 1.093998] [G loss: 1.145201]\n",
            "7873 [D loss: 1.101160] [G loss: 1.145487]\n",
            "7874 [D loss: 1.091569] [G loss: 1.158802]\n",
            "7875 [D loss: 1.090703] [G loss: 1.165116]\n",
            "7876 [D loss: 1.107560] [G loss: 1.132834]\n",
            "7877 [D loss: 1.079982] [G loss: 1.171894]\n",
            "7878 [D loss: 1.111321] [G loss: 1.137763]\n",
            "7879 [D loss: 1.092530] [G loss: 1.157620]\n",
            "7880 [D loss: 1.089461] [G loss: 1.132371]\n",
            "7881 [D loss: 1.120342] [G loss: 1.139929]\n",
            "7882 [D loss: 1.109810] [G loss: 1.146053]\n",
            "7883 [D loss: 1.114130] [G loss: 1.133955]\n",
            "7884 [D loss: 1.120082] [G loss: 1.140118]\n",
            "7885 [D loss: 1.104879] [G loss: 1.137195]\n",
            "7886 [D loss: 1.119173] [G loss: 1.129305]\n",
            "7887 [D loss: 1.098324] [G loss: 1.144914]\n",
            "7888 [D loss: 1.113201] [G loss: 1.128217]\n",
            "7889 [D loss: 1.092902] [G loss: 1.168772]\n",
            "7890 [D loss: 1.103992] [G loss: 1.139193]\n",
            "7891 [D loss: 1.091639] [G loss: 1.165592]\n",
            "7892 [D loss: 1.113280] [G loss: 1.134862]\n",
            "7893 [D loss: 1.099864] [G loss: 1.137721]\n",
            "7894 [D loss: 1.101234] [G loss: 1.126614]\n",
            "7895 [D loss: 1.086761] [G loss: 1.143846]\n",
            "7896 [D loss: 1.125570] [G loss: 1.126709]\n",
            "7897 [D loss: 1.108364] [G loss: 1.138312]\n",
            "7898 [D loss: 1.118354] [G loss: 1.133926]\n",
            "7899 [D loss: 1.087723] [G loss: 1.162703]\n",
            "7900 [D loss: 1.105264] [G loss: 1.124702]\n",
            "[[0.5536397  0.4507845  0.4942307  0.5192171 ]\n",
            " [0.5502447  0.45828736 0.48989686 0.5119729 ]\n",
            " [0.88879764 0.2598895  0.8095954  0.1337769 ]\n",
            " [0.58110297 0.47980097 0.52470505 0.4613499 ]\n",
            " [0.60426944 0.34992447 0.63967633 0.46526548]]\n",
            "7901 [D loss: 1.116494] [G loss: 1.137377]\n",
            "7902 [D loss: 1.095193] [G loss: 1.165703]\n",
            "7903 [D loss: 1.101465] [G loss: 1.168639]\n",
            "7904 [D loss: 1.100896] [G loss: 1.150825]\n",
            "7905 [D loss: 1.126794] [G loss: 1.117215]\n",
            "7906 [D loss: 1.112528] [G loss: 1.132454]\n",
            "7907 [D loss: 1.117019] [G loss: 1.129627]\n",
            "7908 [D loss: 1.087266] [G loss: 1.176401]\n",
            "7909 [D loss: 1.111093] [G loss: 1.139810]\n",
            "7910 [D loss: 1.094282] [G loss: 1.138341]\n",
            "7911 [D loss: 1.090169] [G loss: 1.160108]\n",
            "7912 [D loss: 1.113012] [G loss: 1.132193]\n",
            "7913 [D loss: 1.102711] [G loss: 1.148450]\n",
            "7914 [D loss: 1.107447] [G loss: 1.133195]\n",
            "7915 [D loss: 1.096635] [G loss: 1.152886]\n",
            "7916 [D loss: 1.099292] [G loss: 1.155142]\n",
            "7917 [D loss: 1.104079] [G loss: 1.135465]\n",
            "7918 [D loss: 1.105850] [G loss: 1.146423]\n",
            "7919 [D loss: 1.099756] [G loss: 1.150236]\n",
            "7920 [D loss: 1.114123] [G loss: 1.134544]\n",
            "7921 [D loss: 1.097759] [G loss: 1.146467]\n",
            "7922 [D loss: 1.109583] [G loss: 1.147614]\n",
            "7923 [D loss: 1.113334] [G loss: 1.130343]\n",
            "7924 [D loss: 1.075174] [G loss: 1.156949]\n",
            "7925 [D loss: 1.093754] [G loss: 1.152236]\n",
            "7926 [D loss: 1.098951] [G loss: 1.155119]\n",
            "7927 [D loss: 1.106370] [G loss: 1.127385]\n",
            "7928 [D loss: 1.114366] [G loss: 1.138962]\n",
            "7929 [D loss: 1.107609] [G loss: 1.147145]\n",
            "7930 [D loss: 1.100107] [G loss: 1.137738]\n",
            "7931 [D loss: 1.094679] [G loss: 1.153087]\n",
            "7932 [D loss: 1.111946] [G loss: 1.115796]\n",
            "7933 [D loss: 1.087224] [G loss: 1.156116]\n",
            "7934 [D loss: 1.111381] [G loss: 1.139807]\n",
            "7935 [D loss: 1.121618] [G loss: 1.125249]\n",
            "7936 [D loss: 1.106583] [G loss: 1.139057]\n",
            "7937 [D loss: 1.107071] [G loss: 1.132350]\n",
            "7938 [D loss: 1.099709] [G loss: 1.158127]\n",
            "7939 [D loss: 1.093014] [G loss: 1.158435]\n",
            "7940 [D loss: 1.083417] [G loss: 1.165309]\n",
            "7941 [D loss: 1.111711] [G loss: 1.135156]\n",
            "7942 [D loss: 1.090951] [G loss: 1.168815]\n",
            "7943 [D loss: 1.083923] [G loss: 1.150226]\n",
            "7944 [D loss: 1.107380] [G loss: 1.127355]\n",
            "7945 [D loss: 1.111456] [G loss: 1.155193]\n",
            "7946 [D loss: 1.091396] [G loss: 1.150671]\n",
            "7947 [D loss: 1.091190] [G loss: 1.155536]\n",
            "7948 [D loss: 1.108131] [G loss: 1.139476]\n",
            "7949 [D loss: 1.117663] [G loss: 1.115029]\n",
            "7950 [D loss: 1.109860] [G loss: 1.130102]\n",
            "[[0.7766799  0.3012488  0.6577596  0.34616196]\n",
            " [0.5052696  0.49626678 0.5025792  0.5000766 ]\n",
            " [0.71877193 0.3490885  0.63345814 0.3584072 ]\n",
            " [0.64818966 0.42286226 0.60573906 0.36554727]\n",
            " [0.5052696  0.49626678 0.5025792  0.5000766 ]]\n",
            "7951 [D loss: 1.110322] [G loss: 1.144627]\n",
            "7952 [D loss: 1.111389] [G loss: 1.137010]\n",
            "7953 [D loss: 1.107587] [G loss: 1.132541]\n",
            "7954 [D loss: 1.124831] [G loss: 1.116984]\n",
            "7955 [D loss: 1.105647] [G loss: 1.118457]\n",
            "7956 [D loss: 1.100783] [G loss: 1.163591]\n",
            "7957 [D loss: 1.096733] [G loss: 1.157630]\n",
            "7958 [D loss: 1.080284] [G loss: 1.152363]\n",
            "7959 [D loss: 1.104772] [G loss: 1.148419]\n",
            "7960 [D loss: 1.067897] [G loss: 1.165545]\n",
            "7961 [D loss: 1.101134] [G loss: 1.159443]\n",
            "7962 [D loss: 1.121870] [G loss: 1.132224]\n",
            "7963 [D loss: 1.121990] [G loss: 1.137810]\n",
            "7964 [D loss: 1.110851] [G loss: 1.142988]\n",
            "7965 [D loss: 1.111531] [G loss: 1.139706]\n",
            "7966 [D loss: 1.124620] [G loss: 1.135536]\n",
            "7967 [D loss: 1.098735] [G loss: 1.154442]\n",
            "7968 [D loss: 1.107216] [G loss: 1.130842]\n",
            "7969 [D loss: 1.111506] [G loss: 1.136571]\n",
            "7970 [D loss: 1.099807] [G loss: 1.145999]\n",
            "7971 [D loss: 1.097931] [G loss: 1.149668]\n",
            "7972 [D loss: 1.096189] [G loss: 1.144016]\n",
            "7973 [D loss: 1.098459] [G loss: 1.126482]\n",
            "7974 [D loss: 1.086499] [G loss: 1.160608]\n",
            "7975 [D loss: 1.108320] [G loss: 1.145030]\n",
            "7976 [D loss: 1.099080] [G loss: 1.152185]\n",
            "7977 [D loss: 1.099605] [G loss: 1.126617]\n",
            "7978 [D loss: 1.115373] [G loss: 1.114687]\n",
            "7979 [D loss: 1.105135] [G loss: 1.132235]\n",
            "7980 [D loss: 1.093049] [G loss: 1.142128]\n",
            "7981 [D loss: 1.094729] [G loss: 1.144299]\n",
            "7982 [D loss: 1.110197] [G loss: 1.120431]\n",
            "7983 [D loss: 1.101258] [G loss: 1.142657]\n",
            "7984 [D loss: 1.101404] [G loss: 1.156747]\n",
            "7985 [D loss: 1.101461] [G loss: 1.145907]\n",
            "7986 [D loss: 1.096701] [G loss: 1.159510]\n",
            "7987 [D loss: 1.116994] [G loss: 1.129893]\n",
            "7988 [D loss: 1.099651] [G loss: 1.136859]\n",
            "7989 [D loss: 1.099720] [G loss: 1.149326]\n",
            "7990 [D loss: 1.111734] [G loss: 1.141315]\n",
            "7991 [D loss: 1.103855] [G loss: 1.154966]\n",
            "7992 [D loss: 1.103093] [G loss: 1.154655]\n",
            "7993 [D loss: 1.100345] [G loss: 1.130026]\n",
            "7994 [D loss: 1.097322] [G loss: 1.139119]\n",
            "7995 [D loss: 1.103392] [G loss: 1.141860]\n",
            "7996 [D loss: 1.130645] [G loss: 1.139330]\n",
            "7997 [D loss: 1.115776] [G loss: 1.132756]\n",
            "7998 [D loss: 1.114436] [G loss: 1.117581]\n",
            "7999 [D loss: 1.099145] [G loss: 1.152661]\n",
            "8000 [D loss: 1.108301] [G loss: 1.145526]\n",
            "[[0.5252173  0.47094908 0.5109414  0.501454  ]\n",
            " [0.7162312  0.31035742 0.5649385  0.49748972]\n",
            " [0.50866294 0.45285413 0.5610026  0.51145726]\n",
            " [0.73589283 0.40648648 0.6894486  0.2731016 ]\n",
            " [0.5474356  0.48171952 0.49762088 0.4781718 ]]\n",
            "8001 [D loss: 1.096985] [G loss: 1.139268]\n",
            "8002 [D loss: 1.109786] [G loss: 1.134842]\n",
            "8003 [D loss: 1.106123] [G loss: 1.139037]\n",
            "8004 [D loss: 1.108402] [G loss: 1.137730]\n",
            "8005 [D loss: 1.110159] [G loss: 1.145560]\n",
            "8006 [D loss: 1.105547] [G loss: 1.120340]\n",
            "8007 [D loss: 1.109836] [G loss: 1.132325]\n",
            "8008 [D loss: 1.104331] [G loss: 1.133627]\n",
            "8009 [D loss: 1.104632] [G loss: 1.136854]\n",
            "8010 [D loss: 1.113974] [G loss: 1.123526]\n",
            "8011 [D loss: 1.114945] [G loss: 1.140719]\n",
            "8012 [D loss: 1.102574] [G loss: 1.162289]\n",
            "8013 [D loss: 1.109104] [G loss: 1.143765]\n",
            "8014 [D loss: 1.106248] [G loss: 1.149531]\n",
            "8015 [D loss: 1.098556] [G loss: 1.149225]\n",
            "8016 [D loss: 1.103974] [G loss: 1.136008]\n",
            "8017 [D loss: 1.090604] [G loss: 1.149384]\n",
            "8018 [D loss: 1.119183] [G loss: 1.136293]\n",
            "8019 [D loss: 1.105017] [G loss: 1.136503]\n",
            "8020 [D loss: 1.109870] [G loss: 1.124008]\n",
            "8021 [D loss: 1.092090] [G loss: 1.140459]\n",
            "8022 [D loss: 1.117751] [G loss: 1.133257]\n",
            "8023 [D loss: 1.098594] [G loss: 1.167189]\n",
            "8024 [D loss: 1.109541] [G loss: 1.145827]\n",
            "8025 [D loss: 1.097579] [G loss: 1.150886]\n",
            "8026 [D loss: 1.108765] [G loss: 1.150729]\n",
            "8027 [D loss: 1.102396] [G loss: 1.136671]\n",
            "8028 [D loss: 1.108888] [G loss: 1.139514]\n",
            "8029 [D loss: 1.114670] [G loss: 1.124004]\n",
            "8030 [D loss: 1.111121] [G loss: 1.139756]\n",
            "8031 [D loss: 1.107878] [G loss: 1.141051]\n",
            "8032 [D loss: 1.099118] [G loss: 1.148118]\n",
            "8033 [D loss: 1.103161] [G loss: 1.139192]\n",
            "8034 [D loss: 1.103953] [G loss: 1.132511]\n",
            "8035 [D loss: 1.109621] [G loss: 1.134706]\n",
            "8036 [D loss: 1.093689] [G loss: 1.163539]\n",
            "8037 [D loss: 1.095612] [G loss: 1.164592]\n",
            "8038 [D loss: 1.100860] [G loss: 1.144624]\n",
            "8039 [D loss: 1.095844] [G loss: 1.149308]\n",
            "8040 [D loss: 1.092035] [G loss: 1.165283]\n",
            "8041 [D loss: 1.111763] [G loss: 1.139412]\n",
            "8042 [D loss: 1.104679] [G loss: 1.140858]\n",
            "8043 [D loss: 1.084633] [G loss: 1.166923]\n",
            "8044 [D loss: 1.111788] [G loss: 1.136911]\n",
            "8045 [D loss: 1.109143] [G loss: 1.131540]\n",
            "8046 [D loss: 1.096686] [G loss: 1.155800]\n",
            "8047 [D loss: 1.125499] [G loss: 1.145555]\n",
            "8048 [D loss: 1.106177] [G loss: 1.148165]\n",
            "8049 [D loss: 1.125805] [G loss: 1.137191]\n",
            "8050 [D loss: 1.127920] [G loss: 1.119263]\n",
            "[[0.94306153 0.19497211 0.8798409  0.07177524]\n",
            " [0.5999233  0.43748155 0.5131176  0.46800283]\n",
            " [0.81357676 0.3382204  0.7460665  0.19427592]\n",
            " [0.58510524 0.41335222 0.4850155  0.5085104 ]\n",
            " [0.52999544 0.48403236 0.51095885 0.49461943]]\n",
            "8051 [D loss: 1.091523] [G loss: 1.123893]\n",
            "8052 [D loss: 1.107777] [G loss: 1.152957]\n",
            "8053 [D loss: 1.113625] [G loss: 1.114871]\n",
            "8054 [D loss: 1.131532] [G loss: 1.124396]\n",
            "8055 [D loss: 1.093939] [G loss: 1.148373]\n",
            "8056 [D loss: 1.109367] [G loss: 1.127807]\n",
            "8057 [D loss: 1.105925] [G loss: 1.124179]\n",
            "8058 [D loss: 1.106776] [G loss: 1.148414]\n",
            "8059 [D loss: 1.112452] [G loss: 1.124194]\n",
            "8060 [D loss: 1.107972] [G loss: 1.138640]\n",
            "8061 [D loss: 1.111106] [G loss: 1.146198]\n",
            "8062 [D loss: 1.103862] [G loss: 1.139405]\n",
            "8063 [D loss: 1.095025] [G loss: 1.155270]\n",
            "8064 [D loss: 1.100451] [G loss: 1.154737]\n",
            "8065 [D loss: 1.101030] [G loss: 1.156829]\n",
            "8066 [D loss: 1.111998] [G loss: 1.124853]\n",
            "8067 [D loss: 1.111770] [G loss: 1.120491]\n",
            "8068 [D loss: 1.101202] [G loss: 1.146239]\n",
            "8069 [D loss: 1.113389] [G loss: 1.147671]\n",
            "8070 [D loss: 1.110971] [G loss: 1.137409]\n",
            "8071 [D loss: 1.102321] [G loss: 1.146746]\n",
            "8072 [D loss: 1.106892] [G loss: 1.134896]\n",
            "8073 [D loss: 1.095418] [G loss: 1.141874]\n",
            "8074 [D loss: 1.110017] [G loss: 1.130731]\n",
            "8075 [D loss: 1.083021] [G loss: 1.144405]\n",
            "8076 [D loss: 1.100826] [G loss: 1.136639]\n",
            "8077 [D loss: 1.109347] [G loss: 1.132689]\n",
            "8078 [D loss: 1.102880] [G loss: 1.123067]\n",
            "8079 [D loss: 1.103752] [G loss: 1.134446]\n",
            "8080 [D loss: 1.100887] [G loss: 1.139398]\n",
            "8081 [D loss: 1.100072] [G loss: 1.146686]\n",
            "8082 [D loss: 1.121960] [G loss: 1.113609]\n",
            "8083 [D loss: 1.116510] [G loss: 1.120399]\n",
            "8084 [D loss: 1.109348] [G loss: 1.136032]\n",
            "8085 [D loss: 1.105526] [G loss: 1.138556]\n",
            "8086 [D loss: 1.098637] [G loss: 1.138973]\n",
            "8087 [D loss: 1.108494] [G loss: 1.148116]\n",
            "8088 [D loss: 1.104501] [G loss: 1.132665]\n",
            "8089 [D loss: 1.095279] [G loss: 1.131416]\n",
            "8090 [D loss: 1.107485] [G loss: 1.122861]\n",
            "8091 [D loss: 1.093421] [G loss: 1.130886]\n",
            "8092 [D loss: 1.120013] [G loss: 1.122668]\n",
            "8093 [D loss: 1.106682] [G loss: 1.133311]\n",
            "8094 [D loss: 1.112187] [G loss: 1.131411]\n",
            "8095 [D loss: 1.104514] [G loss: 1.150126]\n",
            "8096 [D loss: 1.104535] [G loss: 1.147589]\n",
            "8097 [D loss: 1.089643] [G loss: 1.132112]\n",
            "8098 [D loss: 1.118196] [G loss: 1.132276]\n",
            "8099 [D loss: 1.107814] [G loss: 1.138532]\n",
            "8100 [D loss: 1.084604] [G loss: 1.146205]\n",
            "[[0.69903344 0.2068718  0.5124348  0.59603727]\n",
            " [0.71984416 0.2660059  0.5554993  0.45035464]\n",
            " [0.5053856  0.49618742 0.502626   0.5000343 ]\n",
            " [0.56367    0.4463066  0.5200633  0.50970304]\n",
            " [0.66964185 0.3900887  0.5334842  0.4272315 ]]\n",
            "8101 [D loss: 1.097544] [G loss: 1.136524]\n",
            "8102 [D loss: 1.111669] [G loss: 1.140496]\n",
            "8103 [D loss: 1.109236] [G loss: 1.150132]\n",
            "8104 [D loss: 1.097725] [G loss: 1.142125]\n",
            "8105 [D loss: 1.125098] [G loss: 1.131754]\n",
            "8106 [D loss: 1.106131] [G loss: 1.135475]\n",
            "8107 [D loss: 1.100037] [G loss: 1.133671]\n",
            "8108 [D loss: 1.104010] [G loss: 1.139733]\n",
            "8109 [D loss: 1.117280] [G loss: 1.135049]\n",
            "8110 [D loss: 1.116594] [G loss: 1.149265]\n",
            "8111 [D loss: 1.095259] [G loss: 1.146187]\n",
            "8112 [D loss: 1.102449] [G loss: 1.134410]\n",
            "8113 [D loss: 1.107276] [G loss: 1.144765]\n",
            "8114 [D loss: 1.092658] [G loss: 1.156405]\n",
            "8115 [D loss: 1.116065] [G loss: 1.132543]\n",
            "8116 [D loss: 1.104358] [G loss: 1.131560]\n",
            "8117 [D loss: 1.103283] [G loss: 1.152062]\n",
            "8118 [D loss: 1.106117] [G loss: 1.145906]\n",
            "8119 [D loss: 1.098027] [G loss: 1.129020]\n",
            "8120 [D loss: 1.081862] [G loss: 1.157901]\n",
            "8121 [D loss: 1.101767] [G loss: 1.128670]\n",
            "8122 [D loss: 1.097907] [G loss: 1.129902]\n",
            "8123 [D loss: 1.110836] [G loss: 1.148693]\n",
            "8124 [D loss: 1.103414] [G loss: 1.163163]\n",
            "8125 [D loss: 1.115773] [G loss: 1.121689]\n",
            "8126 [D loss: 1.112225] [G loss: 1.134502]\n",
            "8127 [D loss: 1.112973] [G loss: 1.134807]\n",
            "8128 [D loss: 1.125790] [G loss: 1.110998]\n",
            "8129 [D loss: 1.102515] [G loss: 1.128599]\n",
            "8130 [D loss: 1.102206] [G loss: 1.131388]\n",
            "8131 [D loss: 1.103082] [G loss: 1.129029]\n",
            "8132 [D loss: 1.102038] [G loss: 1.129225]\n",
            "8133 [D loss: 1.095143] [G loss: 1.145414]\n",
            "8134 [D loss: 1.114922] [G loss: 1.127643]\n",
            "8135 [D loss: 1.128640] [G loss: 1.122183]\n",
            "8136 [D loss: 1.118817] [G loss: 1.145303]\n",
            "8137 [D loss: 1.109388] [G loss: 1.136348]\n",
            "8138 [D loss: 1.097919] [G loss: 1.138352]\n",
            "8139 [D loss: 1.097478] [G loss: 1.137963]\n",
            "8140 [D loss: 1.106063] [G loss: 1.120033]\n",
            "8141 [D loss: 1.095869] [G loss: 1.135068]\n",
            "8142 [D loss: 1.089697] [G loss: 1.136755]\n",
            "8143 [D loss: 1.113226] [G loss: 1.156289]\n",
            "8144 [D loss: 1.112719] [G loss: 1.136609]\n",
            "8145 [D loss: 1.113324] [G loss: 1.151542]\n",
            "8146 [D loss: 1.130922] [G loss: 1.119948]\n",
            "8147 [D loss: 1.111944] [G loss: 1.129595]\n",
            "8148 [D loss: 1.102995] [G loss: 1.148284]\n",
            "8149 [D loss: 1.104076] [G loss: 1.116813]\n",
            "8150 [D loss: 1.111588] [G loss: 1.132909]\n",
            "[[0.7652972  0.36696315 0.75787497 0.31938472]\n",
            " [0.77238727 0.31355575 0.6744412  0.3162363 ]\n",
            " [0.6780419  0.2878833  0.5262126  0.47317   ]\n",
            " [0.61778444 0.40700573 0.46803927 0.47827315]\n",
            " [0.5092031  0.4757647  0.5081923  0.507719  ]]\n",
            "8151 [D loss: 1.079762] [G loss: 1.147131]\n",
            "8152 [D loss: 1.115393] [G loss: 1.131448]\n",
            "8153 [D loss: 1.099932] [G loss: 1.144878]\n",
            "8154 [D loss: 1.113458] [G loss: 1.130999]\n",
            "8155 [D loss: 1.095492] [G loss: 1.118463]\n",
            "8156 [D loss: 1.108165] [G loss: 1.111734]\n",
            "8157 [D loss: 1.096428] [G loss: 1.136771]\n",
            "8158 [D loss: 1.091133] [G loss: 1.154534]\n",
            "8159 [D loss: 1.092609] [G loss: 1.143431]\n",
            "8160 [D loss: 1.113728] [G loss: 1.138689]\n",
            "8161 [D loss: 1.118479] [G loss: 1.114479]\n",
            "8162 [D loss: 1.132377] [G loss: 1.125566]\n",
            "8163 [D loss: 1.103483] [G loss: 1.150685]\n",
            "8164 [D loss: 1.099968] [G loss: 1.151447]\n",
            "8165 [D loss: 1.107811] [G loss: 1.140857]\n",
            "8166 [D loss: 1.116534] [G loss: 1.123917]\n",
            "8167 [D loss: 1.130089] [G loss: 1.105899]\n",
            "8168 [D loss: 1.104502] [G loss: 1.142578]\n",
            "8169 [D loss: 1.118551] [G loss: 1.121562]\n",
            "8170 [D loss: 1.114324] [G loss: 1.141729]\n",
            "8171 [D loss: 1.105726] [G loss: 1.125858]\n",
            "8172 [D loss: 1.109963] [G loss: 1.126268]\n",
            "8173 [D loss: 1.119219] [G loss: 1.140036]\n",
            "8174 [D loss: 1.114176] [G loss: 1.140622]\n",
            "8175 [D loss: 1.111235] [G loss: 1.138212]\n",
            "8176 [D loss: 1.093283] [G loss: 1.145680]\n",
            "8177 [D loss: 1.107314] [G loss: 1.138695]\n",
            "8178 [D loss: 1.097616] [G loss: 1.137232]\n",
            "8179 [D loss: 1.114060] [G loss: 1.132539]\n",
            "8180 [D loss: 1.094074] [G loss: 1.158212]\n",
            "8181 [D loss: 1.124953] [G loss: 1.112456]\n",
            "8182 [D loss: 1.102803] [G loss: 1.136579]\n",
            "8183 [D loss: 1.115401] [G loss: 1.125687]\n",
            "8184 [D loss: 1.090129] [G loss: 1.149763]\n",
            "8185 [D loss: 1.118129] [G loss: 1.142288]\n",
            "8186 [D loss: 1.124344] [G loss: 1.128055]\n",
            "8187 [D loss: 1.118181] [G loss: 1.132364]\n",
            "8188 [D loss: 1.094479] [G loss: 1.137699]\n",
            "8189 [D loss: 1.104755] [G loss: 1.126027]\n",
            "8190 [D loss: 1.114876] [G loss: 1.131716]\n",
            "8191 [D loss: 1.106398] [G loss: 1.129096]\n",
            "8192 [D loss: 1.099267] [G loss: 1.135545]\n",
            "8193 [D loss: 1.114196] [G loss: 1.145029]\n",
            "8194 [D loss: 1.098872] [G loss: 1.133218]\n",
            "8195 [D loss: 1.111140] [G loss: 1.135685]\n",
            "8196 [D loss: 1.116240] [G loss: 1.135231]\n",
            "8197 [D loss: 1.129227] [G loss: 1.130649]\n",
            "8198 [D loss: 1.096545] [G loss: 1.126358]\n",
            "8199 [D loss: 1.085170] [G loss: 1.140584]\n",
            "8200 [D loss: 1.105662] [G loss: 1.114678]\n",
            "[[0.86640424 0.28487086 0.8034163  0.14371562]\n",
            " [0.65371466 0.33940166 0.5190217  0.52474344]\n",
            " [0.8098002  0.2753939  0.64668316 0.3650002 ]\n",
            " [0.50794137 0.4936673  0.50451136 0.50001353]\n",
            " [0.64824563 0.28926858 0.4850407  0.54857594]]\n",
            "8201 [D loss: 1.120898] [G loss: 1.120166]\n",
            "8202 [D loss: 1.106980] [G loss: 1.111853]\n",
            "8203 [D loss: 1.084714] [G loss: 1.154884]\n",
            "8204 [D loss: 1.098042] [G loss: 1.162304]\n",
            "8205 [D loss: 1.103645] [G loss: 1.155600]\n",
            "8206 [D loss: 1.116328] [G loss: 1.136362]\n",
            "8207 [D loss: 1.110577] [G loss: 1.145588]\n",
            "8208 [D loss: 1.113996] [G loss: 1.124948]\n",
            "8209 [D loss: 1.106550] [G loss: 1.146696]\n",
            "8210 [D loss: 1.094150] [G loss: 1.143559]\n",
            "8211 [D loss: 1.118413] [G loss: 1.120679]\n",
            "8212 [D loss: 1.117991] [G loss: 1.122578]\n",
            "8213 [D loss: 1.116819] [G loss: 1.102812]\n",
            "8214 [D loss: 1.115160] [G loss: 1.105413]\n",
            "8215 [D loss: 1.094053] [G loss: 1.149108]\n",
            "8216 [D loss: 1.102080] [G loss: 1.137315]\n",
            "8217 [D loss: 1.115420] [G loss: 1.132328]\n",
            "8218 [D loss: 1.121287] [G loss: 1.120958]\n",
            "8219 [D loss: 1.107857] [G loss: 1.134583]\n",
            "8220 [D loss: 1.108991] [G loss: 1.130016]\n",
            "8221 [D loss: 1.096508] [G loss: 1.133546]\n",
            "8222 [D loss: 1.110660] [G loss: 1.124662]\n",
            "8223 [D loss: 1.108891] [G loss: 1.135058]\n",
            "8224 [D loss: 1.106633] [G loss: 1.139232]\n",
            "8225 [D loss: 1.117417] [G loss: 1.127922]\n",
            "8226 [D loss: 1.101091] [G loss: 1.157488]\n",
            "8227 [D loss: 1.107393] [G loss: 1.136810]\n",
            "8228 [D loss: 1.119916] [G loss: 1.121495]\n",
            "8229 [D loss: 1.109166] [G loss: 1.123615]\n",
            "8230 [D loss: 1.107634] [G loss: 1.134769]\n",
            "8231 [D loss: 1.122804] [G loss: 1.124074]\n",
            "8232 [D loss: 1.098219] [G loss: 1.147931]\n",
            "8233 [D loss: 1.101094] [G loss: 1.146377]\n",
            "8234 [D loss: 1.116745] [G loss: 1.127654]\n",
            "8235 [D loss: 1.096228] [G loss: 1.134774]\n",
            "8236 [D loss: 1.110758] [G loss: 1.132432]\n",
            "8237 [D loss: 1.096119] [G loss: 1.162180]\n",
            "8238 [D loss: 1.088155] [G loss: 1.160202]\n",
            "8239 [D loss: 1.108920] [G loss: 1.128633]\n",
            "8240 [D loss: 1.103886] [G loss: 1.142460]\n",
            "8241 [D loss: 1.115770] [G loss: 1.123312]\n",
            "8242 [D loss: 1.102234] [G loss: 1.144347]\n",
            "8243 [D loss: 1.118678] [G loss: 1.129054]\n",
            "8244 [D loss: 1.122580] [G loss: 1.124942]\n",
            "8245 [D loss: 1.132914] [G loss: 1.115688]\n",
            "8246 [D loss: 1.100022] [G loss: 1.139154]\n",
            "8247 [D loss: 1.119673] [G loss: 1.118177]\n",
            "8248 [D loss: 1.103599] [G loss: 1.124551]\n",
            "8249 [D loss: 1.118340] [G loss: 1.106119]\n",
            "8250 [D loss: 1.115663] [G loss: 1.113464]\n",
            "[[0.67270917 0.3105174  0.6488689  0.35423127]\n",
            " [0.50931275 0.49217695 0.5053414  0.5002325 ]\n",
            " [0.5837458  0.34307766 0.58366835 0.57325107]\n",
            " [0.82760423 0.29086548 0.68360674 0.29264256]\n",
            " [0.59640133 0.48367646 0.47512233 0.43768567]]\n",
            "8251 [D loss: 1.112619] [G loss: 1.136796]\n",
            "8252 [D loss: 1.103509] [G loss: 1.132625]\n",
            "8253 [D loss: 1.114669] [G loss: 1.127202]\n",
            "8254 [D loss: 1.112027] [G loss: 1.124144]\n",
            "8255 [D loss: 1.114937] [G loss: 1.126832]\n",
            "8256 [D loss: 1.118352] [G loss: 1.110341]\n",
            "8257 [D loss: 1.104799] [G loss: 1.136320]\n",
            "8258 [D loss: 1.094894] [G loss: 1.135604]\n",
            "8259 [D loss: 1.109801] [G loss: 1.147367]\n",
            "8260 [D loss: 1.110610] [G loss: 1.129786]\n",
            "8261 [D loss: 1.110375] [G loss: 1.148910]\n",
            "8262 [D loss: 1.096836] [G loss: 1.141425]\n",
            "8263 [D loss: 1.112423] [G loss: 1.129538]\n",
            "8264 [D loss: 1.120439] [G loss: 1.125301]\n",
            "8265 [D loss: 1.090083] [G loss: 1.149432]\n",
            "8266 [D loss: 1.106674] [G loss: 1.148705]\n",
            "8267 [D loss: 1.119557] [G loss: 1.120432]\n",
            "8268 [D loss: 1.103012] [G loss: 1.139288]\n",
            "8269 [D loss: 1.098883] [G loss: 1.132862]\n",
            "8270 [D loss: 1.108148] [G loss: 1.139325]\n",
            "8271 [D loss: 1.100143] [G loss: 1.150871]\n",
            "8272 [D loss: 1.102107] [G loss: 1.133883]\n",
            "8273 [D loss: 1.102228] [G loss: 1.126281]\n",
            "8274 [D loss: 1.103528] [G loss: 1.130789]\n",
            "8275 [D loss: 1.118066] [G loss: 1.129094]\n",
            "8276 [D loss: 1.112448] [G loss: 1.124288]\n",
            "8277 [D loss: 1.096378] [G loss: 1.146866]\n",
            "8278 [D loss: 1.103225] [G loss: 1.144399]\n",
            "8279 [D loss: 1.111299] [G loss: 1.153040]\n",
            "8280 [D loss: 1.111589] [G loss: 1.131423]\n",
            "8281 [D loss: 1.099862] [G loss: 1.148476]\n",
            "8282 [D loss: 1.100259] [G loss: 1.123560]\n",
            "8283 [D loss: 1.093244] [G loss: 1.162601]\n",
            "8284 [D loss: 1.117172] [G loss: 1.126243]\n",
            "8285 [D loss: 1.123432] [G loss: 1.117227]\n",
            "8286 [D loss: 1.105651] [G loss: 1.126738]\n",
            "8287 [D loss: 1.112374] [G loss: 1.141110]\n",
            "8288 [D loss: 1.100136] [G loss: 1.144975]\n",
            "8289 [D loss: 1.103307] [G loss: 1.128722]\n",
            "8290 [D loss: 1.093067] [G loss: 1.141129]\n",
            "8291 [D loss: 1.093929] [G loss: 1.146102]\n",
            "8292 [D loss: 1.107155] [G loss: 1.139557]\n",
            "8293 [D loss: 1.091772] [G loss: 1.165205]\n",
            "8294 [D loss: 1.123716] [G loss: 1.127820]\n",
            "8295 [D loss: 1.115812] [G loss: 1.123195]\n",
            "8296 [D loss: 1.095531] [G loss: 1.149637]\n",
            "8297 [D loss: 1.109061] [G loss: 1.133694]\n",
            "8298 [D loss: 1.104132] [G loss: 1.133699]\n",
            "8299 [D loss: 1.100512] [G loss: 1.133949]\n",
            "8300 [D loss: 1.090306] [G loss: 1.173757]\n",
            "[[0.64753795 0.30579042 0.52351236 0.5350471 ]\n",
            " [0.74084854 0.3510057  0.66659695 0.31117103]\n",
            " [0.56265026 0.35984194 0.5648671  0.5808498 ]\n",
            " [0.5298917  0.44933003 0.51529455 0.5246066 ]\n",
            " [0.5222588  0.4655801  0.518558   0.5233736 ]]\n",
            "8301 [D loss: 1.111564] [G loss: 1.119285]\n",
            "8302 [D loss: 1.100203] [G loss: 1.134513]\n",
            "8303 [D loss: 1.111761] [G loss: 1.141082]\n",
            "8304 [D loss: 1.120920] [G loss: 1.140767]\n",
            "8305 [D loss: 1.103057] [G loss: 1.137059]\n",
            "8306 [D loss: 1.105720] [G loss: 1.123623]\n",
            "8307 [D loss: 1.085515] [G loss: 1.160360]\n",
            "8308 [D loss: 1.099469] [G loss: 1.149417]\n",
            "8309 [D loss: 1.111292] [G loss: 1.141285]\n",
            "8310 [D loss: 1.100248] [G loss: 1.137141]\n",
            "8311 [D loss: 1.098137] [G loss: 1.123443]\n",
            "8312 [D loss: 1.091303] [G loss: 1.154525]\n",
            "8313 [D loss: 1.105505] [G loss: 1.141543]\n",
            "8314 [D loss: 1.093050] [G loss: 1.161004]\n",
            "8315 [D loss: 1.104086] [G loss: 1.113349]\n",
            "8316 [D loss: 1.105777] [G loss: 1.131537]\n",
            "8317 [D loss: 1.102820] [G loss: 1.152032]\n",
            "8318 [D loss: 1.109228] [G loss: 1.103775]\n",
            "8319 [D loss: 1.105160] [G loss: 1.140222]\n",
            "8320 [D loss: 1.136617] [G loss: 1.115710]\n",
            "8321 [D loss: 1.132506] [G loss: 1.109777]\n",
            "8322 [D loss: 1.108460] [G loss: 1.135194]\n",
            "8323 [D loss: 1.089163] [G loss: 1.128384]\n",
            "8324 [D loss: 1.124899] [G loss: 1.115019]\n",
            "8325 [D loss: 1.097376] [G loss: 1.135792]\n",
            "8326 [D loss: 1.106648] [G loss: 1.128117]\n",
            "8327 [D loss: 1.101573] [G loss: 1.140480]\n",
            "8328 [D loss: 1.129557] [G loss: 1.115604]\n",
            "8329 [D loss: 1.097495] [G loss: 1.149667]\n",
            "8330 [D loss: 1.120364] [G loss: 1.118928]\n",
            "8331 [D loss: 1.112574] [G loss: 1.131704]\n",
            "8332 [D loss: 1.101580] [G loss: 1.131139]\n",
            "8333 [D loss: 1.114487] [G loss: 1.128064]\n",
            "8334 [D loss: 1.112979] [G loss: 1.127281]\n",
            "8335 [D loss: 1.092680] [G loss: 1.125947]\n",
            "8336 [D loss: 1.107089] [G loss: 1.135355]\n",
            "8337 [D loss: 1.098920] [G loss: 1.149670]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2530\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m         \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2532\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Operation 'Identity' has no attr named '_read_only_resource_inputs'.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-81034f59bbf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;31m###################################epoch4000 to 400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m#################################batch size 32 to 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-81034f59bbf3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;31m# Generate a batch of new images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0mgen_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0;31m# print(gen_data.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1606\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1835\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m     \"\"\"\n\u001b[0;32m-> 1837\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m   def interleave(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   4283\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4284\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 4285\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   4286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4287\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3523\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3524\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3052\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3053\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         if x is not None)\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m     \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    407\u001b[0m       \u001b[0;31m# Check for any resource inputs. If we find any, we update control_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m       \u001b[0;31m# and last_write_to_resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_get_resource_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mis_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mResourceType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_ONLY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0minput_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m_get_resource_inputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_resource_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m   \u001b[0;34m\"\"\"Returns an iterable of resources touched by this `op`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m   \u001b[0mreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_read_write_resource_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m   \u001b[0msaturated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msaturated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/auto_control_deps_utils.py\u001b[0m in \u001b[0;36mget_read_write_resource_inputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mread_only_input_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAD_ONLY_RESOURCE_INPUTS_ATTR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# Attr was not set. Add all resource inputs to `writes` and return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2530\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m         \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2532\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BptgxlUgeLc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "7af5aa4a-05d1-41e1-9eb5-1128f89c62e1"
      },
      "source": [
        "# print(wgan.DLOS)\n",
        "# print(wgan.GLOS)\n",
        "plt.plot(wgan.GLOS)\n",
        "plt.plot(wgan.DLOS)\n",
        "plt.title('Train BCE Losses')\n",
        "plt.ylabel('wasserstein_loss')\n",
        "plt.xlabel('Epoch #')\n",
        "plt.legend(['Critic','Generator'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wHtHOIZyAgi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "084cda60-b185-4307-8118-7e46b65b971b"
      },
      "source": [
        "_, scaler = load_knn_data()\n",
        "import pandas as pd\n",
        "r, c = 6, 2000\n",
        "noise = np.random.normal(0, 1, (c, 4))\n",
        "gen_data = wgan.generator.predict(noise)\n",
        "gen_data = scaler.inverse_transform(gen_data)\n",
        "\n",
        "gen_data = pd.DataFrame(gen_data).to_csv('generateddata2.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}