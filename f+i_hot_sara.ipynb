{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "f+i_hot sara",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRPVSpsqOQkl"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPGPSRWTmYG8",
        "outputId": "e83f8ec9-5757-4f88-c08e-b656714a25f3"
      },
      "source": [
        " \n",
        "import tensorflow as tf \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
        "from tensorflow.keras import layers \n",
        "from tensorflow.keras import Model \n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import regularizers, optimizers\n",
        "import numpy as np\n",
        "from tensorflow.keras import optimizers\n",
        "import tensorflow.keras as keras\n",
        "from sklearn import preprocessing\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "wNH8NN-RCJes",
        "outputId": "810c1a5d-95ec-4608-88cb-0f195a4b320a"
      },
      "source": [
        "!pip install kaggle\n",
        "!pip install --upgrade kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c uw-cs480-fall20\n",
        "!ls\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('uw-cs480-fall20.zip', 'r')\n",
        "zip_ref.extractall('files')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.9)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.11.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Collecting kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/33/365c0d13f07a2a54744d027fe20b60dacdfdfb33bc04746db6ad0b79340b/kaggle-1.5.10.tar.gz (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.11.8)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.10-cp36-none-any.whl size=73269 sha256=22d8e6edfb6e4d124fe2b1b25bec832705958d25729f531c589c154571730c4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/d1/7e/6ce09b72b770149802c653a02783821629146983ee5a360f10\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Found existing installation: kaggle 1.5.9\n",
            "    Uninstalling kaggle-1.5.9:\n",
            "      Successfully uninstalled kaggle-1.5.9\n",
            "Successfully installed kaggle-1.5.10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e451ed12-0c61-48ff-96be-c03e191df97d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e451ed12-0c61-48ff-96be-c03e191df97d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading uw-cs480-fall20.zip to /content\n",
            " 95% 81.0M/85.6M [00:06<00:00, 9.72MB/s]\n",
            "100% 85.6M/85.6M [00:06<00:00, 14.6MB/s]\n",
            "drive  kaggle.json  sample_data  uw-cs480-fall20.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9KjS-ShOT3Q"
      },
      "source": [
        "# Data Upload"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGnSEbvrl2EP"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('/content/files/train.csv',dtype=str)\n",
        "test = pd.read_csv('/content/files/test.csv',dtype=str)\n",
        "# train=train.reset_index(drop=True)\n",
        "# test=test.reset_index(drop=True)\n",
        "def append_ext(fn):\n",
        "    return fn+\".jpg\"\n",
        "train[\"id\"]=train[\"id\"].apply(append_ext)\n",
        "test[\"id\"]=test[\"id\"].apply(append_ext)   \n",
        "train_folder = '/content/files/suffled-images/shuffled-images'\n",
        "test_folder = '/content/files/suffled-images/shuffled-images'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxVD9nodF21u",
        "outputId": "3743523e-4c55-4243-e046-b7b9bd4dbeeb"
      },
      "source": [
        "def feature2Num (train,cat):\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  ax=train[cat]\n",
        "  ax2=pd.Categorical(ax)\n",
        "  le.fit(ax2)\n",
        "  ax3 = le.transform(ax2)\n",
        "  return pd.get_dummies(ax3).to_numpy()\n",
        "\n",
        "gender = feature2Num(train,\"gender\")\n",
        "\n",
        "print(gender.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21627, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EzqNmqCGreK",
        "outputId": "762c98f5-496e-4fa0-8979-e2022c502231"
      },
      "source": [
        "gender = feature2Num(train,\"gender\")\n",
        "baseColour = feature2Num(train,\"baseColour\")\n",
        "season = feature2Num(train,\"season\")\n",
        "usage = feature2Num(train,\"usage\")\n",
        "feature_train = np.concatenate((gender, baseColour, season, usage), axis=1)\n",
        "feature_train_lable = feature2Num(train,\"category\")\n",
        "print(feature_train.shape)\n",
        "\n",
        "gender = feature2Num(test,\"gender\")\n",
        "baseColour = feature2Num(test,\"baseColour\")\n",
        "season = feature2Num(test,\"season\")\n",
        "usage = feature2Num(test,\"usage\")\n",
        "feature_test = np.concatenate((gender, baseColour, season, usage), axis=1)\n",
        "print(feature_test.shape)\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21627, 62)\n",
            "(21628, 62)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fgO8A9G1OCa"
      },
      "source": [
        "model2 = keras.models.load_model('/content/drive/MyDrive/my_model_92.h2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eZRKvJw1O7X",
        "outputId": "f183b01a-84d6-4d63-90a4-7fa230687b4c"
      },
      "source": [
        "# Define your data generator\n",
        "train_gen = ImageDataGenerator(rescale=1./255)\n",
        "test_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "X_train_img = train_gen.flow_from_dataframe(dataframe = train, \n",
        "directory = train_folder, x_col = 'id', \n",
        "y_col = 'category', subset=\"training\", seed = 42,\n",
        "batch_size = 100, shuffle = False, \n",
        "class_mode=\"categorical\",target_size = (80, 60))\n",
        "############################################################\n",
        "X_test_img = test_gen.flow_from_dataframe(dataframe = test, \n",
        "directory = test_folder, x_col = 'id', \n",
        "y_col = None,\n",
        "batch_size = 100, shuffle = False, \n",
        "class_mode=None,target_size = (80, 60))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 21627 validated image filenames belonging to 27 classes.\n",
            "Found 21628 validated image filenames.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s5-_Kd7028v",
        "outputId": "91bb6bdc-e51b-467b-b563-f64ca1bfe0d4"
      },
      "source": [
        "vgg_out_train = model2.predict_proba(X_train_img,verbose=1)\n",
        "vgg_out_test = model2.predict_proba(X_test_img,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-8-097058872837>:1: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use `model.predict()` instead.\n",
            "  1/217 [..............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_predict_batch_end` time: 0.0227s). Check your callbacks.\n",
            "217/217 [==============================] - 9s 40ms/step\n",
            "217/217 [==============================] - 8s 39ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuJBNnJl7q3h",
        "outputId": "f88b1b12-6eca-4a4f-d2c3-02449aab0a74"
      },
      "source": [
        "ax = model2.predict_proba(X_train_img,verbose=1)\n",
        "print(ax)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "217/217 [==============================] - 8s 37ms/step\n",
            "[[3.7275952e-14 3.0140463e-09 4.1152384e-06 ... 9.8850548e-01\n",
            "  2.4767660e-14 5.1111118e-07]\n",
            " [1.5650393e-12 3.5764143e-13 5.8651779e-07 ... 1.3389703e-07\n",
            "  2.5506983e-09 7.5390270e-11]\n",
            " [1.7618684e-18 8.7691909e-11 2.9406604e-09 ... 9.9999309e-01\n",
            "  6.1342291e-17 7.9800840e-11]\n",
            " ...\n",
            " [4.6418008e-23 2.6785230e-34 1.7165532e-13 ... 1.9308075e-14\n",
            "  2.6774555e-15 1.6746539e-21]\n",
            " [7.2769013e-09 2.3566233e-08 3.3816668e-05 ... 2.7687651e-05\n",
            "  8.0381500e-11 9.9887234e-01]\n",
            " [1.1525995e-25 1.3068848e-16 1.0256029e-10 ... 1.0000000e+00\n",
            "  4.9314772e-22 3.6345747e-13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC3j0Ov_vlRA",
        "outputId": "5d4044d2-0ce1-4840-c1dc-e647f35a847d"
      },
      "source": [
        "sentences = train['noisyTextDescription'].values\n",
        "labels = train['category'].values\n",
        "# print(labels)\n",
        "sentences_train = train['noisyTextDescription'].values\n",
        "sentences_test  = test['noisyTextDescription'].values\n",
        "y_train = train['category'].values\n",
        "print(sentences_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Femella Women Ankle-Length Grey AQ-S800WD-1EVDFAD170'\n",
            " 'Converse Unisex Casual Skirts Slipper' 'Velia Women Acetone Kurta' ...\n",
            " 'Rocia Women R348 Flats' 'Fastrack Men Black Amethyst Watch'\n",
            " 'Arrow Ghicha Men White Slim Surf Shirt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnggV65Otkuw"
      },
      "source": [
        "model3 = keras.models.load_model('/content/drive/MyDrive/MLP.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "id": "wQPd67ZZtwMu",
        "outputId": "02e68321-799c-4d09-e3fa-c6e7f7e332ab"
      },
      "source": [
        "text_out_train = model3.predict_proba(sentences_train,verbose=1)\n",
        "print(text_out_train[0])\n",
        "\n",
        "text_out_test = model3.predict_proba(sentences_test,verbose=1)\n",
        "print(text_out_test[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 30) for input Tensor(\"embedding_2_input:0\", shape=(None, 30), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-742df75e9da3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtext_out_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_out_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mprobability\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \"\"\"\n\u001b[0;32m--> 423\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m       logging.warning('Network returning invalid probability values. '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3208\u001b[0m           \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 3210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3142\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3143\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1418 predict_step\n        return self(x, training=False)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:372 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:386 call\n        inputs, training=training, mask=mask)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:216 assert_input_compatibility\n        ' but received input with shape ' + str(shape))\n\n    ValueError: Input 0 of layer dense_10 is incompatible with the layer: expected axis -1 of input shape to have value 4500 but received input with shape [None, 150]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9sKkQxx1iXg",
        "outputId": "54c1aaff-f960-4ef6-e15a-724336277e3c"
      },
      "source": [
        "print(vgg_out_train.shape)\n",
        "print(feature_train.T.shape)\n",
        "print(vgg_out_test.shape)\n",
        "print(feature_test.T.shape)\n",
        "X_train_conc=np.concatenate((vgg_out_train,feature_train), axis=1)\n",
        "X_test_conc=np.concatenate((vgg_out_test,feature_test),axis=1)\n",
        "print(X_train_conc.shape)\n",
        "print(X_test_conc.shape)\n",
        "print(feature_train_lable.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21627, 27)\n",
            "(62, 21627)\n",
            "(21628, 27)\n",
            "(62, 21628)\n",
            "(21627, 89)\n",
            "(21628, 89)\n",
            "(21627, 27)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf_QknVT0yUT",
        "outputId": "0d693e43-89b1-42cd-f642-f52075c1e0ea"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_conc, feature_train_lable, test_size=0.05, random_state=0)\n",
        "# y_train = pd.get_dummies(y_train).to_numpy()\n",
        "# y_val = pd.get_dummies(y_val).to_numpy()\n",
        "X_test = X_test_conc;\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20545, 89)\n",
            "(1082, 89)\n",
            "(20545, 27)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URI5aFISbmgT",
        "outputId": "c9c00d15-d8af-4be2-8141-e037da41d523"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(1024, input_dim=X_train.shape[1], activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512, input_dim=X_train.shape[1], activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(27))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200, batch_size=50, verbose=1)\n",
        "# evaluate the keras model\n",
        "_, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.5975 - accuracy: 0.8599 - val_loss: 0.1633 - val_accuracy: 0.9649\n",
            "Epoch 2/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.2042 - accuracy: 0.9580 - val_loss: 0.1378 - val_accuracy: 0.9704\n",
            "Epoch 3/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.1671 - accuracy: 0.9660 - val_loss: 0.1327 - val_accuracy: 0.9695\n",
            "Epoch 4/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.1481 - accuracy: 0.9690 - val_loss: 0.1224 - val_accuracy: 0.9732\n",
            "Epoch 5/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.1380 - accuracy: 0.9702 - val_loss: 0.1241 - val_accuracy: 0.9723\n",
            "Epoch 6/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.1276 - accuracy: 0.9724 - val_loss: 0.1269 - val_accuracy: 0.9741\n",
            "Epoch 7/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.1213 - accuracy: 0.9733 - val_loss: 0.1296 - val_accuracy: 0.9732\n",
            "Epoch 8/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.1184 - accuracy: 0.9727 - val_loss: 0.1233 - val_accuracy: 0.9723\n",
            "Epoch 9/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.1091 - accuracy: 0.9739 - val_loss: 0.1350 - val_accuracy: 0.9750\n",
            "Epoch 10/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.1042 - accuracy: 0.9748 - val_loss: 0.1284 - val_accuracy: 0.9741\n",
            "Epoch 11/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.1033 - accuracy: 0.9758 - val_loss: 0.1275 - val_accuracy: 0.9723\n",
            "Epoch 12/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.1024 - accuracy: 0.9761 - val_loss: 0.1362 - val_accuracy: 0.9741\n",
            "Epoch 13/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0963 - accuracy: 0.9773 - val_loss: 0.1341 - val_accuracy: 0.9760\n",
            "Epoch 14/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0950 - accuracy: 0.9770 - val_loss: 0.1317 - val_accuracy: 0.9741\n",
            "Epoch 15/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0922 - accuracy: 0.9772 - val_loss: 0.1461 - val_accuracy: 0.9750\n",
            "Epoch 16/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0930 - accuracy: 0.9772 - val_loss: 0.1407 - val_accuracy: 0.9732\n",
            "Epoch 17/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0881 - accuracy: 0.9775 - val_loss: 0.1393 - val_accuracy: 0.9760\n",
            "Epoch 18/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0846 - accuracy: 0.9776 - val_loss: 0.1526 - val_accuracy: 0.9713\n",
            "Epoch 19/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0861 - accuracy: 0.9777 - val_loss: 0.1468 - val_accuracy: 0.9741\n",
            "Epoch 20/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0829 - accuracy: 0.9789 - val_loss: 0.1529 - val_accuracy: 0.9723\n",
            "Epoch 21/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0808 - accuracy: 0.9796 - val_loss: 0.1518 - val_accuracy: 0.9760\n",
            "Epoch 22/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0795 - accuracy: 0.9798 - val_loss: 0.1579 - val_accuracy: 0.9732\n",
            "Epoch 23/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0801 - accuracy: 0.9791 - val_loss: 0.1656 - val_accuracy: 0.9713\n",
            "Epoch 24/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0757 - accuracy: 0.9797 - val_loss: 0.1736 - val_accuracy: 0.9732\n",
            "Epoch 25/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0796 - accuracy: 0.9803 - val_loss: 0.1635 - val_accuracy: 0.9723\n",
            "Epoch 26/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0754 - accuracy: 0.9810 - val_loss: 0.1633 - val_accuracy: 0.9713\n",
            "Epoch 27/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0765 - accuracy: 0.9797 - val_loss: 0.1507 - val_accuracy: 0.9723\n",
            "Epoch 28/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0746 - accuracy: 0.9803 - val_loss: 0.1658 - val_accuracy: 0.9750\n",
            "Epoch 29/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0750 - accuracy: 0.9802 - val_loss: 0.1741 - val_accuracy: 0.9750\n",
            "Epoch 30/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0737 - accuracy: 0.9810 - val_loss: 0.1649 - val_accuracy: 0.9723\n",
            "Epoch 31/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0701 - accuracy: 0.9811 - val_loss: 0.1895 - val_accuracy: 0.9713\n",
            "Epoch 32/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0677 - accuracy: 0.9822 - val_loss: 0.1739 - val_accuracy: 0.9741\n",
            "Epoch 33/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0675 - accuracy: 0.9818 - val_loss: 0.2049 - val_accuracy: 0.9732\n",
            "Epoch 34/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0676 - accuracy: 0.9810 - val_loss: 0.1782 - val_accuracy: 0.9704\n",
            "Epoch 35/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0658 - accuracy: 0.9815 - val_loss: 0.1924 - val_accuracy: 0.9723\n",
            "Epoch 36/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0695 - accuracy: 0.9805 - val_loss: 0.1862 - val_accuracy: 0.9741\n",
            "Epoch 37/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0676 - accuracy: 0.9819 - val_loss: 0.1883 - val_accuracy: 0.9723\n",
            "Epoch 38/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0672 - accuracy: 0.9819 - val_loss: 0.2004 - val_accuracy: 0.9741\n",
            "Epoch 39/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0651 - accuracy: 0.9818 - val_loss: 0.1900 - val_accuracy: 0.9723\n",
            "Epoch 40/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0642 - accuracy: 0.9822 - val_loss: 0.1867 - val_accuracy: 0.9713\n",
            "Epoch 41/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0638 - accuracy: 0.9817 - val_loss: 0.1902 - val_accuracy: 0.9723\n",
            "Epoch 42/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0677 - accuracy: 0.9818 - val_loss: 0.1990 - val_accuracy: 0.9732\n",
            "Epoch 43/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0619 - accuracy: 0.9828 - val_loss: 0.1898 - val_accuracy: 0.9732\n",
            "Epoch 44/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0639 - accuracy: 0.9819 - val_loss: 0.1945 - val_accuracy: 0.9750\n",
            "Epoch 45/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0609 - accuracy: 0.9827 - val_loss: 0.2024 - val_accuracy: 0.9723\n",
            "Epoch 46/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0619 - accuracy: 0.9822 - val_loss: 0.2079 - val_accuracy: 0.9769\n",
            "Epoch 47/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0644 - accuracy: 0.9816 - val_loss: 0.1928 - val_accuracy: 0.9704\n",
            "Epoch 48/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0607 - accuracy: 0.9830 - val_loss: 0.2229 - val_accuracy: 0.9713\n",
            "Epoch 49/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0609 - accuracy: 0.9823 - val_loss: 0.2213 - val_accuracy: 0.9704\n",
            "Epoch 50/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0607 - accuracy: 0.9822 - val_loss: 0.2195 - val_accuracy: 0.9732\n",
            "Epoch 51/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0619 - accuracy: 0.9828 - val_loss: 0.2255 - val_accuracy: 0.9741\n",
            "Epoch 52/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0607 - accuracy: 0.9829 - val_loss: 0.2367 - val_accuracy: 0.9732\n",
            "Epoch 53/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0597 - accuracy: 0.9831 - val_loss: 0.2438 - val_accuracy: 0.9695\n",
            "Epoch 54/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0607 - accuracy: 0.9819 - val_loss: 0.2397 - val_accuracy: 0.9732\n",
            "Epoch 55/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0608 - accuracy: 0.9822 - val_loss: 0.2296 - val_accuracy: 0.9732\n",
            "Epoch 56/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0544 - accuracy: 0.9843 - val_loss: 0.2305 - val_accuracy: 0.9723\n",
            "Epoch 57/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0594 - accuracy: 0.9831 - val_loss: 0.2547 - val_accuracy: 0.9713\n",
            "Epoch 58/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0559 - accuracy: 0.9835 - val_loss: 0.2441 - val_accuracy: 0.9732\n",
            "Epoch 59/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0616 - accuracy: 0.9823 - val_loss: 0.2507 - val_accuracy: 0.9732\n",
            "Epoch 60/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0571 - accuracy: 0.9840 - val_loss: 0.2369 - val_accuracy: 0.9732\n",
            "Epoch 61/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0565 - accuracy: 0.9834 - val_loss: 0.2369 - val_accuracy: 0.9686\n",
            "Epoch 62/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0580 - accuracy: 0.9833 - val_loss: 0.2304 - val_accuracy: 0.9723\n",
            "Epoch 63/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0575 - accuracy: 0.9832 - val_loss: 0.2573 - val_accuracy: 0.9723\n",
            "Epoch 64/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0604 - accuracy: 0.9843 - val_loss: 0.2576 - val_accuracy: 0.9732\n",
            "Epoch 65/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0580 - accuracy: 0.9831 - val_loss: 0.2431 - val_accuracy: 0.9695\n",
            "Epoch 66/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0550 - accuracy: 0.9835 - val_loss: 0.2483 - val_accuracy: 0.9713\n",
            "Epoch 67/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0542 - accuracy: 0.9839 - val_loss: 0.2843 - val_accuracy: 0.9750\n",
            "Epoch 68/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.2645 - val_accuracy: 0.9695\n",
            "Epoch 69/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0561 - accuracy: 0.9834 - val_loss: 0.2559 - val_accuracy: 0.9695\n",
            "Epoch 70/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0551 - accuracy: 0.9841 - val_loss: 0.2566 - val_accuracy: 0.9704\n",
            "Epoch 71/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0564 - accuracy: 0.9849 - val_loss: 0.2566 - val_accuracy: 0.9723\n",
            "Epoch 72/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0553 - accuracy: 0.9832 - val_loss: 0.2519 - val_accuracy: 0.9713\n",
            "Epoch 73/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0560 - accuracy: 0.9837 - val_loss: 0.2824 - val_accuracy: 0.9723\n",
            "Epoch 74/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0570 - accuracy: 0.9842 - val_loss: 0.2883 - val_accuracy: 0.9704\n",
            "Epoch 75/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0565 - accuracy: 0.9843 - val_loss: 0.2916 - val_accuracy: 0.9723\n",
            "Epoch 76/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0565 - accuracy: 0.9837 - val_loss: 0.2873 - val_accuracy: 0.9695\n",
            "Epoch 77/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0546 - accuracy: 0.9847 - val_loss: 0.3030 - val_accuracy: 0.9695\n",
            "Epoch 78/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0589 - accuracy: 0.9836 - val_loss: 0.2787 - val_accuracy: 0.9713\n",
            "Epoch 79/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0563 - accuracy: 0.9845 - val_loss: 0.2778 - val_accuracy: 0.9713\n",
            "Epoch 80/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0553 - accuracy: 0.9832 - val_loss: 0.2727 - val_accuracy: 0.9723\n",
            "Epoch 81/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0551 - accuracy: 0.9843 - val_loss: 0.2746 - val_accuracy: 0.9732\n",
            "Epoch 82/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0514 - accuracy: 0.9846 - val_loss: 0.2794 - val_accuracy: 0.9723\n",
            "Epoch 83/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0561 - accuracy: 0.9833 - val_loss: 0.2913 - val_accuracy: 0.9713\n",
            "Epoch 84/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0561 - accuracy: 0.9836 - val_loss: 0.2580 - val_accuracy: 0.9686\n",
            "Epoch 85/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0558 - accuracy: 0.9839 - val_loss: 0.2642 - val_accuracy: 0.9695\n",
            "Epoch 86/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0537 - accuracy: 0.9835 - val_loss: 0.2742 - val_accuracy: 0.9741\n",
            "Epoch 87/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0528 - accuracy: 0.9835 - val_loss: 0.2709 - val_accuracy: 0.9732\n",
            "Epoch 88/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0562 - accuracy: 0.9830 - val_loss: 0.2834 - val_accuracy: 0.9741\n",
            "Epoch 89/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0565 - accuracy: 0.9839 - val_loss: 0.2726 - val_accuracy: 0.9723\n",
            "Epoch 90/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0519 - accuracy: 0.9854 - val_loss: 0.2837 - val_accuracy: 0.9713\n",
            "Epoch 91/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0516 - accuracy: 0.9845 - val_loss: 0.2928 - val_accuracy: 0.9723\n",
            "Epoch 92/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0519 - accuracy: 0.9847 - val_loss: 0.2975 - val_accuracy: 0.9704\n",
            "Epoch 93/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0562 - accuracy: 0.9841 - val_loss: 0.2948 - val_accuracy: 0.9713\n",
            "Epoch 94/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0541 - accuracy: 0.9847 - val_loss: 0.3119 - val_accuracy: 0.9732\n",
            "Epoch 95/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0559 - accuracy: 0.9840 - val_loss: 0.3010 - val_accuracy: 0.9723\n",
            "Epoch 96/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0533 - accuracy: 0.9830 - val_loss: 0.3016 - val_accuracy: 0.9723\n",
            "Epoch 97/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0536 - accuracy: 0.9841 - val_loss: 0.2831 - val_accuracy: 0.9704\n",
            "Epoch 98/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0517 - accuracy: 0.9849 - val_loss: 0.3082 - val_accuracy: 0.9704\n",
            "Epoch 99/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0529 - accuracy: 0.9849 - val_loss: 0.3152 - val_accuracy: 0.9695\n",
            "Epoch 100/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0556 - accuracy: 0.9836 - val_loss: 0.3018 - val_accuracy: 0.9732\n",
            "Epoch 101/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0498 - accuracy: 0.9851 - val_loss: 0.3153 - val_accuracy: 0.9713\n",
            "Epoch 102/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0506 - accuracy: 0.9856 - val_loss: 0.3081 - val_accuracy: 0.9723\n",
            "Epoch 103/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0511 - accuracy: 0.9844 - val_loss: 0.3153 - val_accuracy: 0.9713\n",
            "Epoch 104/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0524 - accuracy: 0.9847 - val_loss: 0.3205 - val_accuracy: 0.9723\n",
            "Epoch 105/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0542 - accuracy: 0.9840 - val_loss: 0.3144 - val_accuracy: 0.9741\n",
            "Epoch 106/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0534 - accuracy: 0.9837 - val_loss: 0.3251 - val_accuracy: 0.9732\n",
            "Epoch 107/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0553 - accuracy: 0.9852 - val_loss: 0.2996 - val_accuracy: 0.9704\n",
            "Epoch 108/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0531 - accuracy: 0.9847 - val_loss: 0.2970 - val_accuracy: 0.9713\n",
            "Epoch 109/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0512 - accuracy: 0.9856 - val_loss: 0.3241 - val_accuracy: 0.9750\n",
            "Epoch 110/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0476 - accuracy: 0.9856 - val_loss: 0.3447 - val_accuracy: 0.9732\n",
            "Epoch 111/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0535 - accuracy: 0.9848 - val_loss: 0.3218 - val_accuracy: 0.9704\n",
            "Epoch 112/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0501 - accuracy: 0.9846 - val_loss: 0.3214 - val_accuracy: 0.9713\n",
            "Epoch 113/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0517 - accuracy: 0.9844 - val_loss: 0.3293 - val_accuracy: 0.9750\n",
            "Epoch 114/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0513 - accuracy: 0.9851 - val_loss: 0.3087 - val_accuracy: 0.9686\n",
            "Epoch 115/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0546 - accuracy: 0.9844 - val_loss: 0.3507 - val_accuracy: 0.9704\n",
            "Epoch 116/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0569 - accuracy: 0.9850 - val_loss: 0.3172 - val_accuracy: 0.9732\n",
            "Epoch 117/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0493 - accuracy: 0.9853 - val_loss: 0.3116 - val_accuracy: 0.9704\n",
            "Epoch 118/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0544 - accuracy: 0.9836 - val_loss: 0.3217 - val_accuracy: 0.9723\n",
            "Epoch 119/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0549 - accuracy: 0.9841 - val_loss: 0.3053 - val_accuracy: 0.9760\n",
            "Epoch 120/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0513 - accuracy: 0.9850 - val_loss: 0.3329 - val_accuracy: 0.9732\n",
            "Epoch 121/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0504 - accuracy: 0.9851 - val_loss: 0.3367 - val_accuracy: 0.9713\n",
            "Epoch 122/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0510 - accuracy: 0.9856 - val_loss: 0.3169 - val_accuracy: 0.9741\n",
            "Epoch 123/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0543 - accuracy: 0.9842 - val_loss: 0.3221 - val_accuracy: 0.9695\n",
            "Epoch 124/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0487 - accuracy: 0.9849 - val_loss: 0.3241 - val_accuracy: 0.9723\n",
            "Epoch 125/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0500 - accuracy: 0.9846 - val_loss: 0.3470 - val_accuracy: 0.9704\n",
            "Epoch 126/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0483 - accuracy: 0.9853 - val_loss: 0.3349 - val_accuracy: 0.9695\n",
            "Epoch 127/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0548 - accuracy: 0.9845 - val_loss: 0.3308 - val_accuracy: 0.9695\n",
            "Epoch 128/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0526 - accuracy: 0.9860 - val_loss: 0.3318 - val_accuracy: 0.9704\n",
            "Epoch 129/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0496 - accuracy: 0.9854 - val_loss: 0.3324 - val_accuracy: 0.9695\n",
            "Epoch 130/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0524 - accuracy: 0.9852 - val_loss: 0.3516 - val_accuracy: 0.9677\n",
            "Epoch 131/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0517 - accuracy: 0.9852 - val_loss: 0.3547 - val_accuracy: 0.9704\n",
            "Epoch 132/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0522 - accuracy: 0.9846 - val_loss: 0.3906 - val_accuracy: 0.9667\n",
            "Epoch 133/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0501 - accuracy: 0.9861 - val_loss: 0.3765 - val_accuracy: 0.9695\n",
            "Epoch 134/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0521 - accuracy: 0.9853 - val_loss: 0.3586 - val_accuracy: 0.9686\n",
            "Epoch 135/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0510 - accuracy: 0.9854 - val_loss: 0.3571 - val_accuracy: 0.9695\n",
            "Epoch 136/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0504 - accuracy: 0.9841 - val_loss: 0.3605 - val_accuracy: 0.9713\n",
            "Epoch 137/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0486 - accuracy: 0.9854 - val_loss: 0.3846 - val_accuracy: 0.9704\n",
            "Epoch 138/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0544 - accuracy: 0.9851 - val_loss: 0.3761 - val_accuracy: 0.9704\n",
            "Epoch 139/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0520 - accuracy: 0.9851 - val_loss: 0.3689 - val_accuracy: 0.9723\n",
            "Epoch 140/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0510 - accuracy: 0.9843 - val_loss: 0.3757 - val_accuracy: 0.9695\n",
            "Epoch 141/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0485 - accuracy: 0.9857 - val_loss: 0.3964 - val_accuracy: 0.9741\n",
            "Epoch 142/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0512 - accuracy: 0.9847 - val_loss: 0.3629 - val_accuracy: 0.9713\n",
            "Epoch 143/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0488 - accuracy: 0.9860 - val_loss: 0.3555 - val_accuracy: 0.9704\n",
            "Epoch 144/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0546 - accuracy: 0.9852 - val_loss: 0.3652 - val_accuracy: 0.9713\n",
            "Epoch 145/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0540 - accuracy: 0.9848 - val_loss: 0.3511 - val_accuracy: 0.9723\n",
            "Epoch 146/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0559 - accuracy: 0.9846 - val_loss: 0.3411 - val_accuracy: 0.9723\n",
            "Epoch 147/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0505 - accuracy: 0.9858 - val_loss: 0.3431 - val_accuracy: 0.9695\n",
            "Epoch 148/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0542 - accuracy: 0.9848 - val_loss: 0.3410 - val_accuracy: 0.9723\n",
            "Epoch 149/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0558 - accuracy: 0.9847 - val_loss: 0.3328 - val_accuracy: 0.9695\n",
            "Epoch 150/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0475 - accuracy: 0.9854 - val_loss: 0.3672 - val_accuracy: 0.9704\n",
            "Epoch 151/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0493 - accuracy: 0.9855 - val_loss: 0.3935 - val_accuracy: 0.9704\n",
            "Epoch 152/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0477 - accuracy: 0.9863 - val_loss: 0.3982 - val_accuracy: 0.9723\n",
            "Epoch 153/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0534 - accuracy: 0.9848 - val_loss: 0.3864 - val_accuracy: 0.9695\n",
            "Epoch 154/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0514 - accuracy: 0.9862 - val_loss: 0.4062 - val_accuracy: 0.9713\n",
            "Epoch 155/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0499 - accuracy: 0.9858 - val_loss: 0.3863 - val_accuracy: 0.9704\n",
            "Epoch 156/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0478 - accuracy: 0.9856 - val_loss: 0.4140 - val_accuracy: 0.9677\n",
            "Epoch 157/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0477 - accuracy: 0.9851 - val_loss: 0.4084 - val_accuracy: 0.9686\n",
            "Epoch 158/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0518 - accuracy: 0.9850 - val_loss: 0.4123 - val_accuracy: 0.9695\n",
            "Epoch 159/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0504 - accuracy: 0.9861 - val_loss: 0.4077 - val_accuracy: 0.9713\n",
            "Epoch 160/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0552 - accuracy: 0.9851 - val_loss: 0.4032 - val_accuracy: 0.9686\n",
            "Epoch 161/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0540 - accuracy: 0.9845 - val_loss: 0.3865 - val_accuracy: 0.9704\n",
            "Epoch 162/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0506 - accuracy: 0.9853 - val_loss: 0.4131 - val_accuracy: 0.9695\n",
            "Epoch 163/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0529 - accuracy: 0.9854 - val_loss: 0.3925 - val_accuracy: 0.9704\n",
            "Epoch 164/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0514 - accuracy: 0.9859 - val_loss: 0.4201 - val_accuracy: 0.9704\n",
            "Epoch 165/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0494 - accuracy: 0.9858 - val_loss: 0.3927 - val_accuracy: 0.9677\n",
            "Epoch 166/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0475 - accuracy: 0.9854 - val_loss: 0.3787 - val_accuracy: 0.9686\n",
            "Epoch 167/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0503 - accuracy: 0.9856 - val_loss: 0.3985 - val_accuracy: 0.9704\n",
            "Epoch 168/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0515 - accuracy: 0.9862 - val_loss: 0.4436 - val_accuracy: 0.9686\n",
            "Epoch 169/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0532 - accuracy: 0.9852 - val_loss: 0.4524 - val_accuracy: 0.9695\n",
            "Epoch 170/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0506 - accuracy: 0.9850 - val_loss: 0.4223 - val_accuracy: 0.9704\n",
            "Epoch 171/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0468 - accuracy: 0.9864 - val_loss: 0.4304 - val_accuracy: 0.9713\n",
            "Epoch 172/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0432 - accuracy: 0.9867 - val_loss: 0.4375 - val_accuracy: 0.9741\n",
            "Epoch 173/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0521 - accuracy: 0.9850 - val_loss: 0.4445 - val_accuracy: 0.9732\n",
            "Epoch 174/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0566 - accuracy: 0.9852 - val_loss: 0.3970 - val_accuracy: 0.9723\n",
            "Epoch 175/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0500 - accuracy: 0.9854 - val_loss: 0.4392 - val_accuracy: 0.9695\n",
            "Epoch 176/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0527 - accuracy: 0.9847 - val_loss: 0.4405 - val_accuracy: 0.9667\n",
            "Epoch 177/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0455 - accuracy: 0.9859 - val_loss: 0.4748 - val_accuracy: 0.9695\n",
            "Epoch 178/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0494 - accuracy: 0.9856 - val_loss: 0.4425 - val_accuracy: 0.9704\n",
            "Epoch 179/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0525 - accuracy: 0.9847 - val_loss: 0.4560 - val_accuracy: 0.9686\n",
            "Epoch 180/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0544 - accuracy: 0.9853 - val_loss: 0.4178 - val_accuracy: 0.9686\n",
            "Epoch 181/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0571 - accuracy: 0.9846 - val_loss: 0.4174 - val_accuracy: 0.9704\n",
            "Epoch 182/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0529 - accuracy: 0.9847 - val_loss: 0.4237 - val_accuracy: 0.9695\n",
            "Epoch 183/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0511 - accuracy: 0.9854 - val_loss: 0.4552 - val_accuracy: 0.9704\n",
            "Epoch 184/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0470 - accuracy: 0.9854 - val_loss: 0.4403 - val_accuracy: 0.9686\n",
            "Epoch 185/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0485 - accuracy: 0.9857 - val_loss: 0.4889 - val_accuracy: 0.9723\n",
            "Epoch 186/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0530 - accuracy: 0.9856 - val_loss: 0.4518 - val_accuracy: 0.9695\n",
            "Epoch 187/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0532 - accuracy: 0.9852 - val_loss: 0.4631 - val_accuracy: 0.9704\n",
            "Epoch 188/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0457 - accuracy: 0.9865 - val_loss: 0.4453 - val_accuracy: 0.9723\n",
            "Epoch 189/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0520 - accuracy: 0.9853 - val_loss: 0.4909 - val_accuracy: 0.9723\n",
            "Epoch 190/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0498 - accuracy: 0.9860 - val_loss: 0.4873 - val_accuracy: 0.9704\n",
            "Epoch 191/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0477 - accuracy: 0.9873 - val_loss: 0.4989 - val_accuracy: 0.9704\n",
            "Epoch 192/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0486 - accuracy: 0.9855 - val_loss: 0.4866 - val_accuracy: 0.9723\n",
            "Epoch 193/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0452 - accuracy: 0.9865 - val_loss: 0.4705 - val_accuracy: 0.9723\n",
            "Epoch 194/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0477 - accuracy: 0.9867 - val_loss: 0.4421 - val_accuracy: 0.9723\n",
            "Epoch 195/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0478 - accuracy: 0.9862 - val_loss: 0.4362 - val_accuracy: 0.9723\n",
            "Epoch 196/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0502 - accuracy: 0.9855 - val_loss: 0.4406 - val_accuracy: 0.9695\n",
            "Epoch 197/200\n",
            "411/411 [==============================] - 1s 3ms/step - loss: 0.0535 - accuracy: 0.9854 - val_loss: 0.4949 - val_accuracy: 0.9713\n",
            "Epoch 198/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0527 - accuracy: 0.9857 - val_loss: 0.4728 - val_accuracy: 0.9686\n",
            "Epoch 199/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0489 - accuracy: 0.9849 - val_loss: 0.4714 - val_accuracy: 0.9695\n",
            "Epoch 200/200\n",
            "411/411 [==============================] - 1s 2ms/step - loss: 0.0510 - accuracy: 0.9859 - val_loss: 0.4625 - val_accuracy: 0.9713\n",
            "Accuracy: 97.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hniSUsLJaJRp",
        "outputId": "09f09eb2-1665-44df-d5b1-184ad39add09"
      },
      "source": [
        "pred_vgg=model.predict_generator(X_test,verbose=1)\n",
        "predicted_class_indices_vgg=np.argmax(pred_vgg,axis=1)\n",
        "\n",
        "labels = (X_train_img.class_indices)\n",
        "labels = dict((v,k) for k,v in labels.items())\n",
        "predictions_vgg = [labels[k] for k in predicted_class_indices_vgg]\n",
        "filenames=X_test_img.filenames\n",
        "results_vgg=pd.DataFrame({\"id\":filenames,\"category\":predictions_vgg})\n",
        "results_vgg.to_csv(\"results_rmsp_vgg_hot.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "676/676 [==============================] - 1s 891us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhD-20d8Reb4"
      },
      "source": [
        "ax = model.save_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AKc--cpWwhy"
      },
      "source": [
        "model.save('my_model_91.h2')\n",
        "model2 = keras.models.load_model('my_model_91.h2')\n",
        "print(model2.predict_generator(test_data,verbose=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qKUN-SIUxnN"
      },
      "source": [
        "print(ax)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}