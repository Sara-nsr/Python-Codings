{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification_bootstrapping_Unbiased.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "z2iHOSOcc9tY",
        "outputId": "3009b358-fca8-457d-e992-2da23caebbd8"
      },
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
        "from tensorflow.keras import layers \n",
        "from tensorflow.keras import Model \n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import regularizers, optimizers\n",
        "import numpy as np\n",
        "from tensorflow.keras import optimizers\n",
        "import tensorflow.keras as keras\n",
        "from sklearn import preprocessing\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import RMSprop, SGD\n",
        "from tensorflow.keras import initializers\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aa17c802-f8f3-49d9-8f83-e2c51bba7c8a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-aa17c802-f8f3-49d9-8f83-e2c51bba7c8a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Round 3 Unbiased.csv to Round 3 Unbiased.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QjSPtZVdLTD"
      },
      "source": [
        "def load_knn_data():\n",
        "  X_train = np.genfromtxt('Round 3 Unbiased.csv', delimiter=',')\n",
        "  np.random.shuffle(X_train)\n",
        "  Y_train_S = X_train[:,6]\n",
        "  X_train_S = X_train[:,0:6]\n",
        "  return X_train_S, Y_train_S"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCt4ZKSD_FCN",
        "outputId": "1481fc52-1b6f-42c4-b330-e45c61878665"
      },
      "source": [
        "from numpy import asarray\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "X , Y = load_knn_data() \n",
        "data = asarray(X)\n",
        "# print(data)\n",
        "scaler =  MinMaxScaler()\n",
        "scaled = scaler.fit_transform(data)\n",
        "print(scaled)\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.24431079 0.27788784 0.56531578 0.57479249 0.84070796 0.93137255]\n",
            " [0.10853001 0.13316811 0.16334088 0.30405604 0.15929204 0.12745098]\n",
            " [0.56496765 0.76883366 0.86011109 0.78512981 0.90265487 0.97058824]\n",
            " ...\n",
            " [0.23745164 0.26958305 0.54309813 0.56313652 0.83185841 0.92156863]\n",
            " [0.19158318 0.21228696 0.40279778 0.47871902 0.44247788 0.58823529]\n",
            " [0.35015458 0.61313626 0.75375437 0.70194855 0.87610619 0.95098039]]\n",
            "[1. 0. 1. ... 1. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D34TRMj-elSh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "293c1c60-11e2-4ad6-d33f-47c273926839"
      },
      "source": [
        "X , Y = load_knn_data() \n",
        "print(Y)\n",
        "print(scaled)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 1. ... 0. 1. 1.]\n",
            "[[0.24431079 0.27788784 0.56531578 0.57479249 0.84070796 0.93137255]\n",
            " [0.10853001 0.13316811 0.16334088 0.30405604 0.15929204 0.12745098]\n",
            " [0.56496765 0.76883366 0.86011109 0.78512981 0.90265487 0.97058824]\n",
            " ...\n",
            " [0.23745164 0.26958305 0.54309813 0.56313652 0.83185841 0.92156863]\n",
            " [0.19158318 0.21228696 0.40279778 0.47871902 0.44247788 0.58823529]\n",
            " [0.35015458 0.61313626 0.75375437 0.70194855 0.87610619 0.95098039]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4XPVXYZdMKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f36b44d-c1b6-49d5-ab05-f4f9de0eba3b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(scaled, Y, test_size=0.1, random_state=0)\n",
        "print(X_train.shape)\n",
        "# print(X_val.shape)\n",
        "print(y_train)\n",
        "# print(X_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3482, 6)\n",
            "[1. 0. 0. ... 0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_FttGTIzke_",
        "outputId": "aac4020d-307b-481c-9338-dff538606f98"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim=6, activation='relu'))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "model.summary()\n",
        "# opt = keras.optimizers.RMSprop(lr=0.001, decay=1e-6)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit the keras model on the dataset\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=300, verbose=1)\n",
        "# evaluate the keras model\n",
        "_, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 10)                70        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 81\n",
            "Trainable params: 81\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "12/12 [==============================] - 1s 29ms/step - loss: 0.7373 - accuracy: 0.4977 - val_loss: 0.7190 - val_accuracy: 0.5375\n",
            "Epoch 2/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7142 - accuracy: 0.5053 - val_loss: 0.7069 - val_accuracy: 0.5530\n",
            "Epoch 3/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7021 - accuracy: 0.5097 - val_loss: 0.7028 - val_accuracy: 0.4910\n",
            "Epoch 4/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6980 - accuracy: 0.5015 - val_loss: 0.7018 - val_accuracy: 0.4780\n",
            "Epoch 5/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6963 - accuracy: 0.5069 - val_loss: 0.7016 - val_accuracy: 0.4677\n",
            "Epoch 6/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6966 - accuracy: 0.5064 - val_loss: 0.7014 - val_accuracy: 0.4522\n",
            "Epoch 7/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6974 - accuracy: 0.4940 - val_loss: 0.7009 - val_accuracy: 0.4625\n",
            "Epoch 8/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6969 - accuracy: 0.4976 - val_loss: 0.7005 - val_accuracy: 0.4806\n",
            "Epoch 9/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6957 - accuracy: 0.5035 - val_loss: 0.7003 - val_accuracy: 0.4806\n",
            "Epoch 10/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6951 - accuracy: 0.5051 - val_loss: 0.7001 - val_accuracy: 0.4755\n",
            "Epoch 11/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6950 - accuracy: 0.5071 - val_loss: 0.7000 - val_accuracy: 0.4755\n",
            "Epoch 12/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6981 - accuracy: 0.4834 - val_loss: 0.6997 - val_accuracy: 0.4755\n",
            "Epoch 13/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6975 - accuracy: 0.4907 - val_loss: 0.6996 - val_accuracy: 0.4703\n",
            "Epoch 14/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6960 - accuracy: 0.4960 - val_loss: 0.6993 - val_accuracy: 0.4755\n",
            "Epoch 15/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6958 - accuracy: 0.4954 - val_loss: 0.6993 - val_accuracy: 0.4677\n",
            "Epoch 16/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6969 - accuracy: 0.4885 - val_loss: 0.6990 - val_accuracy: 0.4729\n",
            "Epoch 17/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6954 - accuracy: 0.4970 - val_loss: 0.6990 - val_accuracy: 0.4599\n",
            "Epoch 18/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6942 - accuracy: 0.5037 - val_loss: 0.6990 - val_accuracy: 0.4625\n",
            "Epoch 19/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6961 - accuracy: 0.4936 - val_loss: 0.6986 - val_accuracy: 0.4677\n",
            "Epoch 20/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6943 - accuracy: 0.5066 - val_loss: 0.6986 - val_accuracy: 0.4625\n",
            "Epoch 21/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6938 - accuracy: 0.5072 - val_loss: 0.6984 - val_accuracy: 0.4651\n",
            "Epoch 22/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6943 - accuracy: 0.5070 - val_loss: 0.6983 - val_accuracy: 0.4625\n",
            "Epoch 23/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6934 - accuracy: 0.5114 - val_loss: 0.6983 - val_accuracy: 0.4574\n",
            "Epoch 24/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6945 - accuracy: 0.4989 - val_loss: 0.6982 - val_accuracy: 0.4574\n",
            "Epoch 25/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6941 - accuracy: 0.5001 - val_loss: 0.6980 - val_accuracy: 0.4599\n",
            "Epoch 26/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6947 - accuracy: 0.4968 - val_loss: 0.6980 - val_accuracy: 0.4574\n",
            "Epoch 27/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6940 - accuracy: 0.5031 - val_loss: 0.6979 - val_accuracy: 0.4574\n",
            "Epoch 28/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6945 - accuracy: 0.4981 - val_loss: 0.6977 - val_accuracy: 0.4574\n",
            "Epoch 29/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6951 - accuracy: 0.4932 - val_loss: 0.6977 - val_accuracy: 0.4599\n",
            "Epoch 30/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6946 - accuracy: 0.4963 - val_loss: 0.6976 - val_accuracy: 0.4599\n",
            "Epoch 31/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6947 - accuracy: 0.5003 - val_loss: 0.6976 - val_accuracy: 0.4599\n",
            "Epoch 32/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.5092 - val_loss: 0.6975 - val_accuracy: 0.4599\n",
            "Epoch 33/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5156 - val_loss: 0.6974 - val_accuracy: 0.4599\n",
            "Epoch 34/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5150 - val_loss: 0.6974 - val_accuracy: 0.4599\n",
            "Epoch 35/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6941 - accuracy: 0.4999 - val_loss: 0.6972 - val_accuracy: 0.4599\n",
            "Epoch 36/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6941 - accuracy: 0.4970 - val_loss: 0.6971 - val_accuracy: 0.4599\n",
            "Epoch 37/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6937 - accuracy: 0.5039 - val_loss: 0.6971 - val_accuracy: 0.4599\n",
            "Epoch 38/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6938 - accuracy: 0.5038 - val_loss: 0.6970 - val_accuracy: 0.4651\n",
            "Epoch 39/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.5072 - val_loss: 0.6971 - val_accuracy: 0.4599\n",
            "Epoch 40/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5307 - val_loss: 0.6970 - val_accuracy: 0.4599\n",
            "Epoch 41/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6942 - accuracy: 0.4963 - val_loss: 0.6969 - val_accuracy: 0.4599\n",
            "Epoch 42/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5042 - val_loss: 0.6970 - val_accuracy: 0.4599\n",
            "Epoch 43/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6943 - accuracy: 0.4933 - val_loss: 0.6969 - val_accuracy: 0.4625\n",
            "Epoch 44/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6938 - accuracy: 0.5049 - val_loss: 0.6968 - val_accuracy: 0.4651\n",
            "Epoch 45/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6926 - accuracy: 0.5131 - val_loss: 0.6968 - val_accuracy: 0.4651\n",
            "Epoch 46/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6940 - accuracy: 0.4997 - val_loss: 0.6968 - val_accuracy: 0.4625\n",
            "Epoch 47/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5153 - val_loss: 0.6967 - val_accuracy: 0.4651\n",
            "Epoch 48/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5127 - val_loss: 0.6966 - val_accuracy: 0.4677\n",
            "Epoch 49/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6937 - accuracy: 0.5019 - val_loss: 0.6965 - val_accuracy: 0.4703\n",
            "Epoch 50/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.5057 - val_loss: 0.6965 - val_accuracy: 0.4677\n",
            "Epoch 51/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6945 - accuracy: 0.4919 - val_loss: 0.6964 - val_accuracy: 0.4677\n",
            "Epoch 52/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5129 - val_loss: 0.6965 - val_accuracy: 0.4677\n",
            "Epoch 53/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5090 - val_loss: 0.6964 - val_accuracy: 0.4677\n",
            "Epoch 54/500\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.5054 - val_loss: 0.6963 - val_accuracy: 0.4677\n",
            "Epoch 55/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6939 - accuracy: 0.4986 - val_loss: 0.6963 - val_accuracy: 0.4677\n",
            "Epoch 56/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5025 - val_loss: 0.6963 - val_accuracy: 0.4651\n",
            "Epoch 57/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5065 - val_loss: 0.6961 - val_accuracy: 0.4651\n",
            "Epoch 58/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5170 - val_loss: 0.6960 - val_accuracy: 0.4703\n",
            "Epoch 59/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6936 - accuracy: 0.5066 - val_loss: 0.6961 - val_accuracy: 0.4651\n",
            "Epoch 60/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6924 - accuracy: 0.5120 - val_loss: 0.6960 - val_accuracy: 0.4625\n",
            "Epoch 61/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5052 - val_loss: 0.6961 - val_accuracy: 0.4651\n",
            "Epoch 62/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5130 - val_loss: 0.6960 - val_accuracy: 0.4599\n",
            "Epoch 63/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5092 - val_loss: 0.6959 - val_accuracy: 0.4703\n",
            "Epoch 64/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5185 - val_loss: 0.6960 - val_accuracy: 0.4651\n",
            "Epoch 65/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5128 - val_loss: 0.6958 - val_accuracy: 0.4703\n",
            "Epoch 66/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5049 - val_loss: 0.6959 - val_accuracy: 0.4703\n",
            "Epoch 67/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.5155 - val_loss: 0.6958 - val_accuracy: 0.4703\n",
            "Epoch 68/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5169 - val_loss: 0.6959 - val_accuracy: 0.4703\n",
            "Epoch 69/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5201 - val_loss: 0.6959 - val_accuracy: 0.4703\n",
            "Epoch 70/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6957 - val_accuracy: 0.4703\n",
            "Epoch 71/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6919 - accuracy: 0.5175 - val_loss: 0.6956 - val_accuracy: 0.4703\n",
            "Epoch 72/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6934 - accuracy: 0.5105 - val_loss: 0.6956 - val_accuracy: 0.4703\n",
            "Epoch 73/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5095 - val_loss: 0.6956 - val_accuracy: 0.4703\n",
            "Epoch 74/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5082 - val_loss: 0.6956 - val_accuracy: 0.4703\n",
            "Epoch 75/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5036 - val_loss: 0.6956 - val_accuracy: 0.4444\n",
            "Epoch 76/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.5145 - val_loss: 0.6956 - val_accuracy: 0.4703\n",
            "Epoch 77/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5206 - val_loss: 0.6955 - val_accuracy: 0.4703\n",
            "Epoch 78/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6935 - accuracy: 0.5074 - val_loss: 0.6954 - val_accuracy: 0.4703\n",
            "Epoch 79/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5102 - val_loss: 0.6955 - val_accuracy: 0.4651\n",
            "Epoch 80/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5117 - val_loss: 0.6954 - val_accuracy: 0.4574\n",
            "Epoch 81/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5048 - val_loss: 0.6953 - val_accuracy: 0.4703\n",
            "Epoch 82/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5079 - val_loss: 0.6953 - val_accuracy: 0.4703\n",
            "Epoch 83/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5128 - val_loss: 0.6953 - val_accuracy: 0.4574\n",
            "Epoch 84/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5068 - val_loss: 0.6953 - val_accuracy: 0.4832\n",
            "Epoch 85/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5016 - val_loss: 0.6951 - val_accuracy: 0.4703\n",
            "Epoch 86/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5125 - val_loss: 0.6951 - val_accuracy: 0.4703\n",
            "Epoch 87/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5180 - val_loss: 0.6952 - val_accuracy: 0.4703\n",
            "Epoch 88/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5078 - val_loss: 0.6952 - val_accuracy: 0.4832\n",
            "Epoch 89/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6953 - val_accuracy: 0.4884\n",
            "Epoch 90/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.5015 - val_loss: 0.6952 - val_accuracy: 0.4780\n",
            "Epoch 91/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5104 - val_loss: 0.6952 - val_accuracy: 0.4625\n",
            "Epoch 92/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5203 - val_loss: 0.6952 - val_accuracy: 0.4548\n",
            "Epoch 93/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5113 - val_loss: 0.6951 - val_accuracy: 0.4780\n",
            "Epoch 94/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5032 - val_loss: 0.6950 - val_accuracy: 0.4574\n",
            "Epoch 95/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5113 - val_loss: 0.6950 - val_accuracy: 0.4574\n",
            "Epoch 96/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5116 - val_loss: 0.6950 - val_accuracy: 0.4780\n",
            "Epoch 97/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.5199 - val_loss: 0.6951 - val_accuracy: 0.4935\n",
            "Epoch 98/500\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6924 - accuracy: 0.5158 - val_loss: 0.6951 - val_accuracy: 0.4832\n",
            "Epoch 99/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6916 - accuracy: 0.5330 - val_loss: 0.6950 - val_accuracy: 0.4548\n",
            "Epoch 100/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5173 - val_loss: 0.6950 - val_accuracy: 0.4935\n",
            "Epoch 101/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5134 - val_loss: 0.6949 - val_accuracy: 0.4832\n",
            "Epoch 102/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6948 - val_accuracy: 0.4858\n",
            "Epoch 103/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6930 - accuracy: 0.5098 - val_loss: 0.6948 - val_accuracy: 0.4780\n",
            "Epoch 104/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5137 - val_loss: 0.6949 - val_accuracy: 0.4884\n",
            "Epoch 105/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5142 - val_loss: 0.6949 - val_accuracy: 0.4935\n",
            "Epoch 106/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5107 - val_loss: 0.6948 - val_accuracy: 0.4935\n",
            "Epoch 107/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5057 - val_loss: 0.6949 - val_accuracy: 0.4935\n",
            "Epoch 108/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6947 - val_accuracy: 0.4548\n",
            "Epoch 109/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5101 - val_loss: 0.6948 - val_accuracy: 0.4935\n",
            "Epoch 110/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5162 - val_loss: 0.6947 - val_accuracy: 0.4858\n",
            "Epoch 111/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6936 - accuracy: 0.4931 - val_loss: 0.6947 - val_accuracy: 0.4858\n",
            "Epoch 112/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5125 - val_loss: 0.6948 - val_accuracy: 0.4780\n",
            "Epoch 113/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5134 - val_loss: 0.6948 - val_accuracy: 0.4599\n",
            "Epoch 114/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5112 - val_loss: 0.6947 - val_accuracy: 0.4935\n",
            "Epoch 115/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5148 - val_loss: 0.6947 - val_accuracy: 0.4935\n",
            "Epoch 116/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.5208 - val_loss: 0.6947 - val_accuracy: 0.4884\n",
            "Epoch 117/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6922 - accuracy: 0.5185 - val_loss: 0.6947 - val_accuracy: 0.4935\n",
            "Epoch 118/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5051 - val_loss: 0.6946 - val_accuracy: 0.4858\n",
            "Epoch 119/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5150 - val_loss: 0.6945 - val_accuracy: 0.4548\n",
            "Epoch 120/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5143 - val_loss: 0.6946 - val_accuracy: 0.4884\n",
            "Epoch 121/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.5140 - val_loss: 0.6947 - val_accuracy: 0.4987\n",
            "Epoch 122/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6946 - val_accuracy: 0.4858\n",
            "Epoch 123/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5164 - val_loss: 0.6947 - val_accuracy: 0.4910\n",
            "Epoch 124/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5138 - val_loss: 0.6946 - val_accuracy: 0.4651\n",
            "Epoch 125/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5202 - val_loss: 0.6945 - val_accuracy: 0.4832\n",
            "Epoch 126/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6946 - val_accuracy: 0.4935\n",
            "Epoch 127/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5216 - val_loss: 0.6945 - val_accuracy: 0.4884\n",
            "Epoch 128/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5116 - val_loss: 0.6944 - val_accuracy: 0.4884\n",
            "Epoch 129/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5107 - val_loss: 0.6945 - val_accuracy: 0.4884\n",
            "Epoch 130/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5058 - val_loss: 0.6946 - val_accuracy: 0.4910\n",
            "Epoch 131/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5187 - val_loss: 0.6945 - val_accuracy: 0.4935\n",
            "Epoch 132/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6921 - accuracy: 0.5211 - val_loss: 0.6945 - val_accuracy: 0.4935\n",
            "Epoch 133/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5135 - val_loss: 0.6946 - val_accuracy: 0.4884\n",
            "Epoch 134/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5077 - val_loss: 0.6946 - val_accuracy: 0.4884\n",
            "Epoch 135/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.5145 - val_loss: 0.6946 - val_accuracy: 0.4935\n",
            "Epoch 136/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.5130 - val_loss: 0.6946 - val_accuracy: 0.4910\n",
            "Epoch 137/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6922 - accuracy: 0.5186 - val_loss: 0.6946 - val_accuracy: 0.4935\n",
            "Epoch 138/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5015 - val_loss: 0.6944 - val_accuracy: 0.4858\n",
            "Epoch 139/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5092 - val_loss: 0.6944 - val_accuracy: 0.4884\n",
            "Epoch 140/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5127 - val_loss: 0.6945 - val_accuracy: 0.4910\n",
            "Epoch 141/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.5075 - val_loss: 0.6945 - val_accuracy: 0.4910\n",
            "Epoch 142/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6944 - val_accuracy: 0.4755\n",
            "Epoch 143/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5145 - val_loss: 0.6944 - val_accuracy: 0.4910\n",
            "Epoch 144/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5080 - val_loss: 0.6946 - val_accuracy: 0.4987\n",
            "Epoch 145/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.5042 - val_loss: 0.6944 - val_accuracy: 0.4910\n",
            "Epoch 146/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5077 - val_loss: 0.6943 - val_accuracy: 0.4574\n",
            "Epoch 147/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6926 - accuracy: 0.5181 - val_loss: 0.6944 - val_accuracy: 0.4884\n",
            "Epoch 148/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5065 - val_loss: 0.6946 - val_accuracy: 0.4987\n",
            "Epoch 149/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.5033 - val_loss: 0.6945 - val_accuracy: 0.4935\n",
            "Epoch 150/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5058 - val_loss: 0.6944 - val_accuracy: 0.4780\n",
            "Epoch 151/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5185 - val_loss: 0.6943 - val_accuracy: 0.4548\n",
            "Epoch 152/500\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6927 - accuracy: 0.5129 - val_loss: 0.6944 - val_accuracy: 0.4935\n",
            "Epoch 153/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6919 - accuracy: 0.5276 - val_loss: 0.6945 - val_accuracy: 0.4961\n",
            "Epoch 154/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5211 - val_loss: 0.6944 - val_accuracy: 0.4935\n",
            "Epoch 155/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5104 - val_loss: 0.6942 - val_accuracy: 0.4806\n",
            "Epoch 156/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6933 - accuracy: 0.5041 - val_loss: 0.6943 - val_accuracy: 0.5039\n",
            "Epoch 157/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5120 - val_loss: 0.6943 - val_accuracy: 0.4961\n",
            "Epoch 158/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.5104 - val_loss: 0.6942 - val_accuracy: 0.4780\n",
            "Epoch 159/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5196 - val_loss: 0.6943 - val_accuracy: 0.4910\n",
            "Epoch 160/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6921 - accuracy: 0.5146 - val_loss: 0.6943 - val_accuracy: 0.4884\n",
            "Epoch 161/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6934 - accuracy: 0.5002 - val_loss: 0.6943 - val_accuracy: 0.5013\n",
            "Epoch 162/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5011 - val_loss: 0.6942 - val_accuracy: 0.4832\n",
            "Epoch 163/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6920 - accuracy: 0.5175 - val_loss: 0.6943 - val_accuracy: 0.4858\n",
            "Epoch 164/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5083 - val_loss: 0.6943 - val_accuracy: 0.4910\n",
            "Epoch 165/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.5197 - val_loss: 0.6944 - val_accuracy: 0.4935\n",
            "Epoch 166/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.5124 - val_loss: 0.6943 - val_accuracy: 0.4935\n",
            "Epoch 167/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5128 - val_loss: 0.6941 - val_accuracy: 0.4729\n",
            "Epoch 168/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5143 - val_loss: 0.6942 - val_accuracy: 0.4935\n",
            "Epoch 169/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5141 - val_loss: 0.6942 - val_accuracy: 0.4961\n",
            "Epoch 170/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.5122 - val_loss: 0.6942 - val_accuracy: 0.4910\n",
            "Epoch 171/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6919 - accuracy: 0.5196 - val_loss: 0.6943 - val_accuracy: 0.4884\n",
            "Epoch 172/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5238 - val_loss: 0.6943 - val_accuracy: 0.4935\n",
            "Epoch 173/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.5067 - val_loss: 0.6942 - val_accuracy: 0.4935\n",
            "Epoch 174/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.5051 - val_loss: 0.6941 - val_accuracy: 0.4910\n",
            "Epoch 175/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6941 - val_accuracy: 0.4884\n",
            "Epoch 176/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5120 - val_loss: 0.6942 - val_accuracy: 0.4935\n",
            "Epoch 177/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6922 - accuracy: 0.5191 - val_loss: 0.6942 - val_accuracy: 0.4910\n",
            "Epoch 178/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5155 - val_loss: 0.6941 - val_accuracy: 0.4884\n",
            "Epoch 179/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6922 - accuracy: 0.5157 - val_loss: 0.6942 - val_accuracy: 0.4987\n",
            "Epoch 180/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5163 - val_loss: 0.6941 - val_accuracy: 0.4884\n",
            "Epoch 181/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5123 - val_loss: 0.6943 - val_accuracy: 0.4961\n",
            "Epoch 182/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5138 - val_loss: 0.6942 - val_accuracy: 0.4935\n",
            "Epoch 183/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5112 - val_loss: 0.6942 - val_accuracy: 0.4935\n",
            "Epoch 184/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5053 - val_loss: 0.6943 - val_accuracy: 0.4935\n",
            "Epoch 185/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5059 - val_loss: 0.6942 - val_accuracy: 0.4884\n",
            "Epoch 186/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6934 - accuracy: 0.5006 - val_loss: 0.6941 - val_accuracy: 0.4806\n",
            "Epoch 187/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5143 - val_loss: 0.6942 - val_accuracy: 0.4935\n",
            "Epoch 188/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5066 - val_loss: 0.6941 - val_accuracy: 0.4858\n",
            "Epoch 189/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6923 - accuracy: 0.5183 - val_loss: 0.6942 - val_accuracy: 0.4910\n",
            "Epoch 190/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5106 - val_loss: 0.6942 - val_accuracy: 0.4884\n",
            "Epoch 191/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5153 - val_loss: 0.6941 - val_accuracy: 0.4884\n",
            "Epoch 192/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5083 - val_loss: 0.6942 - val_accuracy: 0.4935\n",
            "Epoch 193/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5055 - val_loss: 0.6943 - val_accuracy: 0.4935\n",
            "Epoch 194/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.5163 - val_loss: 0.6942 - val_accuracy: 0.4935\n",
            "Epoch 195/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.5035 - val_loss: 0.6942 - val_accuracy: 0.4910\n",
            "Epoch 196/500\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.6921 - accuracy: 0.5202 - val_loss: 0.6941 - val_accuracy: 0.4858\n",
            "Epoch 197/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5117 - val_loss: 0.6941 - val_accuracy: 0.4987\n",
            "Epoch 198/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5101 - val_loss: 0.6940 - val_accuracy: 0.4858\n",
            "Epoch 199/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6918 - accuracy: 0.5160 - val_loss: 0.6941 - val_accuracy: 0.4884\n",
            "Epoch 200/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5200 - val_loss: 0.6941 - val_accuracy: 0.4935\n",
            "Epoch 201/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5069 - val_loss: 0.6941 - val_accuracy: 0.4935\n",
            "Epoch 202/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5125 - val_loss: 0.6940 - val_accuracy: 0.4858\n",
            "Epoch 203/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5002 - val_loss: 0.6940 - val_accuracy: 0.4858\n",
            "Epoch 204/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5078 - val_loss: 0.6942 - val_accuracy: 0.4987\n",
            "Epoch 205/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5213 - val_loss: 0.6940 - val_accuracy: 0.4884\n",
            "Epoch 206/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5100 - val_loss: 0.6940 - val_accuracy: 0.4884\n",
            "Epoch 207/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5080 - val_loss: 0.6941 - val_accuracy: 0.4935\n",
            "Epoch 208/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5143 - val_loss: 0.6941 - val_accuracy: 0.4961\n",
            "Epoch 209/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6933 - accuracy: 0.4947 - val_loss: 0.6940 - val_accuracy: 0.4858\n",
            "Epoch 210/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5133 - val_loss: 0.6941 - val_accuracy: 0.4935\n",
            "Epoch 211/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5167 - val_loss: 0.6941 - val_accuracy: 0.4884\n",
            "Epoch 212/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5102 - val_loss: 0.6940 - val_accuracy: 0.4935\n",
            "Epoch 213/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5260 - val_loss: 0.6940 - val_accuracy: 0.4884\n",
            "Epoch 214/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6941 - accuracy: 0.4895 - val_loss: 0.6940 - val_accuracy: 0.4884\n",
            "Epoch 215/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5128 - val_loss: 0.6940 - val_accuracy: 0.4910\n",
            "Epoch 216/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6941 - val_accuracy: 0.4910\n",
            "Epoch 217/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5099 - val_loss: 0.6940 - val_accuracy: 0.4858\n",
            "Epoch 218/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5033 - val_loss: 0.6940 - val_accuracy: 0.4858\n",
            "Epoch 219/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6943 - val_accuracy: 0.4987\n",
            "Epoch 220/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.4961 - val_loss: 0.6940 - val_accuracy: 0.4910\n",
            "Epoch 221/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5123 - val_loss: 0.6939 - val_accuracy: 0.4961\n",
            "Epoch 222/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6939 - val_accuracy: 0.4987\n",
            "Epoch 223/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6931 - accuracy: 0.5032 - val_loss: 0.6940 - val_accuracy: 0.4910\n",
            "Epoch 224/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5132 - val_loss: 0.6942 - val_accuracy: 0.4987\n",
            "Epoch 225/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6933 - accuracy: 0.5055 - val_loss: 0.6940 - val_accuracy: 0.4858\n",
            "Epoch 226/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5135 - val_loss: 0.6940 - val_accuracy: 0.4910\n",
            "Epoch 227/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6920 - accuracy: 0.5179 - val_loss: 0.6940 - val_accuracy: 0.4858\n",
            "Epoch 228/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5164 - val_loss: 0.6940 - val_accuracy: 0.4935\n",
            "Epoch 229/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5083 - val_loss: 0.6939 - val_accuracy: 0.4858\n",
            "Epoch 230/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6922 - accuracy: 0.5126 - val_loss: 0.6938 - val_accuracy: 0.4832\n",
            "Epoch 231/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5157 - val_loss: 0.6940 - val_accuracy: 0.4987\n",
            "Epoch 232/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6938 - val_accuracy: 0.4935\n",
            "Epoch 233/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5197 - val_loss: 0.6939 - val_accuracy: 0.4935\n",
            "Epoch 234/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5155 - val_loss: 0.6939 - val_accuracy: 0.4858\n",
            "Epoch 235/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5146 - val_loss: 0.6940 - val_accuracy: 0.4910\n",
            "Epoch 236/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6940 - val_accuracy: 0.4884\n",
            "Epoch 237/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.5111 - val_loss: 0.6940 - val_accuracy: 0.4884\n",
            "Epoch 238/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5134 - val_loss: 0.6939 - val_accuracy: 0.4858\n",
            "Epoch 239/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.5290 - val_loss: 0.6939 - val_accuracy: 0.4935\n",
            "Epoch 240/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.5045 - val_loss: 0.6939 - val_accuracy: 0.4961\n",
            "Epoch 241/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.5040 - val_loss: 0.6940 - val_accuracy: 0.4935\n",
            "Epoch 242/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5067 - val_loss: 0.6938 - val_accuracy: 0.4858\n",
            "Epoch 243/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5139 - val_loss: 0.6938 - val_accuracy: 0.4884\n",
            "Epoch 244/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5068 - val_loss: 0.6938 - val_accuracy: 0.4935\n",
            "Epoch 245/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5195 - val_loss: 0.6938 - val_accuracy: 0.4858\n",
            "Epoch 246/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5092 - val_loss: 0.6938 - val_accuracy: 0.4858\n",
            "Epoch 247/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5066 - val_loss: 0.6938 - val_accuracy: 0.4910\n",
            "Epoch 248/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5148 - val_loss: 0.6939 - val_accuracy: 0.4935\n",
            "Epoch 249/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5070 - val_loss: 0.6938 - val_accuracy: 0.4910\n",
            "Epoch 250/500\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.6929 - accuracy: 0.5080 - val_loss: 0.6939 - val_accuracy: 0.4858\n",
            "Epoch 251/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5142 - val_loss: 0.6937 - val_accuracy: 0.4858\n",
            "Epoch 252/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5031 - val_loss: 0.6938 - val_accuracy: 0.4910\n",
            "Epoch 253/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6939 - val_accuracy: 0.4987\n",
            "Epoch 254/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5135 - val_loss: 0.6939 - val_accuracy: 0.4884\n",
            "Epoch 255/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5218 - val_loss: 0.6938 - val_accuracy: 0.4884\n",
            "Epoch 256/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6937 - accuracy: 0.4979 - val_loss: 0.6938 - val_accuracy: 0.4935\n",
            "Epoch 257/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5078 - val_loss: 0.6938 - val_accuracy: 0.4961\n",
            "Epoch 258/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5140 - val_loss: 0.6940 - val_accuracy: 0.4961\n",
            "Epoch 259/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.5088 - val_loss: 0.6939 - val_accuracy: 0.4935\n",
            "Epoch 260/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5106 - val_loss: 0.6937 - val_accuracy: 0.4858\n",
            "Epoch 261/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5132 - val_loss: 0.6936 - val_accuracy: 0.4832\n",
            "Epoch 262/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5040 - val_loss: 0.6936 - val_accuracy: 0.4858\n",
            "Epoch 263/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5161 - val_loss: 0.6937 - val_accuracy: 0.4935\n",
            "Epoch 264/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5145 - val_loss: 0.6938 - val_accuracy: 0.4987\n",
            "Epoch 265/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5109 - val_loss: 0.6936 - val_accuracy: 0.4858\n",
            "Epoch 266/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6921 - accuracy: 0.5256 - val_loss: 0.6938 - val_accuracy: 0.4987\n",
            "Epoch 267/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6934 - accuracy: 0.5017 - val_loss: 0.6936 - val_accuracy: 0.4910\n",
            "Epoch 268/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5093 - val_loss: 0.6936 - val_accuracy: 0.4832\n",
            "Epoch 269/500\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6931 - accuracy: 0.5129 - val_loss: 0.6936 - val_accuracy: 0.4858\n",
            "Epoch 270/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6918 - accuracy: 0.5255 - val_loss: 0.6937 - val_accuracy: 0.4910\n",
            "Epoch 271/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5111 - val_loss: 0.6937 - val_accuracy: 0.4935\n",
            "Epoch 272/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5071 - val_loss: 0.6937 - val_accuracy: 0.4935\n",
            "Epoch 273/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6925 - accuracy: 0.5116 - val_loss: 0.6937 - val_accuracy: 0.4910\n",
            "Epoch 274/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6922 - accuracy: 0.5192 - val_loss: 0.6939 - val_accuracy: 0.4987\n",
            "Epoch 275/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6927 - accuracy: 0.5112 - val_loss: 0.6937 - val_accuracy: 0.4884\n",
            "Epoch 276/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5137 - val_loss: 0.6936 - val_accuracy: 0.4935\n",
            "Epoch 277/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5047 - val_loss: 0.6936 - val_accuracy: 0.4935\n",
            "Epoch 278/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5040 - val_loss: 0.6937 - val_accuracy: 0.4961\n",
            "Epoch 279/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5120 - val_loss: 0.6935 - val_accuracy: 0.4832\n",
            "Epoch 280/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.4937 - val_loss: 0.6935 - val_accuracy: 0.4832\n",
            "Epoch 281/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5126 - val_loss: 0.6936 - val_accuracy: 0.4935\n",
            "Epoch 282/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.5029 - val_loss: 0.6935 - val_accuracy: 0.4935\n",
            "Epoch 283/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5178 - val_loss: 0.6936 - val_accuracy: 0.4910\n",
            "Epoch 284/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5135 - val_loss: 0.6935 - val_accuracy: 0.4832\n",
            "Epoch 285/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6934 - accuracy: 0.4987 - val_loss: 0.6935 - val_accuracy: 0.4884\n",
            "Epoch 286/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5144 - val_loss: 0.6936 - val_accuracy: 0.4935\n",
            "Epoch 287/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5157 - val_loss: 0.6935 - val_accuracy: 0.4832\n",
            "Epoch 288/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5103 - val_loss: 0.6935 - val_accuracy: 0.4832\n",
            "Epoch 289/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.5018 - val_loss: 0.6935 - val_accuracy: 0.4910\n",
            "Epoch 290/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6930 - accuracy: 0.5089 - val_loss: 0.6936 - val_accuracy: 0.4961\n",
            "Epoch 291/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5163 - val_loss: 0.6935 - val_accuracy: 0.4858\n",
            "Epoch 292/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5100 - val_loss: 0.6935 - val_accuracy: 0.4884\n",
            "Epoch 293/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6933 - accuracy: 0.5083 - val_loss: 0.6936 - val_accuracy: 0.4987\n",
            "Epoch 294/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6934 - accuracy: 0.4968 - val_loss: 0.6934 - val_accuracy: 0.4858\n",
            "Epoch 295/500\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6927 - accuracy: 0.5065 - val_loss: 0.6935 - val_accuracy: 0.4858\n",
            "Epoch 296/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6918 - accuracy: 0.5305 - val_loss: 0.6935 - val_accuracy: 0.4884\n",
            "Epoch 297/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5135 - val_loss: 0.6934 - val_accuracy: 0.4935\n",
            "Epoch 298/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5107 - val_loss: 0.6933 - val_accuracy: 0.4884\n",
            "Epoch 299/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5215 - val_loss: 0.6933 - val_accuracy: 0.4858\n",
            "Epoch 300/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5141 - val_loss: 0.6935 - val_accuracy: 0.4935\n",
            "Epoch 301/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5129 - val_loss: 0.6934 - val_accuracy: 0.4910\n",
            "Epoch 302/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5120 - val_loss: 0.6934 - val_accuracy: 0.4961\n",
            "Epoch 303/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5046 - val_loss: 0.6934 - val_accuracy: 0.4935\n",
            "Epoch 304/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5080 - val_loss: 0.6933 - val_accuracy: 0.4832\n",
            "Epoch 305/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5185 - val_loss: 0.6935 - val_accuracy: 0.4935\n",
            "Epoch 306/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6938 - accuracy: 0.5002 - val_loss: 0.6936 - val_accuracy: 0.4987\n",
            "Epoch 307/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5195 - val_loss: 0.6934 - val_accuracy: 0.4858\n",
            "Epoch 308/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.5054 - val_loss: 0.6933 - val_accuracy: 0.4832\n",
            "Epoch 309/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5140 - val_loss: 0.6934 - val_accuracy: 0.4884\n",
            "Epoch 310/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5097 - val_loss: 0.6933 - val_accuracy: 0.4832\n",
            "Epoch 311/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6920 - accuracy: 0.5219 - val_loss: 0.6933 - val_accuracy: 0.4935\n",
            "Epoch 312/500\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6923 - accuracy: 0.5159 - val_loss: 0.6933 - val_accuracy: 0.4832\n",
            "Epoch 313/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5121 - val_loss: 0.6934 - val_accuracy: 0.4910\n",
            "Epoch 314/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5150 - val_loss: 0.6933 - val_accuracy: 0.4910\n",
            "Epoch 315/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6921 - accuracy: 0.5191 - val_loss: 0.6934 - val_accuracy: 0.4935\n",
            "Epoch 316/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5085 - val_loss: 0.6933 - val_accuracy: 0.4832\n",
            "Epoch 317/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.5087 - val_loss: 0.6934 - val_accuracy: 0.4935\n",
            "Epoch 318/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5129 - val_loss: 0.6934 - val_accuracy: 0.4935\n",
            "Epoch 319/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6916 - accuracy: 0.5231 - val_loss: 0.6933 - val_accuracy: 0.4858\n",
            "Epoch 320/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6926 - accuracy: 0.5132 - val_loss: 0.6933 - val_accuracy: 0.4935\n",
            "Epoch 321/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5148 - val_loss: 0.6933 - val_accuracy: 0.4935\n",
            "Epoch 322/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6928 - accuracy: 0.5145 - val_loss: 0.6932 - val_accuracy: 0.4935\n",
            "Epoch 323/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5117 - val_loss: 0.6933 - val_accuracy: 0.4935\n",
            "Epoch 324/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5130 - val_loss: 0.6931 - val_accuracy: 0.4780\n",
            "Epoch 325/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5107 - val_loss: 0.6932 - val_accuracy: 0.4858\n",
            "Epoch 326/500\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6939 - accuracy: 0.4907 - val_loss: 0.6933 - val_accuracy: 0.4987\n",
            "Epoch 327/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5138 - val_loss: 0.6933 - val_accuracy: 0.4961\n",
            "Epoch 328/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6922 - accuracy: 0.5167 - val_loss: 0.6932 - val_accuracy: 0.4858\n",
            "Epoch 329/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5106 - val_loss: 0.6931 - val_accuracy: 0.4910\n",
            "Epoch 330/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.5076 - val_loss: 0.6932 - val_accuracy: 0.4935\n",
            "Epoch 331/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6920 - accuracy: 0.5166 - val_loss: 0.6932 - val_accuracy: 0.4832\n",
            "Epoch 332/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6919 - accuracy: 0.5164 - val_loss: 0.6932 - val_accuracy: 0.4910\n",
            "Epoch 333/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6922 - accuracy: 0.5166 - val_loss: 0.6933 - val_accuracy: 0.4961\n",
            "Epoch 334/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6931 - val_accuracy: 0.4858\n",
            "Epoch 335/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5046 - val_loss: 0.6932 - val_accuracy: 0.4935\n",
            "Epoch 336/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5178 - val_loss: 0.6931 - val_accuracy: 0.4806\n",
            "Epoch 337/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5088 - val_loss: 0.6931 - val_accuracy: 0.4935\n",
            "Epoch 338/500\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6924 - accuracy: 0.5095 - val_loss: 0.6931 - val_accuracy: 0.4910\n",
            "Epoch 339/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5031 - val_loss: 0.6932 - val_accuracy: 0.4987\n",
            "Epoch 340/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5092 - val_loss: 0.6930 - val_accuracy: 0.4910\n",
            "Epoch 341/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5128 - val_loss: 0.6931 - val_accuracy: 0.4910\n",
            "Epoch 342/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6927 - accuracy: 0.5068 - val_loss: 0.6932 - val_accuracy: 0.4935\n",
            "Epoch 343/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6922 - accuracy: 0.5142 - val_loss: 0.6930 - val_accuracy: 0.4884\n",
            "Epoch 344/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5030 - val_loss: 0.6931 - val_accuracy: 0.4935\n",
            "Epoch 345/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5124 - val_loss: 0.6931 - val_accuracy: 0.4935\n",
            "Epoch 346/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5053 - val_loss: 0.6930 - val_accuracy: 0.4884\n",
            "Epoch 347/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5031 - val_loss: 0.6930 - val_accuracy: 0.4832\n",
            "Epoch 348/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5051 - val_loss: 0.6931 - val_accuracy: 0.4884\n",
            "Epoch 349/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5146 - val_loss: 0.6930 - val_accuracy: 0.4832\n",
            "Epoch 350/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5095 - val_loss: 0.6930 - val_accuracy: 0.4910\n",
            "Epoch 351/500\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6923 - accuracy: 0.5193 - val_loss: 0.6932 - val_accuracy: 0.4987\n",
            "Epoch 352/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5027 - val_loss: 0.6930 - val_accuracy: 0.4884\n",
            "Epoch 353/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5171 - val_loss: 0.6930 - val_accuracy: 0.4832\n",
            "Epoch 354/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5156 - val_loss: 0.6929 - val_accuracy: 0.4832\n",
            "Epoch 355/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5142 - val_loss: 0.6931 - val_accuracy: 0.4987\n",
            "Epoch 356/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5141 - val_loss: 0.6930 - val_accuracy: 0.4961\n",
            "Epoch 357/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5086 - val_loss: 0.6930 - val_accuracy: 0.4961\n",
            "Epoch 358/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6930 - accuracy: 0.5060 - val_loss: 0.6929 - val_accuracy: 0.4806\n",
            "Epoch 359/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5140 - val_loss: 0.6930 - val_accuracy: 0.4935\n",
            "Epoch 360/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6930 - accuracy: 0.5049 - val_loss: 0.6930 - val_accuracy: 0.4935\n",
            "Epoch 361/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5047 - val_loss: 0.6930 - val_accuracy: 0.4910\n",
            "Epoch 362/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5070 - val_loss: 0.6930 - val_accuracy: 0.4935\n",
            "Epoch 363/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6919 - accuracy: 0.5317 - val_loss: 0.6931 - val_accuracy: 0.4935\n",
            "Epoch 364/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6929 - val_accuracy: 0.4935\n",
            "Epoch 365/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5167 - val_loss: 0.6928 - val_accuracy: 0.4832\n",
            "Epoch 366/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5151 - val_loss: 0.6929 - val_accuracy: 0.4858\n",
            "Epoch 367/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5155 - val_loss: 0.6929 - val_accuracy: 0.4961\n",
            "Epoch 368/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5166 - val_loss: 0.6929 - val_accuracy: 0.4910\n",
            "Epoch 369/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5061 - val_loss: 0.6929 - val_accuracy: 0.4858\n",
            "Epoch 370/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5201 - val_loss: 0.6930 - val_accuracy: 0.4935\n",
            "Epoch 371/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5111 - val_loss: 0.6930 - val_accuracy: 0.4935\n",
            "Epoch 372/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5175 - val_loss: 0.6930 - val_accuracy: 0.4961\n",
            "Epoch 373/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5164 - val_loss: 0.6929 - val_accuracy: 0.4910\n",
            "Epoch 374/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.6929 - val_accuracy: 0.4910\n",
            "Epoch 375/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5111 - val_loss: 0.6930 - val_accuracy: 0.4935\n",
            "Epoch 376/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5070 - val_loss: 0.6929 - val_accuracy: 0.4935\n",
            "Epoch 377/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5130 - val_loss: 0.6928 - val_accuracy: 0.4832\n",
            "Epoch 378/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6922 - accuracy: 0.5193 - val_loss: 0.6928 - val_accuracy: 0.4935\n",
            "Epoch 379/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5007 - val_loss: 0.6928 - val_accuracy: 0.4935\n",
            "Epoch 380/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5282 - val_loss: 0.6930 - val_accuracy: 0.4987\n",
            "Epoch 381/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5011 - val_loss: 0.6928 - val_accuracy: 0.4910\n",
            "Epoch 382/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5109 - val_loss: 0.6928 - val_accuracy: 0.4832\n",
            "Epoch 383/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6938 - accuracy: 0.5066 - val_loss: 0.6929 - val_accuracy: 0.4935\n",
            "Epoch 384/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6923 - accuracy: 0.5112 - val_loss: 0.6927 - val_accuracy: 0.4832\n",
            "Epoch 385/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6930 - accuracy: 0.5101 - val_loss: 0.6929 - val_accuracy: 0.4935\n",
            "Epoch 386/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6934 - accuracy: 0.5024 - val_loss: 0.6928 - val_accuracy: 0.4832\n",
            "Epoch 387/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6930 - accuracy: 0.5022 - val_loss: 0.6928 - val_accuracy: 0.4935\n",
            "Epoch 388/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6920 - accuracy: 0.5187 - val_loss: 0.6929 - val_accuracy: 0.4935\n",
            "Epoch 389/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6930 - accuracy: 0.5070 - val_loss: 0.6928 - val_accuracy: 0.4858\n",
            "Epoch 390/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6926 - accuracy: 0.5094 - val_loss: 0.6928 - val_accuracy: 0.4935\n",
            "Epoch 391/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5143 - val_loss: 0.6928 - val_accuracy: 0.4910\n",
            "Epoch 392/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6925 - accuracy: 0.5117 - val_loss: 0.6929 - val_accuracy: 0.4935\n",
            "Epoch 393/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6933 - accuracy: 0.5011 - val_loss: 0.6927 - val_accuracy: 0.4858\n",
            "Epoch 394/500\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6915 - accuracy: 0.5251 - val_loss: 0.6928 - val_accuracy: 0.4910\n",
            "Epoch 395/500\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6922 - accuracy: 0.5165 - val_loss: 0.6928 - val_accuracy: 0.4935\n",
            "Epoch 396/500\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6934 - accuracy: 0.5005 - val_loss: 0.6928 - val_accuracy: 0.4987\n",
            "Epoch 397/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5111 - val_loss: 0.6928 - val_accuracy: 0.4935\n",
            "Epoch 398/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.5043 - val_loss: 0.6927 - val_accuracy: 0.4858\n",
            "Epoch 399/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6935 - accuracy: 0.5075 - val_loss: 0.6927 - val_accuracy: 0.4910\n",
            "Epoch 400/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6931 - accuracy: 0.4988 - val_loss: 0.6927 - val_accuracy: 0.4806\n",
            "Epoch 401/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5100 - val_loss: 0.6929 - val_accuracy: 0.4961\n",
            "Epoch 402/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6926 - accuracy: 0.5072 - val_loss: 0.6928 - val_accuracy: 0.4935\n",
            "Epoch 403/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5144 - val_loss: 0.6928 - val_accuracy: 0.4935\n",
            "Epoch 404/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5055 - val_loss: 0.6928 - val_accuracy: 0.4935\n",
            "Epoch 405/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5094 - val_loss: 0.6928 - val_accuracy: 0.4935\n",
            "Epoch 406/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5026 - val_loss: 0.6927 - val_accuracy: 0.4806\n",
            "Epoch 407/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5163 - val_loss: 0.6927 - val_accuracy: 0.4935\n",
            "Epoch 408/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6930 - accuracy: 0.5050 - val_loss: 0.6928 - val_accuracy: 0.4935\n",
            "Epoch 409/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6921 - accuracy: 0.5203 - val_loss: 0.6927 - val_accuracy: 0.4832\n",
            "Epoch 410/500\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6926 - accuracy: 0.5113 - val_loss: 0.6927 - val_accuracy: 0.4910\n",
            "Epoch 411/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5106 - val_loss: 0.6927 - val_accuracy: 0.4910\n",
            "Epoch 412/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6930 - accuracy: 0.5040 - val_loss: 0.6927 - val_accuracy: 0.4935\n",
            "Epoch 413/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5080 - val_loss: 0.6927 - val_accuracy: 0.4935\n",
            "Epoch 414/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5075 - val_loss: 0.6926 - val_accuracy: 0.4806\n",
            "Epoch 415/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5089 - val_loss: 0.6928 - val_accuracy: 0.4987\n",
            "Epoch 416/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5053 - val_loss: 0.6927 - val_accuracy: 0.4987\n",
            "Epoch 417/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5177 - val_loss: 0.6926 - val_accuracy: 0.4910\n",
            "Epoch 418/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.5033 - val_loss: 0.6926 - val_accuracy: 0.4806\n",
            "Epoch 419/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5135 - val_loss: 0.6927 - val_accuracy: 0.4935\n",
            "Epoch 420/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.5022 - val_loss: 0.6926 - val_accuracy: 0.4935\n",
            "Epoch 421/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6925 - accuracy: 0.5112 - val_loss: 0.6927 - val_accuracy: 0.4961\n",
            "Epoch 422/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5020 - val_loss: 0.6926 - val_accuracy: 0.4935\n",
            "Epoch 423/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5125 - val_loss: 0.6926 - val_accuracy: 0.4858\n",
            "Epoch 424/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6939 - accuracy: 0.4954 - val_loss: 0.6926 - val_accuracy: 0.4935\n",
            "Epoch 425/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.5156 - val_loss: 0.6927 - val_accuracy: 0.4858\n",
            "Epoch 426/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.6927 - val_accuracy: 0.4961\n",
            "Epoch 427/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6922 - accuracy: 0.5218 - val_loss: 0.6928 - val_accuracy: 0.4987\n",
            "Epoch 428/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5195 - val_loss: 0.6928 - val_accuracy: 0.4961\n",
            "Epoch 429/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5089 - val_loss: 0.6926 - val_accuracy: 0.4832\n",
            "Epoch 430/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6937 - accuracy: 0.4897 - val_loss: 0.6925 - val_accuracy: 0.4858\n",
            "Epoch 431/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5203 - val_loss: 0.6926 - val_accuracy: 0.4935\n",
            "Epoch 432/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6930 - accuracy: 0.4998 - val_loss: 0.6926 - val_accuracy: 0.4935\n",
            "Epoch 433/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5193 - val_loss: 0.6926 - val_accuracy: 0.4935\n",
            "Epoch 434/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6925 - val_accuracy: 0.4910\n",
            "Epoch 435/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6920 - accuracy: 0.5144 - val_loss: 0.6925 - val_accuracy: 0.4806\n",
            "Epoch 436/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5119 - val_loss: 0.6927 - val_accuracy: 0.4935\n",
            "Epoch 437/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5121 - val_loss: 0.6925 - val_accuracy: 0.4884\n",
            "Epoch 438/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5132 - val_loss: 0.6926 - val_accuracy: 0.4961\n",
            "Epoch 439/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5095 - val_loss: 0.6927 - val_accuracy: 0.4987\n",
            "Epoch 440/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5032 - val_loss: 0.6923 - val_accuracy: 0.4910\n",
            "Epoch 441/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5153 - val_loss: 0.6924 - val_accuracy: 0.4858\n",
            "Epoch 442/500\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6925 - accuracy: 0.5188 - val_loss: 0.6925 - val_accuracy: 0.4935\n",
            "Epoch 443/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.5038 - val_loss: 0.6924 - val_accuracy: 0.4935\n",
            "Epoch 444/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6935 - accuracy: 0.4991 - val_loss: 0.6925 - val_accuracy: 0.4935\n",
            "Epoch 445/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5142 - val_loss: 0.6925 - val_accuracy: 0.4910\n",
            "Epoch 446/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5103 - val_loss: 0.6925 - val_accuracy: 0.4910\n",
            "Epoch 447/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5136 - val_loss: 0.6925 - val_accuracy: 0.4884\n",
            "Epoch 448/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5118 - val_loss: 0.6926 - val_accuracy: 0.4961\n",
            "Epoch 449/500\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.6929 - accuracy: 0.5058 - val_loss: 0.6926 - val_accuracy: 0.4961\n",
            "Epoch 450/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6924 - val_accuracy: 0.4780\n",
            "Epoch 451/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.5106 - val_loss: 0.6927 - val_accuracy: 0.4987\n",
            "Epoch 452/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5053 - val_loss: 0.6925 - val_accuracy: 0.4935\n",
            "Epoch 453/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5058 - val_loss: 0.6925 - val_accuracy: 0.4832\n",
            "Epoch 454/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5116 - val_loss: 0.6926 - val_accuracy: 0.4935\n",
            "Epoch 455/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5101 - val_loss: 0.6925 - val_accuracy: 0.4858\n",
            "Epoch 456/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5160 - val_loss: 0.6927 - val_accuracy: 0.4961\n",
            "Epoch 457/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5159 - val_loss: 0.6926 - val_accuracy: 0.4935\n",
            "Epoch 458/500\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6923 - accuracy: 0.5174 - val_loss: 0.6926 - val_accuracy: 0.4935\n",
            "Epoch 459/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6922 - accuracy: 0.5170 - val_loss: 0.6924 - val_accuracy: 0.4780\n",
            "Epoch 460/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5146 - val_loss: 0.6926 - val_accuracy: 0.4935\n",
            "Epoch 461/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5054 - val_loss: 0.6925 - val_accuracy: 0.4884\n",
            "Epoch 462/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6914 - accuracy: 0.5213 - val_loss: 0.6927 - val_accuracy: 0.4987\n",
            "Epoch 463/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5091 - val_loss: 0.6926 - val_accuracy: 0.4935\n",
            "Epoch 464/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5184 - val_loss: 0.6925 - val_accuracy: 0.4858\n",
            "Epoch 465/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5149 - val_loss: 0.6925 - val_accuracy: 0.4884\n",
            "Epoch 466/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6923 - accuracy: 0.5164 - val_loss: 0.6924 - val_accuracy: 0.4884\n",
            "Epoch 467/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6930 - accuracy: 0.5065 - val_loss: 0.6926 - val_accuracy: 0.4935\n",
            "Epoch 468/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5125 - val_loss: 0.6925 - val_accuracy: 0.4935\n",
            "Epoch 469/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5083 - val_loss: 0.6925 - val_accuracy: 0.4910\n",
            "Epoch 470/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5055 - val_loss: 0.6925 - val_accuracy: 0.4935\n",
            "Epoch 471/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6933 - accuracy: 0.4994 - val_loss: 0.6925 - val_accuracy: 0.4935\n",
            "Epoch 472/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.5036 - val_loss: 0.6924 - val_accuracy: 0.4832\n",
            "Epoch 473/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6923 - accuracy: 0.5195 - val_loss: 0.6924 - val_accuracy: 0.4832\n",
            "Epoch 474/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6920 - accuracy: 0.5223 - val_loss: 0.6926 - val_accuracy: 0.4961\n",
            "Epoch 475/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6921 - accuracy: 0.5213 - val_loss: 0.6926 - val_accuracy: 0.4987\n",
            "Epoch 476/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5107 - val_loss: 0.6925 - val_accuracy: 0.4935\n",
            "Epoch 477/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6926 - accuracy: 0.5100 - val_loss: 0.6923 - val_accuracy: 0.4858\n",
            "Epoch 478/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.5089 - val_loss: 0.6924 - val_accuracy: 0.4935\n",
            "Epoch 479/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5068 - val_loss: 0.6925 - val_accuracy: 0.4987\n",
            "Epoch 480/500\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6926 - accuracy: 0.5098 - val_loss: 0.6923 - val_accuracy: 0.4884\n",
            "Epoch 481/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5151 - val_loss: 0.6924 - val_accuracy: 0.4910\n",
            "Epoch 482/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5161 - val_loss: 0.6924 - val_accuracy: 0.4935\n",
            "Epoch 483/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5064 - val_loss: 0.6925 - val_accuracy: 0.4987\n",
            "Epoch 484/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5177 - val_loss: 0.6925 - val_accuracy: 0.4961\n",
            "Epoch 485/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5105 - val_loss: 0.6924 - val_accuracy: 0.4935\n",
            "Epoch 486/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5111 - val_loss: 0.6924 - val_accuracy: 0.4858\n",
            "Epoch 487/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5185 - val_loss: 0.6926 - val_accuracy: 0.5013\n",
            "Epoch 488/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5129 - val_loss: 0.6924 - val_accuracy: 0.4832\n",
            "Epoch 489/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6919 - accuracy: 0.5188 - val_loss: 0.6924 - val_accuracy: 0.4806\n",
            "Epoch 490/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5056 - val_loss: 0.6924 - val_accuracy: 0.4910\n",
            "Epoch 491/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5093 - val_loss: 0.6924 - val_accuracy: 0.4935\n",
            "Epoch 492/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6921 - accuracy: 0.5169 - val_loss: 0.6923 - val_accuracy: 0.4858\n",
            "Epoch 493/500\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6929 - accuracy: 0.5087 - val_loss: 0.6926 - val_accuracy: 0.4987\n",
            "Epoch 494/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5187 - val_loss: 0.6924 - val_accuracy: 0.4858\n",
            "Epoch 495/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5041 - val_loss: 0.6924 - val_accuracy: 0.4935\n",
            "Epoch 496/500\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6932 - accuracy: 0.4990 - val_loss: 0.6924 - val_accuracy: 0.4935\n",
            "Epoch 497/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5090 - val_loss: 0.6923 - val_accuracy: 0.4910\n",
            "Epoch 498/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6920 - accuracy: 0.5160 - val_loss: 0.6925 - val_accuracy: 0.4935\n",
            "Epoch 499/500\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5133 - val_loss: 0.6925 - val_accuracy: 0.4935\n",
            "Epoch 500/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5174 - val_loss: 0.6926 - val_accuracy: 0.5013\n",
            "Accuracy: 50.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr1yK0p_dcXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "226e9f12-1808-488e-e665-418aa0b27ddb"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(25, input_dim=6, activation='relu', kernel_initializer='he_normal'))\n",
        "# model.add(Dropout(0.5))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(4, activation='relu'))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "model.summary()\n",
        "opt = keras.optimizers.RMSprop(lr=0.001, decay=1e-6)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit the keras model on the dataset\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=20, verbose=1)\n",
        "# evaluate the keras model\n",
        "_, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 25)                175       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 15)                390       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 128       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 702\n",
            "Trainable params: 702\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "182/182 [==============================] - 2s 5ms/step - loss: 3097.6005 - accuracy: 0.4996 - val_loss: 62.3110 - val_accuracy: 0.5186\n",
            "Epoch 2/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 444.1110 - accuracy: 0.5098 - val_loss: 2170.2375 - val_accuracy: 0.5037\n",
            "Epoch 3/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 1302.9624 - accuracy: 0.4793 - val_loss: 563.3571 - val_accuracy: 0.5037\n",
            "Epoch 4/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 546.2404 - accuracy: 0.4970 - val_loss: 169.7326 - val_accuracy: 0.4963\n",
            "Epoch 5/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 395.9036 - accuracy: 0.5163 - val_loss: 1584.4308 - val_accuracy: 0.4963\n",
            "Epoch 6/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 487.2011 - accuracy: 0.5115 - val_loss: 196.4645 - val_accuracy: 0.5037\n",
            "Epoch 7/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 595.2893 - accuracy: 0.5081 - val_loss: 241.5391 - val_accuracy: 0.5012\n",
            "Epoch 8/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 433.1667 - accuracy: 0.5047 - val_loss: 540.3625 - val_accuracy: 0.5037\n",
            "Epoch 9/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 380.3990 - accuracy: 0.4973 - val_loss: 694.5441 - val_accuracy: 0.4963\n",
            "Epoch 10/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 507.1270 - accuracy: 0.4853 - val_loss: 733.8787 - val_accuracy: 0.5037\n",
            "Epoch 11/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 489.9767 - accuracy: 0.5135 - val_loss: 468.6621 - val_accuracy: 0.5037\n",
            "Epoch 12/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 709.9222 - accuracy: 0.4951 - val_loss: 1994.7527 - val_accuracy: 0.5037\n",
            "Epoch 13/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 1145.2169 - accuracy: 0.5024 - val_loss: 204.9241 - val_accuracy: 0.5087\n",
            "Epoch 14/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 231.7849 - accuracy: 0.5089 - val_loss: 687.6295 - val_accuracy: 0.5037\n",
            "Epoch 15/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 744.0736 - accuracy: 0.4932 - val_loss: 98.0869 - val_accuracy: 0.4963\n",
            "Epoch 16/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 342.8373 - accuracy: 0.5179 - val_loss: 381.6827 - val_accuracy: 0.4963\n",
            "Epoch 17/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 289.2954 - accuracy: 0.4987 - val_loss: 228.1441 - val_accuracy: 0.4963\n",
            "Epoch 18/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 573.9297 - accuracy: 0.4988 - val_loss: 195.0735 - val_accuracy: 0.5037\n",
            "Epoch 19/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 414.4202 - accuracy: 0.4991 - val_loss: 297.4401 - val_accuracy: 0.5012\n",
            "Epoch 20/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 471.4686 - accuracy: 0.5085 - val_loss: 1420.9111 - val_accuracy: 0.4963\n",
            "Epoch 21/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 379.6876 - accuracy: 0.5011 - val_loss: 348.6616 - val_accuracy: 0.5012\n",
            "Epoch 22/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 384.6249 - accuracy: 0.5365 - val_loss: 298.3310 - val_accuracy: 0.4963\n",
            "Epoch 23/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 293.2473 - accuracy: 0.5382 - val_loss: 335.1290 - val_accuracy: 0.5012\n",
            "Epoch 24/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 387.9392 - accuracy: 0.4947 - val_loss: 182.2588 - val_accuracy: 0.4963\n",
            "Epoch 25/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 353.7301 - accuracy: 0.5149 - val_loss: 32.3620 - val_accuracy: 0.6129\n",
            "Epoch 26/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 292.5028 - accuracy: 0.5186 - val_loss: 514.6513 - val_accuracy: 0.4963\n",
            "Epoch 27/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 345.0580 - accuracy: 0.5153 - val_loss: 579.3176 - val_accuracy: 0.5037\n",
            "Epoch 28/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 389.4480 - accuracy: 0.5305 - val_loss: 55.5601 - val_accuracy: 0.5732\n",
            "Epoch 29/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 356.0521 - accuracy: 0.5109 - val_loss: 374.3929 - val_accuracy: 0.5037\n",
            "Epoch 30/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 444.8791 - accuracy: 0.5073 - val_loss: 175.6702 - val_accuracy: 0.5112\n",
            "Epoch 31/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 479.1117 - accuracy: 0.5250 - val_loss: 768.4180 - val_accuracy: 0.5037\n",
            "Epoch 32/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 689.5718 - accuracy: 0.5100 - val_loss: 294.7285 - val_accuracy: 0.5012\n",
            "Epoch 33/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 275.4062 - accuracy: 0.5286 - val_loss: 35.5155 - val_accuracy: 0.5831\n",
            "Epoch 34/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 245.2929 - accuracy: 0.4882 - val_loss: 44.1492 - val_accuracy: 0.5806\n",
            "Epoch 35/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 401.0258 - accuracy: 0.5008 - val_loss: 164.9703 - val_accuracy: 0.4963\n",
            "Epoch 36/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 246.4965 - accuracy: 0.5269 - val_loss: 145.8059 - val_accuracy: 0.5161\n",
            "Epoch 37/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 374.1405 - accuracy: 0.5209 - val_loss: 101.4814 - val_accuracy: 0.5335\n",
            "Epoch 38/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 276.3900 - accuracy: 0.5227 - val_loss: 137.5383 - val_accuracy: 0.4963\n",
            "Epoch 39/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 292.9345 - accuracy: 0.4956 - val_loss: 301.6568 - val_accuracy: 0.4963\n",
            "Epoch 40/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 382.2795 - accuracy: 0.5038 - val_loss: 217.7672 - val_accuracy: 0.5062\n",
            "Epoch 41/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 528.1532 - accuracy: 0.5097 - val_loss: 661.0367 - val_accuracy: 0.5037\n",
            "Epoch 42/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 972.9691 - accuracy: 0.5120 - val_loss: 56.7373 - val_accuracy: 0.5782\n",
            "Epoch 43/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 314.4926 - accuracy: 0.5046 - val_loss: 466.1269 - val_accuracy: 0.4963\n",
            "Epoch 44/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 621.0231 - accuracy: 0.4938 - val_loss: 432.6010 - val_accuracy: 0.4963\n",
            "Epoch 45/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 401.4314 - accuracy: 0.5124 - val_loss: 413.3488 - val_accuracy: 0.5037\n",
            "Epoch 46/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 627.0015 - accuracy: 0.4859 - val_loss: 222.6741 - val_accuracy: 0.5062\n",
            "Epoch 47/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 187.5726 - accuracy: 0.5510 - val_loss: 743.6584 - val_accuracy: 0.5037\n",
            "Epoch 48/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 345.3639 - accuracy: 0.5282 - val_loss: 79.2957 - val_accuracy: 0.5558\n",
            "Epoch 49/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 253.5831 - accuracy: 0.5282 - val_loss: 957.6989 - val_accuracy: 0.5037\n",
            "Epoch 50/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 311.4034 - accuracy: 0.4969 - val_loss: 1048.4513 - val_accuracy: 0.4963\n",
            "Epoch 51/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 678.9387 - accuracy: 0.5041 - val_loss: 286.4332 - val_accuracy: 0.4963\n",
            "Epoch 52/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 208.5465 - accuracy: 0.5194 - val_loss: 533.6904 - val_accuracy: 0.5037\n",
            "Epoch 53/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 588.9565 - accuracy: 0.5147 - val_loss: 28.8993 - val_accuracy: 0.6179\n",
            "Epoch 54/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 233.5087 - accuracy: 0.5322 - val_loss: 38.8066 - val_accuracy: 0.5087\n",
            "Epoch 55/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 171.0762 - accuracy: 0.5301 - val_loss: 694.1537 - val_accuracy: 0.5037\n",
            "Epoch 56/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 793.5554 - accuracy: 0.5251 - val_loss: 196.8332 - val_accuracy: 0.4963\n",
            "Epoch 57/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 356.2935 - accuracy: 0.5281 - val_loss: 144.5204 - val_accuracy: 0.5211\n",
            "Epoch 58/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 221.1640 - accuracy: 0.4918 - val_loss: 116.5332 - val_accuracy: 0.5285\n",
            "Epoch 59/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 322.2100 - accuracy: 0.5236 - val_loss: 174.2438 - val_accuracy: 0.4963\n",
            "Epoch 60/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 169.8985 - accuracy: 0.5144 - val_loss: 218.2963 - val_accuracy: 0.5062\n",
            "Epoch 61/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 251.7794 - accuracy: 0.4959 - val_loss: 535.5859 - val_accuracy: 0.5037\n",
            "Epoch 62/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 308.1079 - accuracy: 0.5009 - val_loss: 257.4154 - val_accuracy: 0.4963\n",
            "Epoch 63/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 220.8654 - accuracy: 0.5189 - val_loss: 445.9739 - val_accuracy: 0.5037\n",
            "Epoch 64/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 358.7641 - accuracy: 0.5012 - val_loss: 191.2358 - val_accuracy: 0.4963\n",
            "Epoch 65/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 156.5061 - accuracy: 0.5160 - val_loss: 204.6277 - val_accuracy: 0.5062\n",
            "Epoch 66/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 236.5577 - accuracy: 0.5185 - val_loss: 575.9188 - val_accuracy: 0.4963\n",
            "Epoch 67/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 253.0961 - accuracy: 0.5207 - val_loss: 807.1012 - val_accuracy: 0.4963\n",
            "Epoch 68/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 436.3819 - accuracy: 0.5068 - val_loss: 27.6301 - val_accuracy: 0.6228\n",
            "Epoch 69/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 137.1759 - accuracy: 0.5201 - val_loss: 350.3227 - val_accuracy: 0.5037\n",
            "Epoch 70/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 179.8818 - accuracy: 0.5094 - val_loss: 36.8042 - val_accuracy: 0.4963\n",
            "Epoch 71/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 434.4168 - accuracy: 0.4964 - val_loss: 445.9720 - val_accuracy: 0.4963\n",
            "Epoch 72/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 201.6908 - accuracy: 0.5128 - val_loss: 137.0342 - val_accuracy: 0.4963\n",
            "Epoch 73/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 219.0845 - accuracy: 0.5147 - val_loss: 123.5097 - val_accuracy: 0.4963\n",
            "Epoch 74/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 210.9816 - accuracy: 0.5269 - val_loss: 414.2023 - val_accuracy: 0.4963\n",
            "Epoch 75/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 202.3749 - accuracy: 0.5027 - val_loss: 105.4296 - val_accuracy: 0.5211\n",
            "Epoch 76/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 162.0310 - accuracy: 0.5071 - val_loss: 29.1347 - val_accuracy: 0.5955\n",
            "Epoch 77/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 180.8633 - accuracy: 0.5391 - val_loss: 324.6950 - val_accuracy: 0.4963\n",
            "Epoch 78/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 124.2406 - accuracy: 0.5535 - val_loss: 285.8323 - val_accuracy: 0.5037\n",
            "Epoch 79/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 156.5696 - accuracy: 0.5217 - val_loss: 398.1163 - val_accuracy: 0.5037\n",
            "Epoch 80/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 251.3006 - accuracy: 0.5060 - val_loss: 276.9484 - val_accuracy: 0.4963\n",
            "Epoch 81/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 427.3325 - accuracy: 0.4973 - val_loss: 676.3090 - val_accuracy: 0.4963\n",
            "Epoch 82/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 293.1488 - accuracy: 0.5028 - val_loss: 213.8645 - val_accuracy: 0.4963\n",
            "Epoch 83/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 220.1587 - accuracy: 0.5216 - val_loss: 15.8727 - val_accuracy: 0.6402\n",
            "Epoch 84/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 203.4281 - accuracy: 0.5144 - val_loss: 120.0531 - val_accuracy: 0.5161\n",
            "Epoch 85/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 164.4845 - accuracy: 0.5020 - val_loss: 190.6663 - val_accuracy: 0.4963\n",
            "Epoch 86/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 212.3479 - accuracy: 0.5146 - val_loss: 91.3560 - val_accuracy: 0.5236\n",
            "Epoch 87/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 203.5507 - accuracy: 0.5341 - val_loss: 141.6458 - val_accuracy: 0.4963\n",
            "Epoch 88/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 140.4214 - accuracy: 0.5040 - val_loss: 84.3222 - val_accuracy: 0.5285\n",
            "Epoch 89/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 154.6402 - accuracy: 0.5062 - val_loss: 15.0626 - val_accuracy: 0.6427\n",
            "Epoch 90/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 134.9177 - accuracy: 0.5357 - val_loss: 51.5158 - val_accuracy: 0.5434\n",
            "Epoch 91/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 156.6648 - accuracy: 0.5192 - val_loss: 120.0106 - val_accuracy: 0.5112\n",
            "Epoch 92/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 221.4296 - accuracy: 0.5182 - val_loss: 171.2786 - val_accuracy: 0.5062\n",
            "Epoch 93/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 233.4372 - accuracy: 0.5184 - val_loss: 168.9002 - val_accuracy: 0.4963\n",
            "Epoch 94/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 217.3453 - accuracy: 0.5031 - val_loss: 373.6193 - val_accuracy: 0.4963\n",
            "Epoch 95/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 234.8765 - accuracy: 0.5374 - val_loss: 416.4413 - val_accuracy: 0.4963\n",
            "Epoch 96/100\n",
            "182/182 [==============================] - 0s 3ms/step - loss: 206.3416 - accuracy: 0.5111 - val_loss: 299.0679 - val_accuracy: 0.5037\n",
            "Epoch 97/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 159.3388 - accuracy: 0.5267 - val_loss: 496.0862 - val_accuracy: 0.4963\n",
            "Epoch 98/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 400.0906 - accuracy: 0.5160 - val_loss: 232.4403 - val_accuracy: 0.4963\n",
            "Epoch 99/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 86.0559 - accuracy: 0.5376 - val_loss: 92.9825 - val_accuracy: 0.5186\n",
            "Epoch 100/100\n",
            "182/182 [==============================] - 1s 3ms/step - loss: 158.3217 - accuracy: 0.5196 - val_loss: 19.2204 - val_accuracy: 0.5087\n",
            "Accuracy: 50.87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV2_uHpydf0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcc80073-f9c7-4d17-e844-022f94fa0a59"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pred = model.predict_generator(X_test,verbose=1)\n",
        "pred_labels = model.predict_classes(X_test,verbose=1)\n",
        "print(pred)\n",
        "print(pred_labels)\n",
        "print(y_test)\n",
        "print(X_test)\n",
        "predicted_class_indices=np.argmax(pred,axis=1)\n",
        "# print(predicted_class_indices)\n",
        "# Prob = pd.Series(pred)\n",
        "Label = pd.Series(y_test)\n",
        "Data = pd.DataFrame(X_test, columns =['FName', 'LName', 'Age','aa','aaa','aaaaa'], \n",
        "                                           dtype = float)  \n",
        "\n",
        "Label.to_csv(\"classification.csv\",index=False)\n",
        "Data.to_csv(\"classification2.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 0s 1ms/step\n",
            "13/13 [==============================] - 0s 1ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[0.48353496]\n",
            " [0.50942594]\n",
            " [0.48148218]\n",
            " [0.50621456]\n",
            " [0.5116758 ]\n",
            " [0.5170016 ]\n",
            " [0.5110285 ]\n",
            " [0.48655888]\n",
            " [0.51534104]\n",
            " [0.5091932 ]\n",
            " [0.48454738]\n",
            " [0.51512843]\n",
            " [0.5115004 ]\n",
            " [0.5083582 ]\n",
            " [0.506237  ]\n",
            " [0.50762343]\n",
            " [0.5060426 ]\n",
            " [0.50766426]\n",
            " [0.48294523]\n",
            " [0.5123879 ]\n",
            " [0.50725126]\n",
            " [0.5097631 ]\n",
            " [0.5094035 ]\n",
            " [0.5063008 ]\n",
            " [0.5069233 ]\n",
            " [0.5124977 ]\n",
            " [0.486762  ]\n",
            " [0.48266512]\n",
            " [0.50944066]\n",
            " [0.52592486]\n",
            " [0.48874563]\n",
            " [0.49286932]\n",
            " [0.50983226]\n",
            " [0.48451602]\n",
            " [0.5368487 ]\n",
            " [0.49449325]\n",
            " [0.51073855]\n",
            " [0.5062863 ]\n",
            " [0.5138571 ]\n",
            " [0.50818306]\n",
            " [0.48892343]\n",
            " [0.5339851 ]\n",
            " [0.5121993 ]\n",
            " [0.5152696 ]\n",
            " [0.50631714]\n",
            " [0.5062208 ]\n",
            " [0.48282248]\n",
            " [0.529864  ]\n",
            " [0.52171   ]\n",
            " [0.50466734]\n",
            " [0.48459953]\n",
            " [0.5070411 ]\n",
            " [0.48483673]\n",
            " [0.5094231 ]\n",
            " [0.48134765]\n",
            " [0.51512015]\n",
            " [0.5121094 ]\n",
            " [0.5106154 ]\n",
            " [0.48153266]\n",
            " [0.48874533]\n",
            " [0.5300944 ]\n",
            " [0.53308254]\n",
            " [0.5082137 ]\n",
            " [0.5122311 ]\n",
            " [0.4867276 ]\n",
            " [0.5149178 ]\n",
            " [0.49107218]\n",
            " [0.51091164]\n",
            " [0.48671144]\n",
            " [0.5060244 ]\n",
            " [0.5328666 ]\n",
            " [0.51147306]\n",
            " [0.5184091 ]\n",
            " [0.48379514]\n",
            " [0.5092691 ]\n",
            " [0.5104948 ]\n",
            " [0.4866402 ]\n",
            " [0.5333258 ]\n",
            " [0.51524246]\n",
            " [0.5089852 ]\n",
            " [0.51134413]\n",
            " [0.48621696]\n",
            " [0.5270453 ]\n",
            " [0.50398165]\n",
            " [0.5062585 ]\n",
            " [0.50952786]\n",
            " [0.48231867]\n",
            " [0.53301126]\n",
            " [0.51519936]\n",
            " [0.4840247 ]\n",
            " [0.51416713]\n",
            " [0.53133124]\n",
            " [0.5115966 ]\n",
            " [0.5062716 ]\n",
            " [0.50625575]\n",
            " [0.484718  ]\n",
            " [0.5271752 ]\n",
            " [0.48688066]\n",
            " [0.5089049 ]\n",
            " [0.50972474]\n",
            " [0.50620097]\n",
            " [0.5124838 ]\n",
            " [0.5151108 ]\n",
            " [0.5344054 ]\n",
            " [0.48576146]\n",
            " [0.5060763 ]\n",
            " [0.52532536]\n",
            " [0.48347375]\n",
            " [0.50622195]\n",
            " [0.50798446]\n",
            " [0.49371704]\n",
            " [0.50629437]\n",
            " [0.5095897 ]\n",
            " [0.5099193 ]\n",
            " [0.4868929 ]\n",
            " [0.51817816]\n",
            " [0.51222545]\n",
            " [0.51411736]\n",
            " [0.5085106 ]\n",
            " [0.5334087 ]\n",
            " [0.51516503]\n",
            " [0.49402216]\n",
            " [0.53674716]\n",
            " [0.5339142 ]\n",
            " [0.50991464]\n",
            " [0.5270992 ]\n",
            " [0.5351009 ]\n",
            " [0.48575518]\n",
            " [0.50623626]\n",
            " [0.5099044 ]\n",
            " [0.50394356]\n",
            " [0.49822095]\n",
            " [0.4866869 ]\n",
            " [0.48586088]\n",
            " [0.5333137 ]\n",
            " [0.5233469 ]\n",
            " [0.53492016]\n",
            " [0.51220894]\n",
            " [0.5272015 ]\n",
            " [0.5298292 ]\n",
            " [0.5163702 ]\n",
            " [0.5309805 ]\n",
            " [0.50621724]\n",
            " [0.48401952]\n",
            " [0.5096213 ]\n",
            " [0.51089036]\n",
            " [0.5129815 ]\n",
            " [0.48545107]\n",
            " [0.5144284 ]\n",
            " [0.48216563]\n",
            " [0.48620853]\n",
            " [0.50995165]\n",
            " [0.48532245]\n",
            " [0.50626594]\n",
            " [0.51527154]\n",
            " [0.48893198]\n",
            " [0.53397095]\n",
            " [0.50715655]\n",
            " [0.5278902 ]\n",
            " [0.48565477]\n",
            " [0.5071514 ]\n",
            " [0.5078816 ]\n",
            " [0.5094428 ]\n",
            " [0.48639306]\n",
            " [0.48134825]\n",
            " [0.5071077 ]\n",
            " [0.49964595]\n",
            " [0.48675108]\n",
            " [0.48660022]\n",
            " [0.51527876]\n",
            " [0.5152907 ]\n",
            " [0.51477855]\n",
            " [0.48256928]\n",
            " [0.48487735]\n",
            " [0.5083338 ]\n",
            " [0.48753595]\n",
            " [0.5151706 ]\n",
            " [0.48573768]\n",
            " [0.51532495]\n",
            " [0.51012117]\n",
            " [0.48457915]\n",
            " [0.5152226 ]\n",
            " [0.50971824]\n",
            " [0.4859513 ]\n",
            " [0.5375683 ]\n",
            " [0.4855235 ]\n",
            " [0.5151878 ]\n",
            " [0.48993063]\n",
            " [0.531669  ]\n",
            " [0.5275123 ]\n",
            " [0.5065397 ]\n",
            " [0.5067642 ]\n",
            " [0.506222  ]\n",
            " [0.5240081 ]\n",
            " [0.5116208 ]\n",
            " [0.51021016]\n",
            " [0.51428014]\n",
            " [0.5079104 ]\n",
            " [0.5121136 ]\n",
            " [0.5311642 ]\n",
            " [0.49780768]\n",
            " [0.5362396 ]\n",
            " [0.48652807]\n",
            " [0.50388986]\n",
            " [0.49396062]\n",
            " [0.5121144 ]\n",
            " [0.5151805 ]\n",
            " [0.48861   ]\n",
            " [0.48722097]\n",
            " [0.48406935]\n",
            " [0.49063534]\n",
            " [0.48658207]\n",
            " [0.49679005]\n",
            " [0.48509118]\n",
            " [0.5151458 ]\n",
            " [0.53253174]\n",
            " [0.48696733]\n",
            " [0.5063124 ]\n",
            " [0.5150099 ]\n",
            " [0.515215  ]\n",
            " [0.5292439 ]\n",
            " [0.5105971 ]\n",
            " [0.4910758 ]\n",
            " [0.51052475]\n",
            " [0.5106016 ]\n",
            " [0.5076579 ]\n",
            " [0.51480967]\n",
            " [0.5122225 ]\n",
            " [0.4864766 ]\n",
            " [0.531077  ]\n",
            " [0.49420226]\n",
            " [0.49766368]\n",
            " [0.5113237 ]\n",
            " [0.509406  ]\n",
            " [0.48662448]\n",
            " [0.48139614]\n",
            " [0.48532963]\n",
            " [0.52786714]\n",
            " [0.5169883 ]\n",
            " [0.51090807]\n",
            " [0.4780125 ]\n",
            " [0.52769727]\n",
            " [0.53253347]\n",
            " [0.507045  ]\n",
            " [0.50861114]\n",
            " [0.48122   ]\n",
            " [0.5100716 ]\n",
            " [0.48743138]\n",
            " [0.48456544]\n",
            " [0.48376566]\n",
            " [0.50059277]\n",
            " [0.515195  ]\n",
            " [0.515805  ]\n",
            " [0.48598203]\n",
            " [0.5253416 ]\n",
            " [0.5096528 ]\n",
            " [0.5152973 ]\n",
            " [0.48570296]\n",
            " [0.51479536]\n",
            " [0.5071355 ]\n",
            " [0.51075035]\n",
            " [0.5141265 ]\n",
            " [0.5176013 ]\n",
            " [0.5095735 ]\n",
            " [0.5363367 ]\n",
            " [0.5324973 ]\n",
            " [0.53311044]\n",
            " [0.5092802 ]\n",
            " [0.52654624]\n",
            " [0.5149    ]\n",
            " [0.51516765]\n",
            " [0.5141166 ]\n",
            " [0.51075155]\n",
            " [0.4834572 ]\n",
            " [0.5312897 ]\n",
            " [0.5153501 ]\n",
            " [0.5062484 ]\n",
            " [0.51309866]\n",
            " [0.48155493]\n",
            " [0.48968634]\n",
            " [0.5109091 ]\n",
            " [0.50949365]\n",
            " [0.533433  ]\n",
            " [0.49020767]\n",
            " [0.49343574]\n",
            " [0.5062336 ]\n",
            " [0.4867339 ]\n",
            " [0.4871912 ]\n",
            " [0.51911247]\n",
            " [0.4940257 ]\n",
            " [0.5147384 ]\n",
            " [0.5309289 ]\n",
            " [0.5245926 ]\n",
            " [0.5151904 ]\n",
            " [0.48653367]\n",
            " [0.5070462 ]\n",
            " [0.48730496]\n",
            " [0.48456854]\n",
            " [0.48266757]\n",
            " [0.48251033]\n",
            " [0.4868301 ]\n",
            " [0.50706   ]\n",
            " [0.53292525]\n",
            " [0.506308  ]\n",
            " [0.50631833]\n",
            " [0.50625545]\n",
            " [0.53682935]\n",
            " [0.5066702 ]\n",
            " [0.4856809 ]\n",
            " [0.51269907]\n",
            " [0.53093874]\n",
            " [0.5288373 ]\n",
            " [0.52466065]\n",
            " [0.5150501 ]\n",
            " [0.48674455]\n",
            " [0.5152792 ]\n",
            " [0.5097414 ]\n",
            " [0.5126789 ]\n",
            " [0.5341335 ]\n",
            " [0.51492864]\n",
            " [0.4838355 ]\n",
            " [0.50623745]\n",
            " [0.48487258]\n",
            " [0.5150155 ]\n",
            " [0.5094314 ]\n",
            " [0.4864272 ]\n",
            " [0.50715065]\n",
            " [0.5297333 ]\n",
            " [0.5101312 ]\n",
            " [0.48375344]\n",
            " [0.53766453]\n",
            " [0.51523685]\n",
            " [0.51511514]\n",
            " [0.51555693]\n",
            " [0.5062401 ]\n",
            " [0.5060967 ]\n",
            " [0.4983512 ]\n",
            " [0.5164788 ]\n",
            " [0.50788414]\n",
            " [0.50704694]\n",
            " [0.50835073]\n",
            " [0.5226991 ]\n",
            " [0.5136331 ]\n",
            " [0.53579557]\n",
            " [0.5096549 ]\n",
            " [0.53321046]\n",
            " [0.48583007]\n",
            " [0.4856901 ]\n",
            " [0.4866188 ]\n",
            " [0.48122886]\n",
            " [0.5070496 ]\n",
            " [0.50911367]\n",
            " [0.4864203 ]\n",
            " [0.51520556]\n",
            " [0.5110092 ]\n",
            " [0.5298304 ]\n",
            " [0.51530397]\n",
            " [0.510072  ]\n",
            " [0.5245555 ]\n",
            " [0.50989914]\n",
            " [0.48682126]\n",
            " [0.48406553]\n",
            " [0.49267572]\n",
            " [0.50980115]\n",
            " [0.5062334 ]\n",
            " [0.5152394 ]\n",
            " [0.5097104 ]\n",
            " [0.51496416]\n",
            " [0.4956939 ]\n",
            " [0.48192343]\n",
            " [0.50973237]\n",
            " [0.53109866]\n",
            " [0.51345   ]\n",
            " [0.5362446 ]\n",
            " [0.5296823 ]\n",
            " [0.4829697 ]\n",
            " [0.520956  ]\n",
            " [0.5153489 ]\n",
            " [0.53614897]\n",
            " [0.5366733 ]\n",
            " [0.50735927]\n",
            " [0.48913726]\n",
            " [0.48376912]\n",
            " [0.5062156 ]\n",
            " [0.4847669 ]\n",
            " [0.5151528 ]\n",
            " [0.48257062]]\n",
            "[[0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]]\n",
            "[1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
            " 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1.\n",
            " 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1.\n",
            " 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0.\n",
            " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1.\n",
            " 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1.\n",
            " 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
            " 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1.\n",
            " 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0.\n",
            " 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 1.]\n",
            "[[0.96440337 0.94535586 0.97901666 0.93418496 0.94690265 0.99019608]\n",
            " [0.20972783 0.23428419 0.46945073 0.51486431 0.81415929 0.66666667]\n",
            " [0.98611275 0.97537873 0.99321127 0.9696827  0.96460177 1.        ]\n",
            " ...\n",
            " [0.74761366 0.83123825 0.89796338 0.82262907 0.91150442 0.97058824]\n",
            " [0.2534338  0.29050554 0.58959062 0.58786131 0.84070796 0.93137255]\n",
            " [0.59118785 0.79492048 0.87739148 0.80131866 0.90265487 0.97058824]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRbib9bCNy6o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "369a30aa-20ce-4d1d-a5fa-674133347320"
      },
      "source": [
        "accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "epochs = range(1,501)\n",
        "plt.plot(epochs, accuracy, 'g', label='accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'b', label='val_accuracy')\n",
        "plt.title('Training accuracy and Test Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}